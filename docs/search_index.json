[["index.html", "Data Skills for Reproducible Science Overview 0.1 Course Aims 0.2 Intended Learning Outcomes 0.3 Course Resources 0.4 Course Outline 0.5 Formative Exercises 0.6 I found a bug! 0.7 Other Resources", " Data Skills for Reproducible Science 2021-08-22 Overview This course provides an overview of skills needed for reproducible research and open science using the statistical programming language R. Students will learn about data visualisation, data tidying and wrangling, archiving, iteration and functions, probability and data simulations, general linear models, and reproducible workflows. Learning is reinforced through weekly assignments that involve working with different types of data. 0.1 Course Aims This course aims to teach students the basic principles of reproducible research and to provide practical training in data processing and analysis in the statistical programming language R. 0.2 Intended Learning Outcomes By the end of this course students will be able to: Write scripts in R to organise and transform data sets using best accepted practices Explain basics of probability and its role in statistical inference Critically analyse data and report descriptive and inferential statistics in a reproducible manner 0.3 Course Resources Data Skills Videos Each chapter has several short video lectures for the main learning outcomes at the playlist . The videos are captioned and watching with the captioning on is a useful way to learn the jargon of computational reproducibility. If you cannot access YouTube, the videos are available on the course Teams and Moodle sites or by request from the instructor. dataskills This is a custom R package for this course. You can install it with the code below. It will download all of the packages that are used in the book, along with an offline copy of this book, the shiny apps used in the book, and the exercises. devtools::install_github(&quot;psyteachr/msc-data-skills&quot;) glossary Coding and statistics both have a lot of specialist terms. Throughout this book, jargon will be linked to the glossary. 0.4 Course Outline The overview below lists the beginner learning outcomes only. Some lessons have additional learning outcomes for intermediate or advanced students. Getting Started Understand the components of the RStudio IDE Type commands into the console Understand function syntax Install a package Organise a project Create and compile an Rmarkdown document Working with Data Load built-in datasets Import data from CSV and Excel files Create a data table Understand the use the basic data types Understand and use the basic container types (list, vector) Use vectorized operations Be able to troubleshoot common data import problems Data Visualisation Understand what types of graphs are best for different types of data Create common types of graphs with ggplot2 Set custom labels, colours, and themes Combine plots on the same plot, as facets, or as a grid using cowplot Save plots as an image file Tidy Data Understand the concept of tidy data Be able to convert between long and wide formats using pivot functions Be able to use the 4 basic tidyr verbs Be able to chain functions using pipes Data Wrangling Be able to use the 6 main dplyr one-table verbs: select(), filter(), arrange(), mutate(), summarise(), group_by() Be able to wrangle data by chaining tidyr and dplyr functions Be able to use these additional one-table verbs: rename(), distinct(), count(), slice(), pull() Data Relations Be able to use the 4 mutating join verbs: left_join(), right_join(), inner_join(), full_join() Be able to use the 2 filtering join verbs: semi_join(), anti_join() Be able to use the 2 binding join verbs: bind_rows(), bind_cols() Be able to use the 3 set operations: intersect(), union(), setdiff() Iteration &amp; Functions Work with iteration functions: rep(), seq(), and replicate() Use map() and apply() functions Write your own custom functions with function() Set default values for the arguments in your functions Probability &amp; Simulation Generate and plot data randomly sampled from common distributions: uniform, binomial, normal, poisson Generate related variables from a multivariate distribution Define the following statistical terms: p-value, alpha, power, smallest effect size of interest (SESOI), false positive (type I error), false negative (type II error), confidence interval (CI) Test sampled distributions against a null hypothesis using: exact binomial test, t-test (1-sample, independent samples, paired samples), correlation (pearson, kendall and spearman) Calculate power using iteration and a sampling function Introduction to GLM Define the components of the GLM Simulate data using GLM equations Identify the model parameters that correspond to the data-generation parameters Understand and plot residuals Predict new values using the model Explain the differences among coding schemes Reproducible Workflows Create a reproducible script in R Markdown Edit the YAML header to add table of contents and other options Include a table Include a figure Use source() to include code from an external file Report the output of an analysis using inline R 0.5 Formative Exercises Exercises are available at the end of each lesson’s webpage. These are not marked or mandatory, but if you can work through each of these (using web resources, of course), you will easily complete the marked assessments. Download all exercises and data files below as a ZIP archive. 01 intro: Intro to R, functions, R markdown 02 data: Vectors, tabular data, data import, pipes 03 ggplot: Data visualisation 04 tidyr: Tidy Data 05 dplyr: Data wrangling 06 joins: Data relations 07 functions: Functions and iteration 08 simulation: Simulation 09 glm: GLM 0.6 I found a bug! This book is a work in progress, so you might find errors. Please help me fix them! The best way is to open an issue on github that describes the error, but you can also mention it on the class Teams forum or email Lisa. 0.7 Other Resources Learning Statistics with R by Navarro R for Data Science by Grolemund and Wickham swirl R for Reproducible Scientific Analysis codeschool.com datacamp Improving your statistical inferences on Coursera You can access several cheatsheets in RStudio under the Help menu, or get the most recent RStudio Cheat Sheets Style guide for R programming #rstats on twitter highly recommended! "],["intro.html", "Chapter 1 Getting Started 1.1 Learning Objectives 1.2 Resources 1.3 What is R? 1.4 Getting Started 1.5 Add-on packages 1.6 Organising a project 1.7 Glossary 1.8 Exercises", " Chapter 1 Getting Started 1.1 Learning Objectives Understand the components of the RStudio IDE (video) Type commands into the console (video) Understand function syntax (video) Install a package (video) Organise a project (video) Create and compile an Rmarkdown document (video) 1.2 Resources Chapter 1: Introduction in R for Data Science RStudio IDE Cheatsheet Introduction to R Markdown R Markdown Cheatsheet R Markdown Reference RStudio Cloud 1.3 What is R? R is a programming environment for data processing and statistical analysis. We use R in Psychology at the University of Glasgow to promote reproducible research. This refers to being able to document and reproduce all of the steps between raw data and results. R allows you to write scripts that combine data files, clean data, and run analyses. There are many other ways to do this, including writing SPSS syntax files, but we find R to be a useful tool that is free, open source, and commonly used by research psychologists. See Appendix A for more information on on how to install R and associated programs. 1.3.1 The Base R Console If you open up the application called R, you will see an “R Console” window that looks something like this. Figure 1.1: The R Console window. You can close R and never open it again. We’ll be working entirely in RStudio in this class. ALWAYS REMEMBER: Launch R though the RStudio IDE Launch (RStudio.app), not (R.app). 1.3.2 RStudio RStudio is an Integrated Development Environment (IDE). This is a program that serves as a text editor, file manager, and provides many functions to help you read and write R code. Figure 1.2: The RStudio IDE RStudio is arranged with four window panes. By default, the upper left pane is the source pane, where you view and edit source code from files. The bottom left pane is usually the console pane, where you can type in commands and view output messages. The right panes have several different tabs that show you information about your code. You can change the location of panes and what tabs are shown under Preferences &gt; Pane Layout. Your browser does not support the video tag. 1.3.3 Configure RStudio In this class, you will be learning how to do reproducible research. This involves writing scripts that completely and transparently perform some analysis from start to finish in a way that yields the same result for different people using the same software on different computers. Transparency is a key value of science, as embodied in the “trust but verify” motto. When you do things reproducibly, others can understand and check your work. This benefits science, but there is a selfish reason, too: the most important person who will benefit from a reproducible script is your future self. When you return to an analysis after two weeks of vacation, you will thank your earlier self for doing things in a transparent, reproducible way, as you can easily pick up right where you left off. There are two tweaks that you should do to your RStudio installation to maximize reproducibility. Go to Global Options... under the Tools menu (⌘,), and uncheck the box that says Restore .RData into workspace at startup. If you keep things around in your workspace, things will get messy, and unexpected things will happen. You should always start with a clear workspace. This also means that you never want to save your workspace when you exit, so set this to Never. The only thing you want to save are your scripts. Figure 1.3: Alter these settings for increased reproducibility. Your settings should have: Restore .RData into workspace at startup: Checked Not Checked Save workspace to .RData on exit: Always Never Ask 1.4 Getting Started 1.4.1 Console commands We are first going to learn about how to interact with the console. In general, you will be developing R script or R Markdown files, rather than working directly in the console window. However, you can consider the console a kind of “sandbox” where you can try out lines of code and adapt them until you get them to do what you want. Then you can copy them back into the script editor. Mostly, however, you will be typing into the script editor window (either into an R script or an R Markdown file) and then sending the commands to the console by placing the cursor on the line and holding down the Ctrl key while you press Enter. The Ctrl+Enter key sequence sends the command in the script to the console. One simple way to learn about the R console is to use it as a calculator. Enter the lines of code below and see if your results match. Be prepared to make lots of typos (at first). 1 + 1 ## [1] 2 The R console remembers a history of the commands you typed in the past. Use the up and down arrow keys on your keyboard to scroll backwards and forwards through your history. It’s a lot faster than re-typing. 1 + 1 + 3 ## [1] 5 You can break up mathematical expressions over multiple lines; R waits for a complete expression before processing it. ## here comes a long expression ## let&#39;s break it over multiple lines 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 ## [1] 55 Text inside quotes is called a string. &quot;Good afternoon&quot; ## [1] &quot;Good afternoon&quot; You can break up text over multiple lines; R waits for a close quote before processing it. If you want to include a double quote inside this quoted string, escape it with a backslash. africa &lt;- &quot;I hear the drums echoing tonight But she hears only whispers of some quiet conversation She&#39;s coming in, 12:30 flight The moonlit wings reflect the stars that guide me towards salvation I stopped an old man along the way Hoping to find some old forgotten words or ancient melodies He turned to me as if to say, \\&quot;Hurry boy, it&#39;s waiting there for you\\&quot; - Toto&quot; cat(africa) # cat() prints the string ## I hear the drums echoing tonight ## But she hears only whispers of some quiet conversation ## She&#39;s coming in, 12:30 flight ## The moonlit wings reflect the stars that guide me towards salvation ## I stopped an old man along the way ## Hoping to find some old forgotten words or ancient melodies ## He turned to me as if to say, &quot;Hurry boy, it&#39;s waiting there for you&quot; ## ## - Toto 1.4.2 Objects Often you want to store the result of some computation for later use. You can store it in an object (also sometimes called a variable). An object in R: contains only letters, numbers, full stops, and underscores starts with a letter or a full stop and a letter distinguishes uppercase and lowercase letters (rickastley is not the same as RickAstley) The following are valid and different objects: songdata SongData song_data song.data .song.data never_gonna_give_you_up_never_gonna_let_you_down The following are not valid objects: _song_data 1song .1song song data song-data Use the assignment operator&lt;-` to assign the value on the right to the object named on the left. ## use the assignment operator &#39;&lt;-&#39; ## R stores the number in the object x &lt;- 5 Now that we have set x to a value, we can do something with it: x * 2 ## R evaluates the expression and stores the result in the object boring_calculation boring_calculation &lt;- 2 + 2 ## [1] 10 Note that it doesn’t print the result back at you when it’s stored. To view the result, just type the object name on a blank line. boring_calculation ## [1] 4 Once an object is assigned a value, its value doesn’t change unless you reassign the object, even if the objects you used to calculate it change. Predict what the code below does and test yourself: this_year &lt;- 2019 my_birth_year &lt;- 1976 my_age &lt;- this_year - my_birth_year this_year &lt;- 2020 After all the code above is run: this_year = 43 44 1976 2019 2020 my_birth_year = 43 44 1976 2019 2020 my_age = 43 44 1976 2019 2020 1.4.3 The environment Anytime you assign something to a new object, R creates a new entry in the global environment. Objects in the global environment exist until you end your session; then they disappear forever (unless you save them). Look at the Environment tab in the upper right pane. It lists all of the objects you have created. Click the broom icon to clear all of the objects and start fresh. You can also use the following functions in the console to view all objects, remove one object, or remove all objects. ls() # print the objects in the global environment rm(&quot;x&quot;) # remove the object named x from the global environment rm(list = ls()) # clear out the global environment In the upper right corner of the Environment tab, change List to Grid. Now you can see the type, length, and size of your objects, and reorder the list by any of these attributes. 1.4.4 Whitespace R mostly ignores whitespace: spaces, tabs, and line breaks. This means that you can use whitespace to help you organise your code. # a and b are identical a &lt;- list(ctl = &quot;Control Condition&quot;, exp1 = &quot;Experimental Condition 1&quot;, exp2 = &quot;Experimental Condition 2&quot;) # but b is much easier to read b &lt;- list(ctl = &quot;Control Condition&quot;, exp1 = &quot;Experimental Condition 1&quot;, exp2 = &quot;Experimental Condition 2&quot;) When you see &gt; at the beginning of a line, that means R is waiting for you to start a new command. However, if you see a + instead of &gt; at the start of the line, that means R is waiting for you to finish a command you started on a previous line. If you want to cancel whatever command you started, just press the Esc key in the console window and you’ll get back to the &gt; command prompt. # R waits until next line for evaluation (3 + 2) * 5 ## [1] 25 It is often useful to break up long functions onto several lines. cat(&quot;3, 6, 9, the goose drank wine&quot;, &quot;The monkey chewed tobacco on the streetcar line&quot;, &quot;The line broke, the monkey got choked&quot;, &quot;And they all went to heaven in a little rowboat&quot;, sep = &quot; \\n&quot;) ## 3, 6, 9, the goose drank wine ## The monkey chewed tobacco on the streetcar line ## The line broke, the monkey got choked ## And they all went to heaven in a little rowboat 1.4.5 Function syntax A lot of what you do in R involves calling a function and storing the results. A function is a named section of code that can be reused. For example, sd is a function that returns the standard deviation of the vector of numbers that you provide as the input argument. Functions are set up like this: function_name(argument1, argument2 = \"value\"). The arguments in parentheses can be named (like, argument1 = 10) or you can skip the names if you put them in the exact same order that they’re defined in the function. You can check this by typing ?sd (or whatever function name you’re looking up) into the console and the Help pane will show you the default order under Usage. You can also skip arguments that have a default value specified. Most functions return a value, but may also produce side effects like printing to the console. To illustrate, the function rnorm() generates random numbers from the standard normal distribution. The help page for rnorm() (accessed by typing ?rnorm in the console) shows that it has the syntax rnorm(n, mean = 0, sd = 1) where n is the number of randomly generated numbers you want, mean is the mean of the distribution, and sd is the standard deviation. The default mean is 0, and the default standard deviation is 1. There is no default for n, which means you’ll get an error if you don’t specify it: rnorm() ## Error in rnorm(): argument &quot;n&quot; is missing, with no default If you want 10 random numbers from a normal distribution with mean of 0 and standard deviation, you can just use the defaults. rnorm(10) ## [1] -0.04234663 -2.00393149 0.83611187 -1.46404127 1.31714428 0.42608581 ## [7] -0.46673798 -0.01670509 1.64072295 0.85876439 If you want 10 numbers from a normal distribution with a mean of 100: rnorm(10, 100) ## [1] 101.34917 99.86059 100.36287 99.65575 100.66818 100.04771 99.76782 ## [8] 102.57691 100.05575 99.42490 This would be an equivalent but less efficient way of calling the function: rnorm(n = 10, mean = 100) ## [1] 100.52773 99.40241 100.39641 101.01629 99.41961 100.52202 98.09828 ## [8] 99.52169 100.25677 99.92092 We don’t need to name the arguments because R will recognize that we intended to fill in the first and second arguments by their position in the function call. However, if we want to change the default for an argument coming later in the list, then we need to name it. For instance, if we wanted to keep the default mean = 0 but change the standard deviation to 100 we would do it this way: rnorm(10, sd = 100) ## [1] -68.254349 -17.636619 140.047575 7.570674 -68.309751 -2.378786 ## [7] 117.356343 -104.772092 -40.163750 54.358941 Some functions give a list of options after an argument; this means the default value is the first option. The usage entry for the power.t.test() function looks like this: power.t.test(n = NULL, delta = NULL, sd = 1, sig.level = 0.05, power = NULL, type = c(&quot;two.sample&quot;, &quot;one.sample&quot;, &quot;paired&quot;), alternative = c(&quot;two.sided&quot;, &quot;one.sided&quot;), strict = FALSE, tol = .Machine$double.eps^0.25) What is the default value for sd? NULL 1 0.05 two.sample What is the default value for type? NULL two.sample one.sample paired Which is equivalent to power.t.test(100, 0.5)? power.t.test(100, 0.5, sig.level = 1, sd = 0.05) power.t.test() power.t.test(n = 100) power.t.test(delta = 0.5, n = 100) 1.4.6 Getting help Start up help in a browser using the function help.start(). If a function is in base R or a loaded package, you can use the help(\"function_name\") function or the ?function_name shortcut to access the help file. If the package isn’t loaded, specify the package name as the second argument to the help function. # these methods are all equivalent ways of getting help help(&quot;rnorm&quot;) ?rnorm help(&quot;rnorm&quot;, package=&quot;stats&quot;) When the package isn’t loaded or you aren’t sure what package the function is in, use the shortcut ??function_name. What is the first argument to the mean function? trim na.rm mean x What package is read_excel in? readr readxl base stats 1.5 Add-on packages One of the great things about R is that it is user extensible: anyone can create a new add-on software package that extends its functionality. There are currently thousands of add-on packages that R users have created to solve many different kinds of problems, or just simply to have fun. There are packages for data visualisation, machine learning, neuroimaging, eyetracking, web scraping, and playing games such as Sudoku. Add-on packages are not distributed with base R, but have to be downloaded and installed from an archive, in the same way that you would, for instance, download and install a fitness app on your smartphone. The main repository where packages reside is called CRAN, the Comprehensive R Archive Network. A package has to pass strict tests devised by the R core team to be allowed to be part of the CRAN archive. You can install from the CRAN archive through R using the install.packages() function. There is an important distinction between installing a package and loading a package. 1.5.1 Installing a package This is done using install.packages(). This is like installing an app on your phone: you only have to do it once and the app will remain installed until you remove it. For instance, if you want to use PokemonGo on your phone, you install it once from the App Store or Play Store, and you don’t have to re-install it each time you want to use it. Once you launch the app, it will run in the background until you close it or restart your phone. Likewise, when you install a package, the package will be available (but not loaded) every time you open up R. You may only be able to permanently install packages if you are using R on your own system; you may not be able to do this on public workstations if you lack the appropriate privileges. Install the ggExtra package on your system. This package lets you create plots with marginal histograms. install.packages(&quot;ggExtra&quot;) If you don’t already have packages like ggplot2 and shiny installed, it will also install these dependencies for you. If you don’t get an error message at the end, the installation was successful. 1.5.2 Loading a package This is done using library(packagename). This is like launching an app on your phone: the functionality is only there where the app is launched and remains there until you close the app or restart. Likewise, when you run library(packagename) within a session, the functionality of the package referred to by packagename will be made available for your R session. The next time you start R, you will need to run the library() function again if you want to access its functionality. You can load the functions in ggExtra for your current R session as follows: library(ggExtra) You might get some red text when you load a package, this is normal. It is usually warning you that this package has functions that have the same name as other packages you’ve already loaded. You can use the convention package::function() to indicate in which add-on package a function resides. For instance, if you see readr::read_csv(), that refers to the function read_csv() in the readr add-on package. Now you can run the function ggExtra::runExample(), which runs an interactive example of marginal plots using shiny. ggExtra::runExample() 1.5.3 Install from GitHub Many R packages are not yet on CRAN because they are still in development. Increasingly, datasets and code for papers are available as packages you can download from github. You’ll need to install the devtools package to be able to install packages from github. Check if you have a package installed by trying to load it (e.g., if you don’t have devtools installed, library(\"devtools\") will display an error message) or by searching for it in the packages tab in the lower right pane. All listed packages are installed; all checked packages are currently loaded. Figure 1.4: Check installed and loaded packages in the packages tab in the lower right pane. # install devtools if you get # Error in loadNamespace(name) : there is no package called ‘devtools’ # install.packages(&quot;devtools&quot;) devtools::install_github(&quot;psyteachr/msc-data-skills&quot;) After you install the dataskills package, load it using the library() function. You can then try out some of the functions below. book() opens a local copy of this book in your web browser. app(\"plotdemo\") opens a shiny app that lets you see how simulated data would look in different plot styles exercise(1) creates and opens a file containing the exercises for this chapter ?disgust shows you the documentation for the built-in dataset disgust, which we will be using in future lessons library(dataskills) book() app(&quot;plotdemo&quot;) exercise(1) ?disgust How many different ways can you find to discover what functions are available in the dataskills package? 1.6 Organising a project Projects in RStudio are a way to group all of the files you need for one project. Most projects include scripts, data files, and output files like the PDF version of the script or images. Make a new directory where you will keep all of your materials for this class. If you’re using a lab computer, make sure you make this directory in your network drive so you can access it from other computers. Choose New Project… under the File menu to create a new project called 01-intro in this directory. 1.6.1 Structure Here is what an R script looks like. Don’t worry about the details for now. # load add-on packages library(tidyverse) # set object ---- n &lt;- 100 # simulate data ---- data &lt;- data.frame( id = 1:n, dv = c(rnorm(n/2, 0), rnorm(n/2, 1)), condition = rep(c(&quot;A&quot;, &quot;B&quot;), each = n/2) ) # plot data ---- ggplot(data, aes(condition, dv)) + geom_violin(trim = FALSE) + geom_boxplot(width = 0.25, aes(fill = condition), show.legend = FALSE) # save plot ---- ggsave(&quot;sim_data.png&quot;, width = 8, height = 6) It’s best if you follow the following structure when developing your own scripts: load in any add-on packages you need to use define any custom functions load or simulate the data you will be working with work with the data save anything you need to save Often when you are working on a script, you will realize that you need to load another add-on package. Don’t bury the call to library(package_I_need) way down in the script. Put it in the top, so the user has an overview of what packages are needed. You can add comments to an R script by with the hash symbol (#). The R interpreter will ignore characters from the hash to the end of the line. ## comments: any text from &#39;#&#39; on is ignored until end of line 22 / 7 # approximation to pi ## [1] 3.142857 If you add 4 or more dashes to the end of a comment, it acts like a header and creates an outline that you can see in the document outline (⇧⌘O). 1.6.2 Reproducible reports with R Markdown We will make reproducible reports following the principles of literate programming. The basic idea is to have the text of the report together in a single document along with the code needed to perform all analyses and generate the tables. The report is then “compiled” from the original format into some other, more portable format, such as HTML or PDF. This is different from traditional cutting and pasting approaches where, for instance, you create a graph in Microsoft Excel or a statistics program like SPSS and then paste it into Microsoft Word. We will use R Markdown to create reproducible reports, which enables mixing of text and code. A reproducible script will contain sections of code in code blocks. A code block starts and ends with backtick symbols in a row, with some infomation about the code between curly brackets, such as {r chunk-name, echo=FALSE} (this runs the code, but does not show the text of the code block in the compiled document). The text outside of code blocks is written in markdown, which is a way to specify formatting, such as headers, paragraphs, lists, bolding, and links. Figure 1.5: A reproducible script. If you open up a new R Markdown file from a template, you will see an example document with several code blocks in it. To create an HTML or PDF report from an R Markdown (Rmd) document, you compile it. Compiling a document is called knitting in RStudio. There is a button that looks like a ball of yarn with needles through it that you click on to compile your file into a report. Create a new R Markdown file from the File &gt; New File &gt; R Markdown… menu. Change the title and author, then click the knit button to create an html file. 1.6.3 Working Directory Where should you put all of your files? When developing an analysis, you usually want to have all of your scripts and data files in one subtree of your computer’s directory structure. Usually there is a single working directory where your data and scripts are stored. Your script should only reference files in three locations, using the appropriate format. Where Example on the web “https://psyteachr.github.io/msc-data-skills/data/disgust_scores.csv” in the working directory “disgust_scores.csv” in a subdirectory “data/disgust_scores.csv” Never set or change your working directory in a script. If you are working with an R Markdown file, it will automatically use the same directory the .Rmd file is in as the working directory. If you are working with R scripts, store your main script file in the top-level directory and manually set your working directory to that location. You will have to reset the working directory each time you open RStudio, unless you create a project and access the script from the project. For instance, if you are on a Windows machine your data and scripts are in the directory C:\\Carla's_files\\thesis2\\my_thesis\\new_analysis, you will set your working directory in one of two ways: (1) by going to the Session pull down menu in RStudio and choosing Set Working Directory, or (2) by typing setwd(\"C:\\Carla's_files\\thesis2\\my_thesis\\new_analysis\") in the console window. It’s tempting to make your life simple by putting the setwd() command in your script. Don’t do this! Others will not have the same directory tree as you (and when your laptop dies and you get a new one, neither will you). When manually setting the working directory, always do so by using the Session &gt; Set Working Directory pull-down option or by typing setwd() in the console. If your script needs a file in a subdirectory of new_analysis, say, data/questionnaire.csv, load it in using a relative path so that it is accessible if you move the folder new_analysis to another location or computer: dat &lt;- read_csv(&quot;data/questionnaire.csv&quot;) # correct Do not load it in using an absolute path: dat &lt;- read_csv(&quot;C:/Carla&#39;s_files/thesis22/my_thesis/new_analysis/data/questionnaire.csv&quot;) # wrong Also note the convention of using forward slashes, unlike the Windows-specific convention of using backward slashes. This is to make references to files platform independent. 1.7 Glossary Each chapter ends with a glossary table defining the jargon introduced in this chapter. The links below take you to the glossary book, which you can also download for offline use with devtools::install_github(\"psyteachr/glossary\") and access the glossary offline with glossary::book(). term definition absolute path A file path that starts with / and is not appended to the working directory argument A variable that provides input to a function. assignment operator The symbol &lt;-, which functions like = and assigns the value on the right to the object on the left base r The set of R functions that come with a basic installation of R, before you add external packages console The pane in RStudio where you can type in commands and view output messages. cran The Comprehensive R Archive Network: a network of ftp and web servers around the world that store identical, up-to-date, versions of code and documentation for R. escape Include special characters like \" inside of a string by prefacing them with a backslash. function A named section of code that can be reused. global environment The interactive workspace where your script runs ide Integrated Development Environment: a program that serves as a text editor, file manager, and provides functions to help you read and write code. RStudio is an IDE for R. knit To create an HTML, PDF, or Word document from an R Markdown (Rmd) document markdown A way to specify formatting, such as headers, paragraphs, lists, bolding, and links. normal distribution A symmetric distribution of data where values near the centre are most probable. object A word that identifies and stores the value of some data for later use. package A group of R functions. panes RStudio is arranged with four window “panes.” project A way to organise related files in RStudio r markdown The R-specific version of markdown: a way to specify formatting, such as headers, paragraphs, lists, bolding, and links, as well as code blocks and inline code. relative path The location of a file in relation to the working directory. reproducible research Research that documents all of the steps between raw data and results in a way that can be verified. script A plain-text file that contains commands in a coding language, such as R. scripts NA standard deviation A descriptive statistic that measures how spread out data are relative to the mean. string A piece of text inside of quotes. variable A word that identifies and stores the value of some data for later use. vector A type of data structure that is basically a list of things like T/F values, numbers, or strings. whitespace Spaces, tabs and line breaks working directory The filepath where R is currently reading and writing files. 1.8 Exercises Download the first set of exercises and put it in the project directory you created earlier for today’s exercises. See the answers only after you’ve attempted all the questions. # run this to access the exercise dataskills::exercise(1) # run this to access the answers dataskills::exercise(1, answers = TRUE) "],["data.html", "Chapter 2 Working with Data 2.1 Learning Objectives 2.2 Resources 2.3 Setup 2.4 Data tables 2.5 Basic data types 2.6 Basic container types 2.7 Troubleshooting 2.8 Glossary 2.9 Exercises", " Chapter 2 Working with Data 2.1 Learning Objectives Load built-in datasets (video) Import data from CSV and Excel files (video) Create a data table (video) Understand the use the basic data types (video) Understand and use the basic container types (list, vector) (video) Use vectorized operations (video) Be able to troubleshoot common data import problems (video) 2.2 Resources Chapter 11: Data Import in R for Data Science RStudio Data Import Cheatsheet Scottish Babynames Developing an analysis in R/RStudio: Scottish babynames (1/2) Developing an analysis in R/RStudio: Scottish babynames (2/2) 2.3 Setup # libraries needed for these examples library(tidyverse) library(dataskills) 2.4 Data tables 2.4.1 Built-in data R comes with built-in datasets. Some packages, like tidyr and dataskills, also contain data. The data() function lists the datasets available in a package. # lists datasets in dataskills data(package = &quot;dataskills&quot;) Type the name of a dataset into the console to see the data. Type ?smalldata into the console to see the dataset description. smalldata id group pre post S01 control 98.46606 106.70508 S02 control 104.39774 89.09030 S03 control 105.13377 123.67230 S04 control 92.42574 70.70178 S05 control 123.53268 124.95526 S06 exp 97.48676 101.61697 S07 exp 87.75594 126.30077 S08 exp 77.15375 72.31229 S09 exp 97.00283 108.80713 S10 exp 102.32338 113.74732 You can also use the data() function to load a dataset into your global environment. # loads smalldata into the environment data(&quot;smalldata&quot;) Always, always, always, look at your data once you’ve created or loaded a table. Also look at it after each step that transforms your table. There are three main ways to look at your tibble: print(), glimpse(), and View(). The print() method can be run explicitly, but is more commonly called by just typing the variable name on the blank line. The default is not to print the entire table, but just the first 10 rows. It’s rare to print your data in a script; that is something you usually are doing for a sanity check, and you should just do it in the console. Let’s look at the smalldata table that we made above. smalldata id group pre post S01 control 98.46606 106.70508 S02 control 104.39774 89.09030 S03 control 105.13377 123.67230 S04 control 92.42574 70.70178 S05 control 123.53268 124.95526 S06 exp 97.48676 101.61697 S07 exp 87.75594 126.30077 S08 exp 77.15375 72.31229 S09 exp 97.00283 108.80713 S10 exp 102.32338 113.74732 The function glimpse() gives a sideways version of the tibble. This is useful if the table is very wide and you can’t see all of the columns. It also tells you the data type of each column in angled brackets after each column name. We’ll learn about data types below. glimpse(smalldata) ## Rows: 10 ## Columns: 4 ## $ id &lt;chr&gt; &quot;S01&quot;, &quot;S02&quot;, &quot;S03&quot;, &quot;S04&quot;, &quot;S05&quot;, &quot;S06&quot;, &quot;S07&quot;, &quot;S08&quot;, &quot;S09&quot;, &quot;… ## $ group &lt;chr&gt; &quot;control&quot;, &quot;control&quot;, &quot;control&quot;, &quot;control&quot;, &quot;control&quot;, &quot;exp&quot;, &quot;e… ## $ pre &lt;dbl&gt; 98.46606, 104.39774, 105.13377, 92.42574, 123.53268, 97.48676, 8… ## $ post &lt;dbl&gt; 106.70508, 89.09030, 123.67230, 70.70178, 124.95526, 101.61697, … The other way to look at the table is a more graphical spreadsheet-like version given by View() (capital ‘V’). It can be useful in the console, but don’t ever put this one in a script because it will create an annoying pop-up window when the user runs it. Now you can click on smalldata in the environment pane to open it up in a viewer that looks a bit like Excel. You can get a quick summary of a dataset with the summary() function. summary(smalldata) ## id group pre post ## Length:10 Length:10 Min. : 77.15 Min. : 70.70 ## Class :character Class :character 1st Qu.: 93.57 1st Qu.: 92.22 ## Mode :character Mode :character Median : 97.98 Median :107.76 ## Mean : 98.57 Mean :103.79 ## 3rd Qu.:103.88 3rd Qu.:121.19 ## Max. :123.53 Max. :126.30 You can even do things like calculate the difference between the means of two columns. pre_mean &lt;- mean(smalldata$pre) post_mean &lt;- mean(smalldata$post) post_mean - pre_mean ## [1] 5.223055 2.4.2 Importing data Built-in data are nice for examples, but you’re probably more interested in your own data. There are many different types of files that you might work with when doing data analysis. These different file types are usually distinguished by the three letter extension following a period at the end of the file name. Here are some examples of different types of files and the functions you would use to read them in or write them out. Extension File Type Reading Writing .csv Comma-separated values readr::read_csv() readr::write_csv() .tsv, .txt Tab-separated values readr::read_tsv() readr::write_tsv() .xls, .xlsx Excel workbook readxl::read_excel() NA .sav, .mat, … Multiple types rio::import() NA The double colon means that the function on the right comes from the package on the left, so readr::read_csv() refers to the read_csv() function in the readr package, and readxl::read_excel() refers to the function read_excel() in the package readxl. The function rio::import() from the rio package will read almost any type of data file, including SPSS and Matlab. Check the help with ?rio::import to see a full list. You can get a directory of data files used in this class for tutorials and exercises with the following code, which will create a directory called “data” in your project directory. Alternatively, you can download a zip file of the datasets. dataskills::getdata() Probably the most common file type you will encounter is .csv (comma-separated values). As the name suggests, a CSV file distinguishes which values go with which variable by separating them with commas, and text values are sometimes enclosed in double quotes. The first line of a file usually provides the names of the variables. For example, here are the first few lines of a CSV containing personality scores: subj_id,O,C,E,A,N S01,4.428571429,4.5,3.333333333,5.142857143,1.625 S02,5.714285714,2.9,3.222222222,3,2.625 S03,5.142857143,2.8,6,3.571428571,2.5 S04,3.142857143,5.2,1.333333333,1.571428571,3.125 S05,5.428571429,4.4,2.444444444,4.714285714,1.625 ``` There are six variables in this dataset, and their names are given in the first line of the file: `subj_id`, `O`, `C`, `E`, `A`, and `N`. You can see that the values for each of these variables are given in order, separated by commas, on each subsequent line of the file. When you read in CSV files, it is best practice to use the `readr::read_csv()` function. The `readr` package is automatically loaded as part of the `tidyverse` package, which we will be using in almost every script. Note that you would normally want to store the result of the `read_csv()` function to an object, as so: ```r csv_data &lt;- read_csv(&quot;data/5factor.csv&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## subj_id = col_character(), ## O = col_double(), ## C = col_double(), ## E = col_double(), ## A = col_double(), ## N = col_double() ## ) The read_csv() and read_tsv() functions will give you some information about the data you just read in so you can check the column names and data types. For now, it’s enough to know that col_double() refers to columns with numbers and col_character() refers to columns with words. We’ll learn in the toroubleshooting section below how to fix it if the function guesses the wrong data type. tsv_data &lt;- read_tsv(&quot;data/5factor.txt&quot;) xls_data &lt;- readxl::read_xls(&quot;data/5factor.xls&quot;) # you can load sheets from excel files by name or number rep_data &lt;- readxl::read_xls(&quot;data/5factor.xls&quot;, sheet = &quot;replication&quot;) spss_data &lt;- rio::import(&quot;data/5factor.sav&quot;) Once loaded, you can view your data using the data viewer. In the upper right hand window of RStudio, under the Environment tab, you will see the object babynames listed. If you click on the View icon (), it will bring up a table view of the data you loaded in the top left pane of RStudio. This allows you to check that the data have been loaded in properly. You can close the tab when you’re done looking at it, it won’t remove the object. 2.4.3 Creating data If we are creating a data table from scratch, we can use the tibble::tibble() function, and type the data right in. The tibble package is part of the tidyverse package that we loaded at the start of this chapter. Let’s create a small table with the names of three Avatar characters and their bending type. The tibble() function takes arguments with the names that you want your columns to have. The values are vectors that list the column values in order. If you don’t know the value for one of the cells, you can enter NA, which we have to do for Sokka because he doesn’t have any bending ability. If all the values in the column are the same, you can just enter one value and it will be copied for each row. avatar &lt;- tibble( name = c(&quot;Katara&quot;, &quot;Toph&quot;, &quot;Sokka&quot;), bends = c(&quot;water&quot;, &quot;earth&quot;, NA), friendly = TRUE ) # print it avatar name bends friendly Katara water TRUE Toph earth TRUE Sokka NA TRUE 2.4.4 Writing Data If you have data that you want to save to a CSV file, use readr::write_csv(), as follows. write_csv(avatar, &quot;avatar.csv&quot;) This will save the data in CSV format to your working directory. Create a new table called family with the first name, last name, and age of your family members. Save it to a CSV file called “family.csv.” Clear the object from your environment by restarting R or with the code remove(family). Load the data back in and view it. We’ll be working with tabular data a lot in this class, but tabular data is made up of vectors, which group together data with the same basic data type. The following sections explain some of this terminology to help you understand the functions we’ll be learning to process and analyse data. 2.5 Basic data types Data can be numbers, words, true/false values or combinations of these. In order to understand some later concepts, it’s useful to have a basic understanding of data types in R: numeric, character, and logical There is also a specific data type called a factor, which will probably give you a headache sooner or later, but we can ignore it for now. 2.5.1 Numeric data All of the real numbers are numeric data types (imaginary numbers are “complex”). There are two types of numeric data, integer and double. Integers are the whole numbers, like -1, 0 and 1. Doubles are numbers that can have fractional amounts. If you just type a plain number such as 10, it is stored as a double, even if it doesn’t have a decimal point. If you want it to be an exact integer, use the L suffix (10L). If you ever want to know the data type of something, use the typeof function. typeof(10) # double typeof(10.0) # double typeof(10L) # integer typeof(10i) # complex ## [1] &quot;double&quot; ## [1] &quot;double&quot; ## [1] &quot;integer&quot; ## [1] &quot;complex&quot; If you want to know if something is numeric (a double or an integer), you can use the function is.numeric() and it will tell you if it is numeric (TRUE) or not (FALSE). is.numeric(10L) is.numeric(10.0) is.numeric(&quot;Not a number&quot;) ## [1] TRUE ## [1] TRUE ## [1] FALSE 2.5.2 Character data Character strings are any text between quotation marks. typeof(&quot;This is a character string&quot;) typeof(&#39;You can use double or single quotes&#39;) ## [1] &quot;character&quot; ## [1] &quot;character&quot; This can include quotes, but you have to escape it using a backslash to signal the the quote isn’t meant to be the end of the string. my_string &lt;- &quot;The instructor said, \\&quot;R is cool,\\&quot; and the class agreed.&quot; cat(my_string) # cat() prints the arguments ## The instructor said, &quot;R is cool,&quot; and the class agreed. 2.5.3 Logical Data Logical data (also sometimes called “boolean” values) is one of two values: true or false. In R, we always write them in uppercase: TRUE and FALSE. class(TRUE) class(FALSE) ## [1] &quot;logical&quot; ## [1] &quot;logical&quot; When you compare two values with an operator, such as checking to see if 10 is greater than 5, the resulting value is logical. is.logical(10 &gt; 5) ## [1] TRUE You might also see logical values abbreviated as T and F, or 0 and 1. This can cause some problems down the road, so we will always spell out the whole thing. What data types are these: 100 integer double character logical factor 100L integer double character logical factor \"100\" integer double character logical factor 100.0 integer double character logical factor -100L integer double character logical factor factor(100) integer double character logical factor TRUE integer double character logical factor \"TRUE\" integer double character logical factor FALSE integer double character logical factor 1 == 2 integer double character logical factor 2.6 Basic container types Individual data values can be grouped together into containers. The main types of containers we’ll work with are vectors, lists, and data tables. 2.6.1 Vectors A vector in R is like a vector in mathematics: a set of ordered elements. All of the elements in a vector must be of the same data type (numeric, character, logical). You can create a vector by enclosing the elements in the function c(). ## put information into a vector using c(...) c(1, 2, 3, 4) c(&quot;this&quot;, &quot;is&quot;, &quot;cool&quot;) 1:6 # shortcut to make a vector of all integers x:y ## [1] 1 2 3 4 ## [1] &quot;this&quot; &quot;is&quot; &quot;cool&quot; ## [1] 1 2 3 4 5 6 What happens when you mix types? What class is the variable mixed? mixed &lt;- c(2, &quot;good&quot;, 2L, &quot;b&quot;, TRUE) You can’t mix data types in a vector; all elements of the vector must be the same data type. If you mix them, R will “coerce” them so that they are all the same. If you mix doubles and integers, the integers will be changed to doubles. If you mix characters and numeric types, the numbers will be coerced to characters, so 10 would turn into “10.” 2.6.1.1 Selecting values from a vector If we wanted to pick specific values out of a vector by position, we can use square brackets (an extract operator, or []) after the vector. values &lt;- c(10, 20, 30, 40, 50) values[2] # selects the second value ## [1] 20 You can select more than one value from the vector by putting a vector of numbers inside the square brackets. For example, you can select the 18th, 19th, 20th, 21st, 4th, 9th and 15th letter from the built-in vector LETTERS (which gives all the uppercase letters in the Latin alphabet). word &lt;- c(18, 19, 20, 21, 4, 9, 15) LETTERS[word] ## [1] &quot;R&quot; &quot;S&quot; &quot;T&quot; &quot;U&quot; &quot;D&quot; &quot;I&quot; &quot;O&quot; Can you decode the secret message? secret &lt;- c(14, 5, 22, 5, 18, 7, 15, 14, 14, 1, 7, 9, 22, 5, 25, 15, 21, 21, 16) You can also create ‘named’ vectors, where each element has a name. For example: vec &lt;- c(first = 77.9, second = -13.2, third = 100.1) vec ## first second third ## 77.9 -13.2 100.1 We can then access elements by name using a character vector within the square brackets. We can put them in any order we want, and we can repeat elements: vec[c(&quot;third&quot;, &quot;second&quot;, &quot;second&quot;)] ## third second second ## 100.1 -13.2 -13.2 We can get the vector of names using the names() function, and we can set or change them using something like names(vec2) &lt;- c(“n1,” “n2,” “n3”). Another way to access elements is by using a logical vector within the square brackets. This will pull out the elements of the vector for which the corresponding element of the logical vector is TRUE. If the logical vector doesn’t have the same length as the original, it will repeat. You can find out how long a vector is using the length() function. length(LETTERS) LETTERS[c(TRUE, FALSE)] ## [1] 26 ## [1] &quot;A&quot; &quot;C&quot; &quot;E&quot; &quot;G&quot; &quot;I&quot; &quot;K&quot; &quot;M&quot; &quot;O&quot; &quot;Q&quot; &quot;S&quot; &quot;U&quot; &quot;W&quot; &quot;Y&quot; 2.6.1.2 Repeating Sequences Here are some useful tricks to save typing when creating vectors. In the command x:y the : operator would give you the sequence of number starting at x, and going to y in increments of 1. 1:10 15.3:20.5 0:-10 ## [1] 1 2 3 4 5 6 7 8 9 10 ## [1] 15.3 16.3 17.3 18.3 19.3 20.3 ## [1] 0 -1 -2 -3 -4 -5 -6 -7 -8 -9 -10 What if you want to create a sequence but with something other than integer steps? You can use the seq() function. Look at the examples below and work out what the arguments do. seq(from = -1, to = 1, by = 0.2) seq(0, 100, length.out = 11) seq(0, 10, along.with = LETTERS) ## [1] -1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0 ## [1] 0 10 20 30 40 50 60 70 80 90 100 ## [1] 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 4.0 4.4 4.8 5.2 5.6 ## [16] 6.0 6.4 6.8 7.2 7.6 8.0 8.4 8.8 9.2 9.6 10.0 What if you want to repeat a vector many times? You could either type it out (painful) or use the rep() function, which can repeat vectors in different ways. rep(0, 10) # ten zeroes rep(c(1L, 3L), times = 7) # alternating 1 and 3, 7 times rep(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), each = 2) # A to C, 2 times each ## [1] 0 0 0 0 0 0 0 0 0 0 ## [1] 1 3 1 3 1 3 1 3 1 3 1 3 1 3 ## [1] &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; The rep() function is useful to create a vector of logical values (TRUE/FALSE or 1/0) to select values from another vector. # Get subject IDs in the pattern Y Y N N ... subject_ids &lt;- 1:40 yynn &lt;- rep(c(TRUE, FALSE), each = 2, length.out = length(subject_ids)) subject_ids[yynn] ## [1] 1 2 5 6 9 10 13 14 17 18 21 22 25 26 29 30 33 34 37 38 2.6.1.3 Vectorized Operations R performs calculations on vectors in a special way. Let’s look at an example using \\(z\\)-scores. A \\(z\\)-score is a deviation score(a score minus a mean) divided by a standard deviation. Let’s say we have a set of four IQ scores. ## example IQ scores: mu = 100, sigma = 15 iq &lt;- c(86, 101, 127, 99) If we want to subtract the mean from these four scores, we just use the following code: iq - 100 ## [1] -14 1 27 -1 This subtracts 100 from each element of the vector. R automatically assumes that this is what you wanted to do; it is called a vectorized operation and it makes it possible to express operations more efficiently. To calculate \\(z\\)-scores we use the formula: \\(z = \\frac{X - \\mu}{\\sigma}\\) where X are the scores, \\(\\mu\\) is the mean, and \\(\\sigma\\) is the standard deviation. We can expression this formula in R as follows: ## z-scores (iq - 100) / 15 ## [1] -0.93333333 0.06666667 1.80000000 -0.06666667 You can see that it computed all four \\(z\\)-scores with a single line of code. In later chapters, we’ll use vectorised operations to process our data, such as reverse-scoring some questionnaire items. 2.6.2 Lists Recall that vectors can contain data of only one type. What if you want to store a collection of data of different data types? For that purpose you would use a list. Define a list using the list() function. data_types &lt;- list( double = 10.0, integer = 10L, character = &quot;10&quot;, logical = TRUE ) str(data_types) # str() prints lists in a condensed format ## List of 4 ## $ double : num 10 ## $ integer : int 10 ## $ character: chr &quot;10&quot; ## $ logical : logi TRUE You can refer to elements of a list using square brackets like a vector, but you can also use the dollar sign notation ($) if the list items have names. data_types$logical ## [1] TRUE Explore the 5 ways shown below to extract a value from a list. What data type is each object? What is the difference between the single and double brackets? Which one is the same as the dollar sign? bracket1 &lt;- data_types[1] bracket2 &lt;- data_types[[1]] name1 &lt;- data_types[&quot;double&quot;] name2 &lt;- data_types[[&quot;double&quot;]] dollar &lt;- data_types$double 2.6.3 Tables The built-in, imported, and created data above are tabular data, data arranged in the form of a table. Tabular data structures allow for a collection of data of different types (characters, integers, logical, etc.) but subject to the constraint that each “column” of the table (element of the list) must have the same number of elements. The base R version of a table is called a data.frame, while the ‘tidyverse’ version is called a tibble. Tibbles are far easier to work with, so we’ll be using those. To learn more about differences between these two data structures, see vignette(\"tibble\"). Tabular data becomes especially important for when we talk about tidy data in chapter 4, which consists of a set of simple principles for structuring data. 2.6.3.1 Creating a table We learned how to create a table by importing a Excel or CSV file, and creating a table from scratch using the tibble() function. You can also use the tibble::tribble() function to create a table by row, rather than by column. You start by listing the column names, each preceded by a tilde (~), then you list the values for each column, row by row, separated by commas (don’t forget a comma at the end of each row). This method can be easier for some data, but doesn’t let you use shortcuts, like setting all of the values in a column to the same value or a repeating sequence. # by column using tibble avatar_by_col &lt;- tibble( name = c(&quot;Katara&quot;, &quot;Toph&quot;, &quot;Sokka&quot;, &quot;Azula&quot;), bends = c(&quot;water&quot;, &quot;earth&quot;, NA, &quot;fire&quot;), friendly = rep(c(TRUE, FALSE), c(3, 1)) ) # by row using tribble avatar_by_row &lt;- tribble( ~name, ~bends, ~friendly, &quot;Katara&quot;, &quot;water&quot;, TRUE, &quot;Toph&quot;, &quot;earth&quot;, TRUE, &quot;Sokka&quot;, NA, TRUE, &quot;Azula&quot;, &quot;fire&quot;, FALSE ) 2.6.3.2 Table info We can get information about the table using the functions ncol() (number of columns), nrow() (number of rows), dim() (the number of rows and number of columns), and name() (the column names). nrow(avatar) # how many rows? ncol(avatar) # how many columns? dim(avatar) # what are the table dimensions? names(avatar) # what are the column names? ## [1] 3 ## [1] 3 ## [1] 3 3 ## [1] &quot;name&quot; &quot;bends&quot; &quot;friendly&quot; 2.6.3.3 Accessing rows and columns There are various ways of accessing specific columns or rows from a table. The ones below are from base R and are useful to know about, but you’ll be learning easier (and more readable) ways in the tidyr and dplyr lessons. Examples of these base R accessing functions are provided here for reference, since you might see them in other people’s scripts. katara &lt;- avatar[1, ] # first row type &lt;- avatar[, 2] # second column (bends) benders &lt;- avatar[c(1, 2), ] # selected rows (by number) bends_name &lt;- avatar[, c(&quot;bends&quot;, &quot;name&quot;)] # selected columns (by name) friendly &lt;- avatar$friendly # by column name 2.7 Troubleshooting What if you import some data and it guesses the wrong column type? The most common reason is that a numeric column has some non-numbers in it somewhere. Maybe someone wrote a note in an otherwise numeric column. Columns have to be all one data type, so if there are any characters, the whole column is converted to character strings, and numbers like 1.2 get represented as “1.2,” which will cause very weird errors like \"100\" &lt; \"9\" == TRUE. You can catch this by looking at the output from read_csv() or using glimpse() to check your data. The data directory you created with dataskills::getdata() contains a file called “mess.csv.” Let’s try loading this dataset. mess &lt;- read_csv(&quot;data/mess.csv&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## `This is my messy dataset` = col_character() ## ) ## Warning: 27 parsing failures. ## row col expected actual file ## 1 -- 1 columns 7 columns &#39;data/mess.csv&#39; ## 2 -- 1 columns 7 columns &#39;data/mess.csv&#39; ## 3 -- 1 columns 7 columns &#39;data/mess.csv&#39; ## 4 -- 1 columns 7 columns &#39;data/mess.csv&#39; ## 5 -- 1 columns 7 columns &#39;data/mess.csv&#39; ## ... ... ......... ......... ............... ## See problems(...) for more details. You’ll get a warning with many parsing errors and mess is just a single column of the word “junk.” View the file data/mess.csv by clicking on it in the File pane, and choosing “View File.” Here are the first 10 lines. What went wrong? This is my messy dataset junk,order,score,letter,good,min_max,date junk,1,-1,a,1,1 - 2,2020-01-1 junk,missing,0.72,b,1,2 - 3,2020-01-2 junk,3,-0.62,c,FALSE,3 - 4,2020-01-3 junk,4,2.03,d,T,4 - 5,2020-01-4 First, the file starts with a note: “This is my messy dataset.” We want to skip the first two lines. You can do this with the argument skip in read_csv(). mess &lt;- read_csv(&quot;data/mess.csv&quot;, skip = 2) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## junk = col_character(), ## order = col_character(), ## score = col_double(), ## letter = col_character(), ## good = col_character(), ## min_max = col_character(), ## date = col_character() ## ) mess junk order score letter good min_max date junk 1 -1.00 a 1 1 - 2 2020-01-1 junk missing 0.72 b 1 2 - 3 2020-01-2 junk 3 -0.62 c FALSE 3 - 4 2020-01-3 junk 4 2.03 d T 4 - 5 2020-01-4 junk 5 NA e 1 5 - 6 2020-01-5 junk 6 0.99 f 0 6 - 7 2020-01-6 junk 7 0.03 g T 7 - 8 2020-01-7 junk 8 0.67 h TRUE 8 - 9 2020-01-8 junk 9 0.57 i 1 9 - 10 2020-01-9 junk 10 0.90 j T 10 - 11 2020-01-10 junk 11 -1.55 k F 11 - 12 2020-01-11 junk 12 NA l FALSE 12 - 13 2020-01-12 junk 13 0.15 m T 13 - 14 2020-01-13 junk 14 -0.66 n TRUE 14 - 15 2020-01-14 junk 15 -0.99 o 1 15 - 16 2020-01-15 junk 16 1.97 p T 16 - 17 2020-01-16 junk 17 -0.44 q TRUE 17 - 18 2020-01-17 junk 18 -0.90 r F 18 - 19 2020-01-18 junk 19 -0.15 s FALSE 19 - 20 2020-01-19 junk 20 -0.83 t 0 20 - 21 2020-01-20 junk 21 1.99 u T 21 - 22 2020-01-21 junk 22 0.04 v F 22 - 23 2020-01-22 junk 23 -0.40 w F 23 - 24 2020-01-23 junk 24 -0.47 x 0 24 - 25 2020-01-24 junk 25 -0.41 y TRUE 25 - 26 2020-01-25 junk 26 0.68 z 0 26 - 27 2020-01-26 OK, that’s a little better, but this table is still a serious mess in several ways: junk is a column that we don’t need order should be an integer column good should be a logical column good uses all kinds of different ways to record TRUE and FALSE values min_max contains two pieces of numeric information, but is a character column date should be a date column We’ll learn how to deal with this mess in the chapters on tidy data and data wrangling, but we can fix a few things by setting the col_types argument in read_csv() to specify the column types for our two columns that were guessed wrong and skip the “junk” column. The argument col_types takes a list where the name of each item in the list is a column name and the value is from the table below. You can use the function, like col_double() or the abbreviation, like \"l\". Omitted column names are guessed. function abbreviation col_logical() l logical values col_integer() i integer values col_double() d numeric values col_character() c strings col_factor(levels, ordered) f a fixed set of values col_date(format = \"\") D with the locale’s date_format col_time(format = \"\") t with the locale’s time_format col_datetime(format = \"\") T ISO8601 date time col_number() n numbers containing the grouping_mark col_skip() _, - don’t import this column col_guess() ? parse using the “best” type based on the input # omitted values are guessed # ?col_date for format options ct &lt;- list( junk = &quot;-&quot;, # skip this column order = &quot;i&quot;, good = &quot;l&quot;, date = col_date(format = &quot;%Y-%m-%d&quot;) ) tidier &lt;- read_csv(&quot;data/mess.csv&quot;, skip = 2, col_types = ct) ## Warning: 1 parsing failure. ## row col expected actual file ## 2 order an integer missing &#39;data/mess.csv&#39; You will get a message about “1 parsing failure” when you run this. Warnings look scary at first, but always start by reading the message. The table tells you what row (2) and column (order) the error was found in, what kind of data was expected (integer), and what the actual value was (missing). If you specifically tell read_csv() to import a column as an integer, any characters in the column will produce a warning like this and then be recorded as NA. You can manually set what the missing values are recorded as with the na argument. tidiest &lt;- read_csv(&quot;data/mess.csv&quot;, skip = 2, na = &quot;missing&quot;, col_types = ct) Now order is an integer where “missing” is now NA, good is a logical value, where 0 and F are converted to FALSE and 1 and T are converted to TRUE, and date is a date type (adding leading zeros to the day). We’ll learn in later chapters how to fix the other problems. tidiest order score letter good min_max date 1 -1 a TRUE 1 - 2 2020-01-01 NA 0.72 b TRUE 2 - 3 2020-01-02 3 -0.62 c FALSE 3 - 4 2020-01-03 4 2.03 d TRUE 4 - 5 2020-01-04 5 NA e TRUE 5 - 6 2020-01-05 6 0.99 f FALSE 6 - 7 2020-01-06 7 0.03 g TRUE 7 - 8 2020-01-07 8 0.67 h TRUE 8 - 9 2020-01-08 9 0.57 i TRUE 9 - 10 2020-01-09 10 0.9 j TRUE 10 - 11 2020-01-10 11 -1.55 k FALSE 11 - 12 2020-01-11 12 NA l FALSE 12 - 13 2020-01-12 13 0.15 m TRUE 13 - 14 2020-01-13 14 -0.66 n TRUE 14 - 15 2020-01-14 15 -0.99 o TRUE 15 - 16 2020-01-15 16 1.97 p TRUE 16 - 17 2020-01-16 17 -0.44 q TRUE 17 - 18 2020-01-17 18 -0.9 r FALSE 18 - 19 2020-01-18 19 -0.15 s FALSE 19 - 20 2020-01-19 20 -0.83 t FALSE 20 - 21 2020-01-20 21 1.99 u TRUE 21 - 22 2020-01-21 22 0.04 v FALSE 22 - 23 2020-01-22 23 -0.4 w FALSE 23 - 24 2020-01-23 24 -0.47 x FALSE 24 - 25 2020-01-24 25 -0.41 y TRUE 25 - 26 2020-01-25 26 0.68 z FALSE 26 - 27 2020-01-26 2.8 Glossary term definition base r The set of R functions that come with a basic installation of R, before you add external packages character A data type representing strings of text. csv Comma-separated variable: a file type for representing data where each variable is separated from the next by a comma. data type The kind of data represented by an object. deviation score A score minus the mean double A data type representing a real decimal number escape Include special characters like \" inside of a string by prefacing them with a backslash. extension The end part of a file name that tells you what type of file it is (e.g., .R or .Rmd). extract operator A symbol used to get values from a container object, such as [, [[, or $ factor A data type where a specific set of values are stored with labels; An explanatory variable manipulated by the experimenter global environment The interactive workspace where your script runs integer A data type representing whole numbers. list A container data type that allows items with different data types to be grouped together. logical A data type representing TRUE or FALSE values. numeric A data type representing a real decimal number or integer. operator A symbol that performs a mathematical operation, such as +, -, *, / tabular data Data in a rectangular table format, where each row has an entry for each column. tidy data A format for data that maps the meaning onto the structure. tidyverse A set of R packages that help you create and work with tidy data vector A type of data structure that is basically a list of things like T/F values, numbers, or strings. vectorized An operator or function that acts on each element in a vector 2.9 Exercises Download the exercises. See the answers only after you’ve attempted all the questions. # run this to access the exercise dataskills::exercise(2) # run this to access the answers dataskills::exercise(2, answers = TRUE) "],["ggplot.html", "Chapter 3 Data Visualisation 3.1 Learning Objectives 3.2 Resources 3.3 Setup 3.4 Common Variable Combinations 3.5 Basic Plots 3.6 GGplots 3.7 Common Plot Types 3.8 Customisation 3.9 Combination Plots 3.10 Overlapping Discrete Data 3.11 Overlapping Continuous Data 3.12 Interactive Plots 3.13 Glossary 3.14 Exercises", " Chapter 3 Data Visualisation 3.1 Learning Objectives 3.1.1 Basic Understand what types of graphs are best for different types of data (video) 1 discrete 1 continuous 2 discrete 2 continuous 1 discrete, 1 continuous 3 continuous Create common types of graphs with ggplot2 (video) geom_bar() geom_density() geom_freqpoly() geom_histogram() geom_col() geom_boxplot() geom_violin() Vertical Intervals geom_crossbar() geom_errorbar() geom_linerange() geom_pointrange() geom_point() geom_smooth() Set custom labels, colours, and themes (video) Combine plots on the same plot, as facets, or as a grid using cowplot (video) Save plots as an image file (video) 3.1.2 Intermediate Add lines to graphs Deal with overlapping data Create less common types of graphs geom_tile() geom_density2d() geom_bin2d() geom_hex() geom_count() Adjust axes (e.g., flip coordinates, set axis limits) Create interactive graphs with plotly 3.2 Resources Chapter 3: Data Visualisation of R for Data Science ggplot2 cheat sheet Chapter 28: Graphics for communication of R for Data Science Look at Data from Data Vizualization for Social Science Hack Your Data Beautiful workshop by University of Glasgow postgraduate students Graphs in Cookbook for R ggplot2 documentation The R Graph Gallery (this is really useful) Top 50 ggplot2 Visualizations R Graphics Cookbook by Winston Chang ggplot extensions plotly for creating interactive graphs 3.3 Setup # libraries needed for these graphs library(tidyverse) library(dataskills) library(plotly) library(cowplot) set.seed(30250) # makes sure random numbers are reproducible 3.4 Common Variable Combinations Continuous variables are properties you can measure, like height. Discrete variables are things you can count, like the number of pets you have. Categorical variables can be nominal, where the categories don’t really have an order, like cats, dogs and ferrets (even though ferrets are obviously best). They can also be ordinal, where there is a clear order, but the distance between the categories isn’t something you could exactly equate, like points on a Likert rating scale. Different types of visualisations are good for different types of variables. Load the pets dataset from the dataskills package and explore it with glimpse(pets) or View(pets). This is a simulated dataset with one random factor (id), two categorical factors (pet, country) and three continuous variables (score, age, weight). data(&quot;pets&quot;) # if you don&#39;t have the dataskills package, use: # pets &lt;- read_csv(&quot;https://psyteachr.github.io/msc-data-skills/data/pets.csv&quot;, col_types = &quot;cffiid&quot;) glimpse(pets) ## Rows: 800 ## Columns: 6 ## $ id &lt;chr&gt; &quot;S001&quot;, &quot;S002&quot;, &quot;S003&quot;, &quot;S004&quot;, &quot;S005&quot;, &quot;S006&quot;, &quot;S007&quot;, &quot;S008&quot;… ## $ pet &lt;fct&gt; dog, dog, dog, dog, dog, dog, dog, dog, dog, dog, dog, dog, do… ## $ country &lt;fct&gt; UK, UK, UK, UK, UK, UK, UK, UK, UK, UK, UK, UK, UK, UK, UK, UK… ## $ score &lt;int&gt; 90, 107, 94, 120, 111, 110, 100, 107, 106, 109, 85, 110, 102, … ## $ age &lt;int&gt; 6, 8, 2, 10, 4, 8, 9, 8, 6, 11, 5, 9, 1, 10, 7, 8, 1, 8, 5, 13… ## $ weight &lt;dbl&gt; 19.78932, 20.01422, 19.14863, 19.56953, 21.39259, 21.31880, 19… Before you read ahead, come up with an example of each type of variable combination and sketch the types of graphs that would best display these data. 1 categorical 1 continuous 2 categorical 2 continuous 1 categorical, 1 continuous 3 continuous 3.5 Basic Plots R has some basic plotting functions, but they’re difficult to use and aesthetically not very nice. They can be useful to have a quick look at data while you’re working on a script, though. The function plot() usually defaults to a sensible type of plot, depending on whether the arguments x and y are categorical, continuous, or missing. plot(x = pets$pet) Figure 3.1: plot() with categorical x plot(x = pets$pet, y = pets$score) Figure 3.2: plot() with categorical x and continuous y plot(x = pets$age, y = pets$weight) Figure 3.3: plot() with continuous x and y The function hist() creates a quick histogram so you can see the distribution of your data. You can adjust how many columns are plotted with the argument breaks. hist(pets$score, breaks = 20) Figure 3.4: hist() 3.6 GGplots While the functions above are nice for quick visualisations, it’s hard to make pretty, publication-ready plots. The package ggplot2 (loaded with tidyverse) is one of the most common packages for creating beautiful visualisations. ggplot2 creates plots using a “grammar of graphics” where you add geoms in layers. It can be complex to understand, but it’s very powerful once you have a mental model of how it works. Let’s start with a totally empty plot layer created by the ggplot() function with no arguments. ggplot() Figure 3.5: A plot base created by ggplot() The first argument to ggplot() is the data table you want to plot. Let’s use the pets data we loaded above. The second argument is the mapping for which columns in your data table correspond to which properties of the plot, such as the x-axis, the y-axis, line colour or linetype, point shape, or object fill. These mappings are specified by the aes() function. Just adding this to the ggplot function creates the labels and ranges for the x and y axes. They usually have sensible default values, given your data, but we’ll learn how to change them later. mapping &lt;- aes(x = pet, y = score, colour = country, fill = country) ggplot(data = pets, mapping = mapping) Figure 3.6: Empty ggplot with x and y labels People usually omit the argument names and just put the aes() function directly as the second argument to ggplot. They also usually omit x and y as argument names to aes() (but you have to name the other properties). Next we can add “geoms,” or plot styles. You literally add them with the + symbol. You can also add other plot attributes, such as labels, or change the theme and base font size. ggplot(pets, aes(pet, score, colour = country, fill = country)) + geom_violin(alpha = 0.5) + labs(x = &quot;Pet type&quot;, y = &quot;Score on an Important Test&quot;, colour = &quot;Country of Origin&quot;, fill = &quot;Country of Origin&quot;, title = &quot;My first plot!&quot;) + theme_bw(base_size = 15) Figure 3.7: Violin plot with country represented by colour. 3.7 Common Plot Types There are many geoms, and they can take different arguments to customise their appearance. We’ll learn about some of the most common below. 3.7.1 Bar plot Bar plots are good for categorical data where you want to represent the count. ggplot(pets, aes(pet)) + geom_bar() Figure 3.8: Bar plot 3.7.2 Density plot Density plots are good for one continuous variable, but only if you have a fairly large number of observations. ggplot(pets, aes(score)) + geom_density() Figure 3.9: Density plot You can represent subsets of a variable by assigning the category variable to the argument group, fill, or color. ggplot(pets, aes(score, fill = pet)) + geom_density(alpha = 0.5) Figure 3.10: Grouped density plot Try changing the alpha argument to figure out what it does. 3.7.3 Frequency polygons If you want the y-axis to represent count rather than density, try geom_freqpoly(). ggplot(pets, aes(score, color = pet)) + geom_freqpoly(binwidth = 5) Figure 3.11: Frequency ploygon plot Try changing the binwidth argument to 10 and 1. How do you figure out the right value? 3.7.4 Histogram Histograms are also good for one continuous variable, and work well if you don’t have many observations. Set the binwidth to control how wide each bar is. ggplot(pets, aes(score)) + geom_histogram(binwidth = 5, fill = &quot;white&quot;, color = &quot;black&quot;) Figure 3.12: Histogram Histograms in ggplot look pretty bad unless you set the fill and color. If you show grouped histograms, you also probably want to change the default position argument. ggplot(pets, aes(score, fill=pet)) + geom_histogram(binwidth = 5, alpha = 0.5, position = &quot;dodge&quot;) Figure 3.13: Grouped Histogram Try changing the position argument to “identity,” “fill,” “dodge,” or “stack.” 3.7.5 Column plot Column plots are the worst way to represent grouped continuous data, but also one of the most common. If your data are already aggregated (e.g., you have rows for each group with columns for the mean and standard error), you can use geom_bar or geom_col and geom_errorbar directly. If not, you can use the function stat_summary to calculate the mean and standard error and send those numbers to the appropriate geom for plotting. ggplot(pets, aes(pet, score, fill=pet)) + stat_summary(fun = mean, geom = &quot;col&quot;, alpha = 0.5) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, width = 0.25) + coord_cartesian(ylim = c(80, 120)) Figure 3.14: Column plot Try changing the values for coord_cartesian. What does this do? 3.7.6 Boxplot Boxplots are great for representing the distribution of grouped continuous variables. They fix most of the problems with using bar/column plots for continuous data. ggplot(pets, aes(pet, score, fill=pet)) + geom_boxplot(alpha = 0.5) Figure 3.15: Box plot 3.7.7 Violin plot Violin pots are like sideways, mirrored density plots. They give even more information than a boxplot about distribution and are especially useful when you have non-normal distributions. ggplot(pets, aes(pet, score, fill=pet)) + geom_violin(draw_quantiles = .5, trim = FALSE, alpha = 0.5,) Figure 3.16: Violin plot Try changing the quantile argument. Set it to a vector of the numbers 0.1 to 0.9 in steps of 0.1. 3.7.8 Vertical intervals Boxplots and violin plots don’t always map well onto inferential stats that use the mean. You can represent the mean and standard error or any other value you can calculate. Here, we will create a table with the means and standard errors for two groups. We’ll learn how to calculate this from raw data in the chapter on data wrangling. We also create a new object called gg that sets up the base of the plot. dat &lt;- tibble( group = c(&quot;A&quot;, &quot;B&quot;), mean = c(10, 20), se = c(2, 3) ) gg &lt;- ggplot(dat, aes(group, mean, ymin = mean-se, ymax = mean+se)) The trick above can be useful if you want to represent the same data in different ways. You can add different geoms to the base plot without having to re-type the base plot code. gg + geom_crossbar() Figure 3.17: geom_crossbar() gg + geom_errorbar() Figure 3.18: geom_errorbar() gg + geom_linerange() Figure 3.19: geom_linerange() gg + geom_pointrange() Figure 3.20: geom_pointrange() You can also use the function stats_summary to calculate mean, standard error, or any other value for your data and display it using any geom. ggplot(pets, aes(pet, score, color=pet)) + stat_summary(fun.data = mean_se, geom = &quot;crossbar&quot;) + stat_summary(fun.min = function(x) mean(x) - sd(x), fun.max = function(x) mean(x) + sd(x), geom = &quot;errorbar&quot;, width = 0) + theme(legend.position = &quot;none&quot;) # gets rid of the legend Figure 3.21: Vertical intervals with stats_summary() 3.7.9 Scatter plot Scatter plots are a good way to represent the relationship between two continuous variables. ggplot(pets, aes(age, score, color = pet)) + geom_point() Figure 3.22: Scatter plot using geom_point() 3.7.10 Line graph You often want to represent the relationship as a single line. ggplot(pets, aes(age, score, color = pet)) + geom_smooth(formula = y ~ x, method=&quot;lm&quot;) Figure 3.23: Line plot using geom_smooth() What are some other options for the method argument to geom_smooth? When might you want to use them? You can plot functions other than the linear y ~ x. The code below creates a data table where x is 101 values between -10 and 10. and y is x squared plus 3*x plus 1. You’ll probably recognise this from algebra as the quadratic equation. You can set the formula argument in geom_smooth to a quadratic formula (y ~ x + I(x^2)) to fit a quadratic function to the data. quad &lt;- tibble( x = seq(-10, 10, length.out = 101), y = x^2 + 3*x + 1 ) ggplot(quad, aes(x, y)) + geom_point() + geom_smooth(formula = y ~ x + I(x^2), method=&quot;lm&quot;) Figure 3.24: Fitting quadratic functions 3.8 Customisation 3.8.1 Labels You can set custom titles and axis labels in a few different ways. ggplot(pets, aes(age, score, color = pet)) + geom_smooth(formula = y ~ x, method=&quot;lm&quot;) + labs(title = &quot;Pet score with Age&quot;, x = &quot;Age (in Years)&quot;, y = &quot;score Score&quot;, color = &quot;Pet Type&quot;) Figure 3.25: Set custom labels with labs() ggplot(pets, aes(age, score, color = pet)) + geom_smooth(formula = y ~ x, method=&quot;lm&quot;) + ggtitle(&quot;Pet score with Age&quot;) + xlab(&quot;Age (in Years)&quot;) + ylab(&quot;score Score&quot;) + scale_color_discrete(name = &quot;Pet Type&quot;) Figure 3.26: Set custom labels with individual functions 3.8.2 Colours You can set custom values for colour and fill using functions like scale_colour_manual() and scale_fill_manual(). The Colours chapter in Cookbook for R has many more ways to customise colour. ggplot(pets, aes(pet, score, colour = pet, fill = pet)) + geom_violin() + scale_color_manual(values = c(&quot;darkgreen&quot;, &quot;dodgerblue&quot;, &quot;orange&quot;)) + scale_fill_manual(values = c(&quot;#CCFFCC&quot;, &quot;#BBDDFF&quot;, &quot;#FFCC66&quot;)) Figure 3.27: Set custom colour 3.8.3 Themes GGplot comes with several additional themes and the ability to fully customise your theme. Type ?theme into the console to see the full list. Other packages such as cowplot also have custom themes. You can add a custom theme to the end of your ggplot object and specify a new base_size to make the default fonts and lines larger or smaller. ggplot(pets, aes(age, score, color = pet)) + geom_smooth(formula = y ~ x, method=&quot;lm&quot;) + theme_minimal(base_size = 18) Figure 3.28: Minimal theme with 18-point base font size It’s more complicated, but you can fully customise your theme with theme(). You can save this to an object and add it to the end of all of your plots to make the style consistent. Alternatively, you can set the theme at the top of a script with theme_set() and this will apply to all subsequent ggplot plots. vampire_theme &lt;- theme( rect = element_rect(fill = &quot;black&quot;), panel.background = element_rect(fill = &quot;black&quot;), text = element_text(size = 20, colour = &quot;white&quot;), axis.text = element_text(size = 16, colour = &quot;grey70&quot;), line = element_line(colour = &quot;white&quot;, size = 2), panel.grid = element_blank(), axis.line = element_line(colour = &quot;white&quot;), axis.ticks = element_blank(), legend.position = &quot;top&quot; ) theme_set(vampire_theme) ggplot(pets, aes(age, score, color = pet)) + geom_smooth(formula = y ~ x, method=&quot;lm&quot;) Figure 3.29: Custom theme 3.8.4 Save as file You can save a ggplot using ggsave(). It saves the last ggplot you made, by default, but you can specify which plot you want to save if you assigned that plot to a variable. You can set the width and height of your plot. The default units are inches, but you can change the units argument to “in,” “cm,” or “mm.” box &lt;- ggplot(pets, aes(pet, score, fill=pet)) + geom_boxplot(alpha = 0.5) violin &lt;- ggplot(pets, aes(pet, score, fill=pet)) + geom_violin(alpha = 0.5) ggsave(&quot;demog_violin_plot.png&quot;, width = 5, height = 7) ggsave(&quot;demog_box_plot.jpg&quot;, plot = box, width = 5, height = 7) The file type is set from the filename suffix, or by specifying the argument device, which can take the following values: “eps,” “ps,” “tex,” “pdf,” “jpeg,” “tiff,” “png,” “bmp,” “svg” or “wmf.” 3.9 Combination Plots 3.9.1 Violinbox plot A combination of a violin plot to show the shape of the distribution and a boxplot to show the median and interquartile ranges can be a very useful visualisation. ggplot(pets, aes(pet, score, fill = pet)) + geom_violin(show.legend = FALSE) + geom_boxplot(width = 0.2, fill = &quot;white&quot;, show.legend = FALSE) Figure 3.30: Violin-box plot Set the show.legend argument to FALSE to hide the legend. We do this here because the x-axis already labels the pet types. 3.9.2 Violin-point-range plot You can use stat_summary() to superimpose a point-range plot showning the mean ± 1 SD. You’ll learn how to write your own functions in the lesson on Iteration and Functions. ggplot(pets, aes(pet, score, fill=pet)) + geom_violin(trim = FALSE, alpha = 0.5) + stat_summary( fun = mean, fun.max = function(x) {mean(x) + sd(x)}, fun.min = function(x) {mean(x) - sd(x)}, geom=&quot;pointrange&quot; ) Figure 3.31: Point-range plot using stat_summary() 3.9.3 Violin-jitter plot If you don’t have a lot of data points, it’s good to represent them individually. You can use geom_jitter to do this. # sample_n chooses 50 random observations from the dataset ggplot(sample_n(pets, 50), aes(pet, score, fill=pet)) + geom_violin( trim = FALSE, draw_quantiles = c(0.25, 0.5, 0.75), alpha = 0.5 ) + geom_jitter( width = 0.15, # points spread out over 15% of available width height = 0, # do not move position on the y-axis alpha = 0.5, size = 3 ) Figure 3.32: Violin-jitter plot 3.9.4 Scatter-line graph If your graph isn’t too complicated, it’s good to also show the individual data points behind the line. ggplot(sample_n(pets, 50), aes(age, weight, colour = pet)) + geom_point() + geom_smooth(formula = y ~ x, method=&quot;lm&quot;) Figure 3.33: Scatter-line plot 3.9.5 Grid of plots You can use the cowplot package to easily make grids of different graphs. First, you have to assign each plot a name. Then you list all the plots as the first arguments of plot_grid() and provide a vector of labels. gg &lt;- ggplot(pets, aes(pet, score, colour = pet)) nolegend &lt;- theme(legend.position = 0) vp &lt;- gg + geom_violin(alpha = 0.5) + nolegend + ggtitle(&quot;Violin Plot&quot;) bp &lt;- gg + geom_boxplot(alpha = 0.5) + nolegend + ggtitle(&quot;Box Plot&quot;) cp &lt;- gg + stat_summary(fun = mean, geom = &quot;col&quot;, fill = &quot;white&quot;) + nolegend + ggtitle(&quot;Column Plot&quot;) dp &lt;- ggplot(pets, aes(score, colour = pet)) + geom_density() + nolegend + ggtitle(&quot;Density Plot&quot;) plot_grid(vp, bp, cp, dp, labels = LETTERS[1:4]) Figure 3.34: Grid of plots 3.10 Overlapping Discrete Data 3.10.1 Reducing Opacity You can deal with overlapping data points (very common if you’re using Likert scales) by reducing the opacity of the points. You need to use trial and error to adjust these so they look right. ggplot(pets, aes(age, score, colour = pet)) + geom_point(alpha = 0.25) + geom_smooth(formula = y ~ x, method=&quot;lm&quot;) Figure 3.35: Deal with overlapping data using transparency 3.10.2 Proportional Dot Plots Or you can set the size of the dot proportional to the number of overlapping observations using geom_count(). ggplot(pets, aes(age, score, colour = pet)) + geom_count() Figure 3.36: Deal with overlapping data using geom_count() Alternatively, you can transform your data (we will learn to do this in the data wrangling chapter) to create a count column and use the count to set the dot colour. pets %&gt;% group_by(age, score) %&gt;% summarise(count = n(), .groups = &quot;drop&quot;) %&gt;% ggplot(aes(age, score, color=count)) + geom_point(size = 2) + scale_color_viridis_c() Figure 3.37: Deal with overlapping data using dot colour The viridis package changes the colour themes to be easier to read by people with colourblindness and to print better in greyscale. Viridis is built into ggplot2 since v3.0.0. It uses scale_colour_viridis_c() and scale_fill_viridis_c() for continuous variables and scale_colour_viridis_d() and scale_fill_viridis_d() for discrete variables. 3.11 Overlapping Continuous Data Even if the variables are continuous, overplotting might obscure any relationships if you have lots of data. ggplot(pets, aes(age, score)) + geom_point() Figure 3.38: Overplotted data 3.11.1 2D Density Plot Use geom_density2d() to create a contour map. ggplot(pets, aes(age, score)) + geom_density2d() Figure 3.39: Contour map with geom_density2d() You can use stat_density_2d(aes(fill = ..level..), geom = \"polygon\") to create a heatmap-style density plot. ggplot(pets, aes(age, score)) + stat_density_2d(aes(fill = ..level..), geom = &quot;polygon&quot;) + scale_fill_viridis_c() Figure 3.40: Heatmap-density plot 3.11.2 2D Histogram Use geom_bin2d() to create a rectangular heatmap of bin counts. Set the binwidth to the x and y dimensions to capture in each box. ggplot(pets, aes(age, score)) + geom_bin2d(binwidth = c(1, 5)) Figure 3.41: Heatmap of bin counts 3.11.3 Hexagonal Heatmap Use geomhex() to create a hexagonal heatmap of bin counts. Adjust the binwidth, xlim(), ylim() and/or the figure dimensions to make the hexagons more or less stretched. ggplot(pets, aes(age, score)) + geom_hex(binwidth = c(1, 5)) Figure 3.42: Hexagonal heatmap of bin counts 3.11.4 Correlation Heatmap I’ve included the code for creating a correlation matrix from a table of variables, but you don’t need to understand how this is done yet. We’ll cover mutate() and gather() functions in the dplyr and tidyr lessons. heatmap &lt;- pets %&gt;% select_if(is.numeric) %&gt;% # get just the numeric columns cor() %&gt;% # create the correlation matrix as_tibble(rownames = &quot;V1&quot;) %&gt;% # make it a tibble gather(&quot;V2&quot;, &quot;r&quot;, 2:ncol(.)) # wide to long (V2) Once you have a correlation matrix in the correct (long) format, it’s easy to make a heatmap using geom_tile(). ggplot(heatmap, aes(V1, V2, fill=r)) + geom_tile() + scale_fill_viridis_c() Figure 3.43: Heatmap using geom_tile() 3.12 Interactive Plots You can use the plotly package to make interactive graphs. Just assign your ggplot to a variable and use the function ggplotly(). demog_plot &lt;- ggplot(pets, aes(age, score, fill=pet)) + geom_point() + geom_smooth(formula = y~x, method = lm) ggplotly(demog_plot) Figure 3.44: Interactive graph using plotly Hover over the data points above and click on the legend items. 3.13 Glossary term definition continuous Data that can take on any values between other existing values. discrete Data that can only take certain values, such as integers. geom The geometric style in which data are displayed, such as boxplot, density, or histogram. likert A rating scale with a small number of discrete points in order nominal Categorical variables that don’t have an inherent order, such as types of animal. ordinal Discrete variables that have an inherent order, such as number of legs 3.14 Exercises Download the exercises. See the plots to see what your plots should look like (this doesn’t contain the answer code). See the answers only after you’ve attempted all the questions. # run this to access the exercise dataskills::exercise(3) # run this to access the answers dataskills::exercise(3, answers = TRUE) "],["tidyr.html", "Chapter 4 Tidy Data 4.1 Learning Objectives 4.2 Resources 4.3 Setup 4.4 Tidy Data 4.5 Pivot Functions 4.6 Tidy Verbs 4.7 Pipes 4.8 More Complex Example 4.9 Glossary 4.10 Exercises", " Chapter 4 Tidy Data 4.1 Learning Objectives 4.1.1 Basic Understand the concept of tidy data (video) Be able to convert between long and wide formats using pivot functions (video) pivot_longer() pivot_wider() Be able to use the 4 basic tidyr verbs (video) gather() separate() spread() unite() Be able to chain functions using pipes (video) 4.1.2 Advanced Be able to use regular expressions to separate complex columns 4.2 Resources Tidy Data Chapter 12: Tidy Data in R for Data Science Chapter 18: Pipes in R for Data Science Data wrangling cheat sheet 4.3 Setup # libraries needed library(tidyverse) library(dataskills) set.seed(8675309) # makes sure random numbers are reproducible 4.4 Tidy Data 4.4.1 Three Rules Each variable must have its own column Each observation must have its own row Each value must have its own cell This table has three observations per row and the total_meanRT column contains two values. Table 4.1: Untidy table id score_1 score_2 score_3 rt_1 rt_2 rt_3 total_meanRT 1 4 3 7 857 890 859 14 (869) 2 3 1 1 902 900 959 5 (920) 3 2 5 4 757 823 901 11 (827) 4 6 2 6 844 788 624 14 (752) 5 1 7 2 659 764 690 10 (704) This is the tidy version. Table 4.1: Tidy table id trial rt score total mean_rt 1 1 857 4 14 869 1 2 890 3 14 869 1 3 859 7 14 869 2 1 902 3 5 920 2 2 900 1 5 920 2 3 959 1 5 920 3 1 757 2 11 827 3 2 823 5 11 827 3 3 901 4 11 827 4 1 844 6 14 752 4 2 788 2 14 752 4 3 624 6 14 752 5 1 659 1 10 704 5 2 764 7 10 704 5 3 690 2 10 704 4.4.2 Wide versus long Data tables can be in wide format or long format (and sometimes a mix of the two). Wide data are where all of the observations about one subject are in the same row, while long data are where each observation is on a separate row. You often need to convert between these formats to do different types of analyses or data processing. Imagine a study where each subject completes a questionnaire with three items. Each answer is an observation of that subject. You are probably most familiar with data like this in a wide format, where the subject id is in one column, and each of the three item responses is in its own column. Table 4.2: Wide data id Q1 Q2 Q3 A 1 2 3 B 4 5 6 The same data can be represented in a long format by creating a new column that specifies what item the observation is from and a new column that specifies the value of that observation. Table 4.3: Long data id item value A Q1 1 B Q1 4 A Q2 2 B Q2 5 A Q3 3 B Q3 6 Create a long version of the following table. id fav_colour fav_animal Lisa red echidna Robbie orange babirusa Steven green frog Answer Your answer doesn’t need to have the same column headers or be in the same order. id fav answer Lisa colour red Lisa animal echidna Robbie colour orange Robbie animal babirusa Steven colour green Steven animal frog 4.5 Pivot Functions The pivot functions allow you to transform a data table from wide to long or long to wide in one step. 4.5.1 Load Data We will used the dataset personality from the dataskills package (or download the data from personality.csv). These data are from a 5-factor (personality) personality questionnaire. Each question is labelled with the domain (Op = openness, Co = conscientiousness, Ex = extroversion, Ag = agreeableness, and Ne = neuroticism) and the question number. data(&quot;personality&quot;, package = &quot;dataskills&quot;) user_id date Op1 Ne1 Ne2 Op2 Ex1 Ex2 Co1 Co2 Ne3 Ag1 Ag2 Ne4 Ex3 Co3 Op3 Ex4 Op4 Ex5 Ag3 Co4 Co5 Ne5 Op5 Ag4 Op6 Co6 Ex6 Ne6 Co7 Ag5 Co8 Ex7 Ne7 Co9 Op7 Ne8 Ag6 Ag7 Co10 Ex8 Ex9 0 2006-03-23 3 4 0 6 3 3 3 3 0 2 1 3 3 2 2 1 3 3 1 3 0 3 6 1 0 6 3 1 3 3 3 3 NA 3 0 2 NA 3 1 2 4 1 2006-02-08 6 0 6 0 0 0 0 0 0 0 6 6 6 0 6 0 0 0 0 6 6 0 6 0 6 0 6 6 6 6 0 6 0 6 6 0 6 0 6 0 6 2 2005-10-24 6 0 6 0 0 0 0 0 0 0 6 6 5 1 5 1 1 1 1 5 5 1 5 1 5 1 5 5 5 5 1 5 1 5 5 1 5 1 5 1 5 5 2005-12-07 6 4 4 4 2 3 3 3 1 4 0 2 5 3 5 3 6 6 1 5 5 4 2 4 1 4 3 1 1 0 1 4 2 4 5 1 2 1 5 4 5 8 2006-07-27 6 1 2 6 2 3 5 4 0 6 5 3 3 4 5 3 6 3 0 5 5 1 5 6 6 6 0 0 3 2 3 1 0 3 5 1 3 1 3 3 5 108 2006-02-28 3 2 1 4 4 4 4 3 1 5 4 2 3 4 4 3 3 3 4 3 3 1 4 5 4 5 4 1 4 5 4 2 2 4 4 1 4 3 5 4 2 4.5.2 pivot_longer() pivot_longer() converts a wide data table to long format by converting the headers from specified columns into the values of new columns, and combining the values of those columns into a new condensed column. cols refers to the columns you want to make long You can refer to them by their names, like col1, col2, col3, col4 or col1:col4 or by their numbers, like 8, 9, 10 or 8:10. names_to is what you want to call the new columns that the gathered column headers will go into; it’s “domain” and “qnumber” in this example. names_sep is an optional argument if you have more than one value for names_to. It specifies the characters or position to split the values of the cols headers. values_to is what you want to call the values in the columns ...; they’re “score” in this example. personality_long &lt;- pivot_longer( data = personality, cols = Op1:Ex9, # columns to make long names_to = c(&quot;domain&quot;, &quot;qnumber&quot;), # new column names for headers names_sep = 2, # how to split the headers values_to = &quot;score&quot; # new column name for values ) %&gt;% glimpse() ## Rows: 615,000 ## Columns: 5 ## $ user_id &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ date &lt;date&gt; 2006-03-23, 2006-03-23, 2006-03-23, 2006-03-23, 2006-03-23, 2… ## $ domain &lt;chr&gt; &quot;Op&quot;, &quot;Ne&quot;, &quot;Ne&quot;, &quot;Op&quot;, &quot;Ex&quot;, &quot;Ex&quot;, &quot;Co&quot;, &quot;Co&quot;, &quot;Ne&quot;, &quot;Ag&quot;, &quot;A… ## $ qnumber &lt;chr&gt; &quot;1&quot;, &quot;1&quot;, &quot;2&quot;, &quot;2&quot;, &quot;1&quot;, &quot;2&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;1&quot;, &quot;2&quot;, &quot;4&quot;, &quot;3… ## $ score &lt;dbl&gt; 3, 4, 0, 6, 3, 3, 3, 3, 0, 2, 1, 3, 3, 2, 2, 1, 3, 3, 1, 3, 0,… You can pipe a data table to glimpse() at the end to have a quick look at it. It will still save to the object. What would you set names_sep to in order to split the cols headers listed below into the results? cols names_to names_sep A_1, A_2, B_1, B_2 c(\"condition\", \"version\") A B 1 2 _ A1, A2, B1, B2 c(\"condition\", \"version\") A B 1 2 _ cat-day&amp;pre, cat-day&amp;post, cat-night&amp;pre, cat-night&amp;post, dog-day&amp;pre, dog-day&amp;post, dog-night&amp;pre, dog-night&amp;post c(\"pet\", \"time\", \"condition\") - &amp; - 4.5.3 pivot_wider() We can also go from long to wide format using the pivot_wider() function. names_from is the columns that contain your new column headers. values_from is the column that contains the values for the new columns. names_sep is the character string used to join names if names_from is more than one column. personality_wide &lt;- pivot_wider( data = personality_long, names_from = c(domain, qnumber), values_from = score, names_sep = &quot;&quot; ) %&gt;% glimpse() ## Rows: 15,000 ## Columns: 43 ## $ user_id &lt;dbl&gt; 0, 1, 2, 5, 8, 108, 233, 298, 426, 436, 685, 807, 871, 881, 94… ## $ date &lt;date&gt; 2006-03-23, 2006-02-08, 2005-10-24, 2005-12-07, 2006-07-27, 2… ## $ Op1 &lt;dbl&gt; 3, 6, 6, 6, 6, 3, 3, 6, 6, 3, 4, 5, 5, 5, 6, 4, 1, 2, 5, 6, 4,… ## $ Ne1 &lt;dbl&gt; 4, 0, 0, 4, 1, 2, 3, 4, 0, 3, 3, 3, 2, 1, 1, 3, 4, 5, 2, 4, 5,… ## $ Ne2 &lt;dbl&gt; 0, 6, 6, 4, 2, 1, 2, 3, 1, 2, 5, 5, 3, 1, 1, 1, 1, 6, 1, 2, 5,… ## $ Op2 &lt;dbl&gt; 6, 0, 0, 4, 6, 4, 4, 0, 0, 3, 4, 3, 3, 4, 5, 3, 3, 4, 1, 6, 6,… ## $ Ex1 &lt;dbl&gt; 3, 0, 0, 2, 2, 4, 4, 3, 5, 4, 1, 1, 3, 3, 1, 3, 5, 1, 0, 4, 1,… ## $ Ex2 &lt;dbl&gt; 3, 0, 0, 3, 3, 4, 5, 2, 5, 3, 4, 1, 3, 2, 1, 6, 5, 3, 4, 4, 1,… ## $ Co1 &lt;dbl&gt; 3, 0, 0, 3, 5, 4, 3, 4, 5, 3, 3, 3, 1, 5, 5, 4, 4, 5, 6, 4, 2,… ## $ Co2 &lt;dbl&gt; 3, 0, 0, 3, 4, 3, 3, 4, 5, 3, 5, 3, 3, 4, 5, 1, 5, 4, 5, 2, 5,… ## $ Ne3 &lt;dbl&gt; 0, 0, 0, 1, 0, 1, 4, 4, 0, 4, 2, 5, 1, 2, 5, 5, 2, 2, 1, 2, 5,… ## $ Ag1 &lt;dbl&gt; 2, 0, 0, 4, 6, 5, 5, 4, 2, 5, 4, 3, 2, 4, 5, 3, 5, 5, 5, 4, 4,… ## $ Ag2 &lt;dbl&gt; 1, 6, 6, 0, 5, 4, 5, 3, 4, 3, 5, 1, 5, 4, 2, 6, 5, 5, 5, 5, 2,… ## $ Ne4 &lt;dbl&gt; 3, 6, 6, 2, 3, 2, 3, 3, 0, 4, 4, 5, 5, 4, 5, 3, 2, 5, 2, 4, 5,… ## $ Ex3 &lt;dbl&gt; 3, 6, 5, 5, 3, 3, 3, 0, 6, 1, 4, 2, 3, 2, 1, 2, 5, 1, 0, 5, 5,… ## $ Co3 &lt;dbl&gt; 2, 0, 1, 3, 4, 4, 5, 4, 5, 3, 4, 3, 4, 4, 5, 4, 2, 4, 5, 2, 2,… ## $ Op3 &lt;dbl&gt; 2, 6, 5, 5, 5, 4, 3, 2, 4, 3, 3, 6, 5, 5, 6, 5, 4, 4, 3, 6, 5,… ## $ Ex4 &lt;dbl&gt; 1, 0, 1, 3, 3, 3, 4, 3, 5, 3, 2, 0, 3, 3, 1, 2, NA, 4, 4, 4, 1… ## $ Op4 &lt;dbl&gt; 3, 0, 1, 6, 6, 3, 3, 0, 6, 3, 4, 5, 4, 5, 6, 6, 2, 2, 4, 5, 5,… ## $ Ex5 &lt;dbl&gt; 3, 0, 1, 6, 3, 3, 4, 2, 5, 2, 2, 4, 2, 3, 0, 4, 5, 2, 3, 1, 1,… ## $ Ag3 &lt;dbl&gt; 1, 0, 1, 1, 0, 4, 4, 4, 3, 3, 4, 4, 3, 4, 4, 5, 5, 4, 5, 3, 4,… ## $ Co4 &lt;dbl&gt; 3, 6, 5, 5, 5, 3, 2, 4, 3, 1, 4, 3, 1, 2, 4, 2, NA, 5, 6, 1, 1… ## $ Co5 &lt;dbl&gt; 0, 6, 5, 5, 5, 3, 3, 1, 5, 1, 2, 4, 4, 4, 2, 1, 6, 4, 3, 1, 3,… ## $ Ne5 &lt;dbl&gt; 3, 0, 1, 4, 1, 1, 4, 5, 0, 3, 4, 6, 2, 0, 1, 1, 0, 4, 3, 1, 5,… ## $ Op5 &lt;dbl&gt; 6, 6, 5, 2, 5, 4, 3, 2, 6, 6, 2, 4, 3, 4, 6, 6, 6, 5, 3, 3, 5,… ## $ Ag4 &lt;dbl&gt; 1, 0, 1, 4, 6, 5, 5, 6, 6, 6, 4, 2, 4, 5, 4, 5, 6, 4, 5, 6, 5,… ## $ Op6 &lt;dbl&gt; 0, 6, 5, 1, 6, 4, 6, 0, 0, 3, 5, 3, 5, 5, 5, 2, 5, 1, 1, 6, 2,… ## $ Co6 &lt;dbl&gt; 6, 0, 1, 4, 6, 5, 6, 5, 4, 3, 5, 5, 4, 6, 6, 1, 3, 4, 5, 4, 6,… ## $ Ex6 &lt;dbl&gt; 3, 6, 5, 3, 0, 4, 3, 1, 6, 3, 2, 1, 4, 2, 1, 5, 6, 2, 1, 2, 1,… ## $ Ne6 &lt;dbl&gt; 1, 6, 5, 1, 0, 1, 3, 4, 0, 4, 4, 5, 2, 1, 5, 6, 1, 2, 2, 3, 5,… ## $ Co7 &lt;dbl&gt; 3, 6, 5, 1, 3, 4, NA, 2, 3, 3, 2, 2, 4, 2, 5, 2, 5, 5, 3, 1, 1… ## $ Ag5 &lt;dbl&gt; 3, 6, 5, 0, 2, 5, 6, 2, 2, 3, 4, 1, 3, 5, 2, 6, 5, 6, 5, 3, 3,… ## $ Co8 &lt;dbl&gt; 3, 0, 1, 1, 3, 4, 3, 0, 1, 3, 2, 2, 1, 2, 4, 3, 2, 4, 5, 2, 6,… ## $ Ex7 &lt;dbl&gt; 3, 6, 5, 4, 1, 2, 5, 3, 6, 3, 4, 3, 5, 1, 1, 6, 6, 3, 1, 1, 3,… ## $ Ne7 &lt;dbl&gt; NA, 0, 1, 2, 0, 2, 4, 4, 0, 3, 2, 5, 1, 2, 5, 2, 2, 4, 1, 3, 5… ## $ Co9 &lt;dbl&gt; 3, 6, 5, 4, 3, 4, 5, 3, 5, 3, 4, 3, 4, 4, 2, 4, 6, 5, 5, 2, 2,… ## $ Op7 &lt;dbl&gt; 0, 6, 5, 5, 5, 4, 6, 2, 1, 3, 2, 4, 5, 5, 6, 3, 6, 5, 2, 6, 5,… ## $ Ne8 &lt;dbl&gt; 2, 0, 1, 1, 1, 1, 5, 4, 0, 4, 4, 5, 1, 2, 5, 2, 1, 5, 1, 2, 5,… ## $ Ag6 &lt;dbl&gt; NA, 6, 5, 2, 3, 4, 5, 6, 1, 3, 4, 2, 3, 5, 1, 6, 2, 6, 6, 5, 3… ## $ Ag7 &lt;dbl&gt; 3, 0, 1, 1, 1, 3, 3, 5, 0, 3, 2, 1, 2, 3, 5, 6, 4, 4, 6, 6, 2,… ## $ Co10 &lt;dbl&gt; 1, 6, 5, 5, 3, 5, 1, 2, 5, 2, 4, 3, 4, 4, 3, 2, 5, 5, 5, 2, 2,… ## $ Ex8 &lt;dbl&gt; 2, 0, 1, 4, 3, 4, 2, 4, 6, 2, 4, 0, 4, 4, 1, 3, 5, 4, 3, 1, 1,… ## $ Ex9 &lt;dbl&gt; 4, 6, 5, 5, 5, 2, 3, 3, 6, 3, 3, 4, 4, 3, 2, 5, 5, 4, 4, 0, 4,… 4.6 Tidy Verbs The pivot functions above are relatively new functions that combine the four basic tidy verbs. You can also convert data between long and wide formats using these functions. Many researchers still use these functions and older code will not use the pivot functions, so it is useful to know how to interpret these. 4.6.1 gather() Much like pivot_longer(), gather() makes a wide data table long by creating a column for the headers and a column for the values. The main difference is that you cannot turn the headers into more than one column. key is what you want to call the new column that the gathered column headers will go into; it’s “question” in this example. It is like names_to in pivot_longer(), but can only take one value (multiple values need to be separated after separate()). value is what you want to call the values in the gathered columns; they’re “score” in this example. It is like values_to in pivot_longer(). ... refers to the columns you want to gather. It is like cols in pivot_longer(). The gather() function converts personality from a wide data table to long format, with a row for each user/question observation. The resulting data table should have the columns: user_id, date, question, and score. personality_gathered &lt;- gather( data = personality, key = &quot;question&quot;, # new column name for gathered headers value = &quot;score&quot;, # new column name for gathered values Op1:Ex9 # columns to gather ) %&gt;% glimpse() ## Rows: 615,000 ## Columns: 4 ## $ user_id &lt;dbl&gt; 0, 1, 2, 5, 8, 108, 233, 298, 426, 436, 685, 807, 871, 881, 9… ## $ date &lt;date&gt; 2006-03-23, 2006-02-08, 2005-10-24, 2005-12-07, 2006-07-27, … ## $ question &lt;chr&gt; &quot;Op1&quot;, &quot;Op1&quot;, &quot;Op1&quot;, &quot;Op1&quot;, &quot;Op1&quot;, &quot;Op1&quot;, &quot;Op1&quot;, &quot;Op1&quot;, &quot;Op1&quot;… ## $ score &lt;dbl&gt; 3, 6, 6, 6, 6, 3, 3, 6, 6, 3, 4, 5, 5, 5, 6, 4, 1, 2, 5, 6, 4… 4.6.2 separate() col is the column you want to separate into is a vector of new column names sep is the character(s) that separate your new columns. This defaults to anything that isn’t alphanumeric, like .,_-/: and is like the names_sep argument in pivot_longer(). Split the question column into two columns: domain and qnumber. There is no character to split on, here, but you can separate a column after a specific number of characters by setting sep to an integer. For example, to split “abcde” after the third character, use sep = 3, which results in c(\"abc\", \"de\"). You can also use negative number to split before the nth character from the right. For example, to split a column that has words of various lengths and 2-digit suffixes (like “lisa03”“,”amanda38\"), you can use sep = -2. personality_sep &lt;- separate( data = personality_gathered, col = question, # column to separate into = c(&quot;domain&quot;, &quot;qnumber&quot;), # new column names sep = 2 # where to separate ) %&gt;% glimpse() ## Rows: 615,000 ## Columns: 5 ## $ user_id &lt;dbl&gt; 0, 1, 2, 5, 8, 108, 233, 298, 426, 436, 685, 807, 871, 881, 94… ## $ date &lt;date&gt; 2006-03-23, 2006-02-08, 2005-10-24, 2005-12-07, 2006-07-27, 2… ## $ domain &lt;chr&gt; &quot;Op&quot;, &quot;Op&quot;, &quot;Op&quot;, &quot;Op&quot;, &quot;Op&quot;, &quot;Op&quot;, &quot;Op&quot;, &quot;Op&quot;, &quot;Op&quot;, &quot;Op&quot;, &quot;O… ## $ qnumber &lt;chr&gt; &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1… ## $ score &lt;dbl&gt; 3, 6, 6, 6, 6, 3, 3, 6, 6, 3, 4, 5, 5, 5, 6, 4, 1, 2, 5, 6, 4,… If you want to separate just at full stops, you need to use sep = \"\\\\.\", not sep = \".\". The two slashes escape the full stop, making it interpreted as a literal full stop and not the regular expression for any character. 4.6.3 unite() col is your new united column ... refers to the columns you want to unite sep is the character(s) that will separate your united columns Put the domain and qnumber columns back together into a new column named domain_n. Make it in a format like “Op_Q1.” personality_unite &lt;- unite( data = personality_sep, col = &quot;domain_n&quot;, # new column name domain, qnumber, # columns to unite sep = &quot;_Q&quot; # separation characters ) %&gt;% glimpse() ## Rows: 615,000 ## Columns: 4 ## $ user_id &lt;dbl&gt; 0, 1, 2, 5, 8, 108, 233, 298, 426, 436, 685, 807, 871, 881, 9… ## $ date &lt;date&gt; 2006-03-23, 2006-02-08, 2005-10-24, 2005-12-07, 2006-07-27, … ## $ domain_n &lt;chr&gt; &quot;Op_Q1&quot;, &quot;Op_Q1&quot;, &quot;Op_Q1&quot;, &quot;Op_Q1&quot;, &quot;Op_Q1&quot;, &quot;Op_Q1&quot;, &quot;Op_Q1&quot;… ## $ score &lt;dbl&gt; 3, 6, 6, 6, 6, 3, 3, 6, 6, 3, 4, 5, 5, 5, 6, 4, 1, 2, 5, 6, 4… 4.6.4 spread() You can reverse the processes above, as well. For example, you can convert data from long format into wide format. key is the column that contains your new column headers. It is like names_from in pivot_wider(), but can only take one value (multiple values need to be merged first using unite()). value is the column that contains the values in the new spread columns. It is like values_from in pivot_wider(). personality_spread &lt;- spread( data = personality_unite, key = domain_n, # column that contains new headers value = score # column that contains new values ) %&gt;% glimpse() ## Rows: 15,000 ## Columns: 43 ## $ user_id &lt;dbl&gt; 0, 1, 2, 5, 8, 108, 233, 298, 426, 436, 685, 807, 871, 881, 94… ## $ date &lt;date&gt; 2006-03-23, 2006-02-08, 2005-10-24, 2005-12-07, 2006-07-27, 2… ## $ Ag_Q1 &lt;dbl&gt; 2, 0, 0, 4, 6, 5, 5, 4, 2, 5, 4, 3, 2, 4, 5, 3, 5, 5, 5, 4, 4,… ## $ Ag_Q2 &lt;dbl&gt; 1, 6, 6, 0, 5, 4, 5, 3, 4, 3, 5, 1, 5, 4, 2, 6, 5, 5, 5, 5, 2,… ## $ Ag_Q3 &lt;dbl&gt; 1, 0, 1, 1, 0, 4, 4, 4, 3, 3, 4, 4, 3, 4, 4, 5, 5, 4, 5, 3, 4,… ## $ Ag_Q4 &lt;dbl&gt; 1, 0, 1, 4, 6, 5, 5, 6, 6, 6, 4, 2, 4, 5, 4, 5, 6, 4, 5, 6, 5,… ## $ Ag_Q5 &lt;dbl&gt; 3, 6, 5, 0, 2, 5, 6, 2, 2, 3, 4, 1, 3, 5, 2, 6, 5, 6, 5, 3, 3,… ## $ Ag_Q6 &lt;dbl&gt; NA, 6, 5, 2, 3, 4, 5, 6, 1, 3, 4, 2, 3, 5, 1, 6, 2, 6, 6, 5, 3… ## $ Ag_Q7 &lt;dbl&gt; 3, 0, 1, 1, 1, 3, 3, 5, 0, 3, 2, 1, 2, 3, 5, 6, 4, 4, 6, 6, 2,… ## $ Co_Q1 &lt;dbl&gt; 3, 0, 0, 3, 5, 4, 3, 4, 5, 3, 3, 3, 1, 5, 5, 4, 4, 5, 6, 4, 2,… ## $ Co_Q10 &lt;dbl&gt; 1, 6, 5, 5, 3, 5, 1, 2, 5, 2, 4, 3, 4, 4, 3, 2, 5, 5, 5, 2, 2,… ## $ Co_Q2 &lt;dbl&gt; 3, 0, 0, 3, 4, 3, 3, 4, 5, 3, 5, 3, 3, 4, 5, 1, 5, 4, 5, 2, 5,… ## $ Co_Q3 &lt;dbl&gt; 2, 0, 1, 3, 4, 4, 5, 4, 5, 3, 4, 3, 4, 4, 5, 4, 2, 4, 5, 2, 2,… ## $ Co_Q4 &lt;dbl&gt; 3, 6, 5, 5, 5, 3, 2, 4, 3, 1, 4, 3, 1, 2, 4, 2, NA, 5, 6, 1, 1… ## $ Co_Q5 &lt;dbl&gt; 0, 6, 5, 5, 5, 3, 3, 1, 5, 1, 2, 4, 4, 4, 2, 1, 6, 4, 3, 1, 3,… ## $ Co_Q6 &lt;dbl&gt; 6, 0, 1, 4, 6, 5, 6, 5, 4, 3, 5, 5, 4, 6, 6, 1, 3, 4, 5, 4, 6,… ## $ Co_Q7 &lt;dbl&gt; 3, 6, 5, 1, 3, 4, NA, 2, 3, 3, 2, 2, 4, 2, 5, 2, 5, 5, 3, 1, 1… ## $ Co_Q8 &lt;dbl&gt; 3, 0, 1, 1, 3, 4, 3, 0, 1, 3, 2, 2, 1, 2, 4, 3, 2, 4, 5, 2, 6,… ## $ Co_Q9 &lt;dbl&gt; 3, 6, 5, 4, 3, 4, 5, 3, 5, 3, 4, 3, 4, 4, 2, 4, 6, 5, 5, 2, 2,… ## $ Ex_Q1 &lt;dbl&gt; 3, 0, 0, 2, 2, 4, 4, 3, 5, 4, 1, 1, 3, 3, 1, 3, 5, 1, 0, 4, 1,… ## $ Ex_Q2 &lt;dbl&gt; 3, 0, 0, 3, 3, 4, 5, 2, 5, 3, 4, 1, 3, 2, 1, 6, 5, 3, 4, 4, 1,… ## $ Ex_Q3 &lt;dbl&gt; 3, 6, 5, 5, 3, 3, 3, 0, 6, 1, 4, 2, 3, 2, 1, 2, 5, 1, 0, 5, 5,… ## $ Ex_Q4 &lt;dbl&gt; 1, 0, 1, 3, 3, 3, 4, 3, 5, 3, 2, 0, 3, 3, 1, 2, NA, 4, 4, 4, 1… ## $ Ex_Q5 &lt;dbl&gt; 3, 0, 1, 6, 3, 3, 4, 2, 5, 2, 2, 4, 2, 3, 0, 4, 5, 2, 3, 1, 1,… ## $ Ex_Q6 &lt;dbl&gt; 3, 6, 5, 3, 0, 4, 3, 1, 6, 3, 2, 1, 4, 2, 1, 5, 6, 2, 1, 2, 1,… ## $ Ex_Q7 &lt;dbl&gt; 3, 6, 5, 4, 1, 2, 5, 3, 6, 3, 4, 3, 5, 1, 1, 6, 6, 3, 1, 1, 3,… ## $ Ex_Q8 &lt;dbl&gt; 2, 0, 1, 4, 3, 4, 2, 4, 6, 2, 4, 0, 4, 4, 1, 3, 5, 4, 3, 1, 1,… ## $ Ex_Q9 &lt;dbl&gt; 4, 6, 5, 5, 5, 2, 3, 3, 6, 3, 3, 4, 4, 3, 2, 5, 5, 4, 4, 0, 4,… ## $ Ne_Q1 &lt;dbl&gt; 4, 0, 0, 4, 1, 2, 3, 4, 0, 3, 3, 3, 2, 1, 1, 3, 4, 5, 2, 4, 5,… ## $ Ne_Q2 &lt;dbl&gt; 0, 6, 6, 4, 2, 1, 2, 3, 1, 2, 5, 5, 3, 1, 1, 1, 1, 6, 1, 2, 5,… ## $ Ne_Q3 &lt;dbl&gt; 0, 0, 0, 1, 0, 1, 4, 4, 0, 4, 2, 5, 1, 2, 5, 5, 2, 2, 1, 2, 5,… ## $ Ne_Q4 &lt;dbl&gt; 3, 6, 6, 2, 3, 2, 3, 3, 0, 4, 4, 5, 5, 4, 5, 3, 2, 5, 2, 4, 5,… ## $ Ne_Q5 &lt;dbl&gt; 3, 0, 1, 4, 1, 1, 4, 5, 0, 3, 4, 6, 2, 0, 1, 1, 0, 4, 3, 1, 5,… ## $ Ne_Q6 &lt;dbl&gt; 1, 6, 5, 1, 0, 1, 3, 4, 0, 4, 4, 5, 2, 1, 5, 6, 1, 2, 2, 3, 5,… ## $ Ne_Q7 &lt;dbl&gt; NA, 0, 1, 2, 0, 2, 4, 4, 0, 3, 2, 5, 1, 2, 5, 2, 2, 4, 1, 3, 5… ## $ Ne_Q8 &lt;dbl&gt; 2, 0, 1, 1, 1, 1, 5, 4, 0, 4, 4, 5, 1, 2, 5, 2, 1, 5, 1, 2, 5,… ## $ Op_Q1 &lt;dbl&gt; 3, 6, 6, 6, 6, 3, 3, 6, 6, 3, 4, 5, 5, 5, 6, 4, 1, 2, 5, 6, 4,… ## $ Op_Q2 &lt;dbl&gt; 6, 0, 0, 4, 6, 4, 4, 0, 0, 3, 4, 3, 3, 4, 5, 3, 3, 4, 1, 6, 6,… ## $ Op_Q3 &lt;dbl&gt; 2, 6, 5, 5, 5, 4, 3, 2, 4, 3, 3, 6, 5, 5, 6, 5, 4, 4, 3, 6, 5,… ## $ Op_Q4 &lt;dbl&gt; 3, 0, 1, 6, 6, 3, 3, 0, 6, 3, 4, 5, 4, 5, 6, 6, 2, 2, 4, 5, 5,… ## $ Op_Q5 &lt;dbl&gt; 6, 6, 5, 2, 5, 4, 3, 2, 6, 6, 2, 4, 3, 4, 6, 6, 6, 5, 3, 3, 5,… ## $ Op_Q6 &lt;dbl&gt; 0, 6, 5, 1, 6, 4, 6, 0, 0, 3, 5, 3, 5, 5, 5, 2, 5, 1, 1, 6, 2,… ## $ Op_Q7 &lt;dbl&gt; 0, 6, 5, 5, 5, 4, 6, 2, 1, 3, 2, 4, 5, 5, 6, 3, 6, 5, 2, 6, 5,… 4.7 Pipes Pipes are a way to order your code in a more readable format. Let’s say you have a small data table with 10 participant IDs, two columns with variable type A, and 2 columns with variable type B. You want to calculate the mean of the A variables and the mean of the B variables and return a table with 10 rows (1 for each participant) and 3 columns (id, A_mean and B_mean). One way you could do this is by creating a new object at every step and using that object in the next step. This is pretty clear, but you’ve created 6 unnecessary data objects in your environment. This can get confusing in very long scripts. # make a data table with 10 subjects data_original &lt;- tibble( id = 1:10, A1 = rnorm(10, 0), A2 = rnorm(10, 1), B1 = rnorm(10, 2), B2 = rnorm(10, 3) ) # gather columns A1 to B2 into &quot;variable&quot; and &quot;value&quot; columns data_gathered &lt;- gather(data_original, variable, value, A1:B2) # separate the variable column at the _ into &quot;var&quot; and &quot;var_n&quot; columns data_separated &lt;- separate(data_gathered, variable, c(&quot;var&quot;, &quot;var_n&quot;), sep = 1) # group the data by id and var data_grouped &lt;- group_by(data_separated, id, var) # calculate the mean value for each id/var data_summarised &lt;- summarise(data_grouped, mean = mean(value), .groups = &quot;drop&quot;) # spread the mean column into A and B columns data_spread &lt;- spread(data_summarised, var, mean) # rename A and B to A_mean and B_mean data &lt;- rename(data_spread, A_mean = A, B_mean = B) data id A_mean B_mean 1 -0.5938256 1.0243046 2 0.7440623 2.7172046 3 0.9309275 3.9262358 4 0.7197686 1.9662632 5 -0.0280832 1.9473456 6 -0.0982555 3.2073687 7 0.1256922 0.9256321 8 1.4526447 2.3778116 9 0.2976443 1.6617481 10 0.5589199 2.1034679 You can name each object data and keep replacing the old data object with the new one at each step. This will keep your environment clean, but I don’t recommend it because it makes it too easy to accidentally run your code out of order when you are running line-by-line for development or debugging. One way to avoid extra objects is to nest your functions, literally replacing each data object with the code that generated it in the previous step. This can be fine for very short chains. mean_petal_width &lt;- round(mean(iris$Petal.Width), 2) But it gets extremely confusing for long chains: # do not ever do this!! data &lt;- rename( spread( summarise( group_by( separate( gather( tibble( id = 1:10, A1 = rnorm(10, 0), A2 = rnorm(10, 1), B1 = rnorm(10, 2), B2 = rnorm(10,3)), variable, value, A1:B2), variable, c(&quot;var&quot;, &quot;var_n&quot;), sep = 1), id, var), mean = mean(value), .groups = &quot;drop&quot;), var, mean), A_mean = A, B_mean = B) The pipe lets you “pipe” the result of each function into the next function, allowing you to put your code in a logical order without creating too many extra objects. # calculate mean of A and B variables for each participant data &lt;- tibble( id = 1:10, A1 = rnorm(10, 0), A2 = rnorm(10, 1), B1 = rnorm(10, 2), B2 = rnorm(10,3) ) %&gt;% gather(variable, value, A1:B2) %&gt;% separate(variable, c(&quot;var&quot;, &quot;var_n&quot;), sep=1) %&gt;% group_by(id, var) %&gt;% summarise(mean = mean(value), .groups = &quot;drop&quot;) %&gt;% spread(var, mean) %&gt;% rename(A_mean = A, B_mean = B) You can read this code from top to bottom as follows: Make a tibble called data with id of 1 to 10, A1 of 10 random numbers from a normal distribution, A2 of 10 random numbers from a normal distribution, B1 of 10 random numbers from a normal distribution, B2 of 10 random numbers from a normal distribution; and then Gather to create variable and value column from columns A_1 to B_2; and then Separate the column variable into 2 new columns called varand var_n, separate at character 1; and then Group by columns id and var; and then Summarise and new column called mean as the mean of the value column for each group and drop the grouping; and then Spread to make new columns with the key names in var and values in mean; and then Rename to make columns called A_mean (old A) and B_mean (old B) You can make intermediate objects whenever you need to break up your code because it’s getting too complicated or you need to debug something. You can debug a pipe by highlighting from the beginning to just before the pipe you want to stop at. Try this by highlighting from data &lt;- to the end of the separate function and typing cmd-return. What does data look like now? Chain all the steps above using pipes. personality_reshaped &lt;- personality %&gt;% gather(&quot;question&quot;, &quot;score&quot;, Op1:Ex9) %&gt;% separate(question, c(&quot;domain&quot;, &quot;qnumber&quot;), sep = 2) %&gt;% unite(&quot;domain_n&quot;, domain, qnumber, sep = &quot;_Q&quot;) %&gt;% spread(domain_n, score) 4.8 More Complex Example 4.8.1 Load Data Get data on infant and maternal mortality rates from the dataskills package. If you don’t have the package, you can download them here: infant mortality maternal mortality data(&quot;infmort&quot;, package = &quot;dataskills&quot;) head(infmort) Country Year Infant mortality rate (probability of dying between birth and age 1 per 1000 live births) Afghanistan 2015 66.3 [52.7-83.9] Afghanistan 2014 68.1 [55.7-83.6] Afghanistan 2013 69.9 [58.7-83.5] Afghanistan 2012 71.7 [61.6-83.7] Afghanistan 2011 73.4 [64.4-84.2] Afghanistan 2010 75.1 [66.9-85.1] data(&quot;matmort&quot;, package = &quot;dataskills&quot;) head(matmort) Country 1990 2000 2015 Afghanistan 1 340 [ 878 - 1 950] 1 100 [ 745 - 1 570] 396 [ 253 - 620] Albania 71 [ 58 - 88] 43 [ 33 - 56] 29 [ 16 - 46] Algeria 216 [ 141 - 327] 170 [ 118 - 241] 140 [ 82 - 244] Angola 1 160 [ 627 - 2 020] 924 [ 472 - 1 730] 477 [ 221 - 988] Argentina 72 [ 64 - 80] 60 [ 54 - 65] 52 [ 44 - 63] Armenia 58 [ 51 - 65] 40 [ 35 - 46] 25 [ 21 - 31] 4.8.2 Wide to Long matmort is in wide format, with a separate column for each year. Change it to long format, with a row for each Country/Year observation. This example is complicated because the column names to gather are numbers. If the column names are non-standard (e.g., have spaces, start with numbers, or have special characters), you can enclose them in backticks (`) like the example below. matmort_long &lt;- matmort %&gt;% pivot_longer(cols = `1990`:`2015`, names_to = &quot;Year&quot;, values_to = &quot;stats&quot;) %&gt;% glimpse() ## Rows: 543 ## Columns: 3 ## $ Country &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Albania&quot;, &quot;Alban… ## $ Year &lt;chr&gt; &quot;1990&quot;, &quot;2000&quot;, &quot;2015&quot;, &quot;1990&quot;, &quot;2000&quot;, &quot;2015&quot;, &quot;1990&quot;, &quot;2000&quot;… ## $ stats &lt;chr&gt; &quot;1 340 [ 878 - 1 950]&quot;, &quot;1 100 [ 745 - 1 570]&quot;, &quot;396 [ 253 - … You can put matmort at the first argument to pivot_longer(); you don’t have to pipe it in. But when I’m working on data processing I often find myself needing to insert or rearrange steps and I constantly introduce errors by forgetting to take the first argument out of a pipe chain, so now I start with the original data table and pipe from there. Alternatively, you can use the gather() function. matmort_long &lt;- matmort %&gt;% gather(&quot;Year&quot;, &quot;stats&quot;, `1990`:`2015`) %&gt;% glimpse() ## Rows: 543 ## Columns: 3 ## $ Country &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Albania&quot;, &quot;Algeria&quot;, &quot;Angola&quot;, &quot;Argentina&quot;, &quot;A… ## $ Year &lt;chr&gt; &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;… ## $ stats &lt;chr&gt; &quot;1 340 [ 878 - 1 950]&quot;, &quot;71 [ 58 - 88]&quot;, &quot;216 [ 141 - 327]&quot;,… 4.8.3 One Piece of Data per Column The data in the stats column is in an unusual format with some sort of confidence interval in brackets and lots of extra spaces. We don’t need any of the spaces, so first we’ll remove them with mutate(), which we’ll learn more about in the next lesson. The separate function will separate your data on anything that is not a number or letter, so try it first without specifying the sep argument. The into argument is a list of the new column names. matmort_split &lt;- matmort_long %&gt;% mutate(stats = gsub(&quot; &quot;, &quot;&quot;, stats)) %&gt;% separate(stats, c(&quot;rate&quot;, &quot;ci_low&quot;, &quot;ci_hi&quot;)) %&gt;% glimpse() ## Warning: Expected 3 pieces. Additional pieces discarded in 543 rows [1, 2, 3, 4, ## 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, ...]. ## Rows: 543 ## Columns: 5 ## $ Country &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Albania&quot;, &quot;Algeria&quot;, &quot;Angola&quot;, &quot;Argentina&quot;, &quot;A… ## $ Year &lt;chr&gt; &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;… ## $ rate &lt;chr&gt; &quot;1340&quot;, &quot;71&quot;, &quot;216&quot;, &quot;1160&quot;, &quot;72&quot;, &quot;58&quot;, &quot;8&quot;, &quot;8&quot;, &quot;64&quot;, &quot;46&quot;,… ## $ ci_low &lt;chr&gt; &quot;878&quot;, &quot;58&quot;, &quot;141&quot;, &quot;627&quot;, &quot;64&quot;, &quot;51&quot;, &quot;7&quot;, &quot;7&quot;, &quot;56&quot;, &quot;34&quot;, &quot;… ## $ ci_hi &lt;chr&gt; &quot;1950&quot;, &quot;88&quot;, &quot;327&quot;, &quot;2020&quot;, &quot;80&quot;, &quot;65&quot;, &quot;9&quot;, &quot;10&quot;, &quot;74&quot;, &quot;61&quot;… The gsub(pattern, replacement, x) function is a flexible way to do search and replace. The example above replaces all occurances of the pattern \" \" (a space), with the replacement \"\" (nothing), in the string x (the stats column). Use sub() instead if you only want to replace the first occurance of a pattern. We only used a simple pattern here, but you can use more complicated regex patterns to replace, for example, all even numbers (e.g., gsub(“[:02468:],” \"“,”id = 123456“)) or all occurances of the word colour in US or UK spelling (e.g., gsub(”colo(u)?r“,”**“,”replace color, colour, or colours, but not collors\")). 4.8.3.1 Handle spare columns with extra The previous example should have given you an error warning about “Additional pieces discarded in 543 rows.” This is because separate splits the column at the brackets and dashes, so the text 100[90-110] would split into four values c(“100,” “90,” “110,” \"“), but we only specified 3 new columns. The fourth value is always empty (just the part after the last bracket), so we are happy to drop it, but separate generates a warning so you don’t do that accidentally. You can turn off the warning by adding the extra argument and setting it to “drop.” Look at the help for ??tidyr::separate to see what the other options do. matmort_split &lt;- matmort_long %&gt;% mutate(stats = gsub(&quot; &quot;, &quot;&quot;, stats)) %&gt;% separate(stats, c(&quot;rate&quot;, &quot;ci_low&quot;, &quot;ci_hi&quot;), extra = &quot;drop&quot;) %&gt;% glimpse() ## Rows: 543 ## Columns: 5 ## $ Country &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Albania&quot;, &quot;Algeria&quot;, &quot;Angola&quot;, &quot;Argentina&quot;, &quot;A… ## $ Year &lt;chr&gt; &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;… ## $ rate &lt;chr&gt; &quot;1340&quot;, &quot;71&quot;, &quot;216&quot;, &quot;1160&quot;, &quot;72&quot;, &quot;58&quot;, &quot;8&quot;, &quot;8&quot;, &quot;64&quot;, &quot;46&quot;,… ## $ ci_low &lt;chr&gt; &quot;878&quot;, &quot;58&quot;, &quot;141&quot;, &quot;627&quot;, &quot;64&quot;, &quot;51&quot;, &quot;7&quot;, &quot;7&quot;, &quot;56&quot;, &quot;34&quot;, &quot;… ## $ ci_hi &lt;chr&gt; &quot;1950&quot;, &quot;88&quot;, &quot;327&quot;, &quot;2020&quot;, &quot;80&quot;, &quot;65&quot;, &quot;9&quot;, &quot;10&quot;, &quot;74&quot;, &quot;61&quot;… 4.8.3.2 Set delimiters with sep Now do the same with infmort. It’s already in long format, so you don’t need to use gather, but the third column has a ridiculously long name, so we can just refer to it by its column number (3). infmort_split &lt;- infmort %&gt;% separate(3, c(&quot;rate&quot;, &quot;ci_low&quot;, &quot;ci_hi&quot;), extra = &quot;drop&quot;) %&gt;% glimpse() ## Rows: 5,044 ## Columns: 5 ## $ Country &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;A… ## $ Year &lt;dbl&gt; 2015, 2014, 2013, 2012, 2011, 2010, 2009, 2008, 2007, 2006, 20… ## $ rate &lt;chr&gt; &quot;66&quot;, &quot;68&quot;, &quot;69&quot;, &quot;71&quot;, &quot;73&quot;, &quot;75&quot;, &quot;76&quot;, &quot;78&quot;, &quot;80&quot;, &quot;82&quot;, &quot;8… ## $ ci_low &lt;chr&gt; &quot;3&quot;, &quot;1&quot;, &quot;9&quot;, &quot;7&quot;, &quot;4&quot;, &quot;1&quot;, &quot;8&quot;, &quot;6&quot;, &quot;4&quot;, &quot;3&quot;, &quot;4&quot;, &quot;7&quot;, &quot;0… ## $ ci_hi &lt;chr&gt; &quot;52&quot;, &quot;55&quot;, &quot;58&quot;, &quot;61&quot;, &quot;64&quot;, &quot;66&quot;, &quot;69&quot;, &quot;71&quot;, &quot;73&quot;, &quot;75&quot;, &quot;7… Wait, that didn’t work at all! It split the column on spaces, brackets, and full stops. We just want to split on the spaces, brackets and dashes. So we need to manually set sep to what the delimiters are. Also, once there are more than a few arguments specified for a function, it’s easier to read them if you put one argument on each line. {#regex} You can use regular expressions to separate complex columns. Here, we want to separate on dashes and brackets. You can separate on a list of delimiters by putting them in parentheses, separated by “|.” It’s a little more complicated because brackets have a special meaning in regex, so you need to “escape” the left one with two backslashes “\\.” infmort_split &lt;- infmort %&gt;% separate( col = 3, into = c(&quot;rate&quot;, &quot;ci_low&quot;, &quot;ci_hi&quot;), extra = &quot;drop&quot;, sep = &quot;(\\\\[|-|])&quot; ) %&gt;% glimpse() ## Rows: 5,044 ## Columns: 5 ## $ Country &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;A… ## $ Year &lt;dbl&gt; 2015, 2014, 2013, 2012, 2011, 2010, 2009, 2008, 2007, 2006, 20… ## $ rate &lt;chr&gt; &quot;66.3 &quot;, &quot;68.1 &quot;, &quot;69.9 &quot;, &quot;71.7 &quot;, &quot;73.4 &quot;, &quot;75.1 &quot;, &quot;76.8 &quot;,… ## $ ci_low &lt;chr&gt; &quot;52.7&quot;, &quot;55.7&quot;, &quot;58.7&quot;, &quot;61.6&quot;, &quot;64.4&quot;, &quot;66.9&quot;, &quot;69.0&quot;, &quot;71.2&quot;… ## $ ci_hi &lt;chr&gt; &quot;83.9&quot;, &quot;83.6&quot;, &quot;83.5&quot;, &quot;83.7&quot;, &quot;84.2&quot;, &quot;85.1&quot;, &quot;86.1&quot;, &quot;87.3&quot;… 4.8.3.3 Fix data types with convert That’s better. Notice the next to Year, rate, ci_low and ci_hi. That means these columns hold characters (like words), not numbers or integers. This can cause problems when you try to do thigs like average the numbers (you can’t average words), so we can fix it by adding the argument convert and setting it to TRUE. infmort_split &lt;- infmort %&gt;% separate(col = 3, into = c(&quot;rate&quot;, &quot;ci_low&quot;, &quot;ci_hi&quot;), extra = &quot;drop&quot;, sep = &quot;(\\\\[|-|])&quot;, convert = TRUE) %&gt;% glimpse() ## Rows: 5,044 ## Columns: 5 ## $ Country &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;A… ## $ Year &lt;dbl&gt; 2015, 2014, 2013, 2012, 2011, 2010, 2009, 2008, 2007, 2006, 20… ## $ rate &lt;dbl&gt; 66.3, 68.1, 69.9, 71.7, 73.4, 75.1, 76.8, 78.6, 80.4, 82.3, 84… ## $ ci_low &lt;dbl&gt; 52.7, 55.7, 58.7, 61.6, 64.4, 66.9, 69.0, 71.2, 73.4, 75.5, 77… ## $ ci_hi &lt;dbl&gt; 83.9, 83.6, 83.5, 83.7, 84.2, 85.1, 86.1, 87.3, 88.9, 90.7, 92… Do the same for matmort. matmort_split &lt;- matmort_long %&gt;% mutate(stats = gsub(&quot; &quot;, &quot;&quot;, stats)) %&gt;% separate(col = stats, into = c(&quot;rate&quot;, &quot;ci_low&quot;, &quot;ci_hi&quot;), extra = &quot;drop&quot;, convert = TRUE) %&gt;% glimpse() ## Rows: 543 ## Columns: 5 ## $ Country &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Albania&quot;, &quot;Algeria&quot;, &quot;Angola&quot;, &quot;Argentina&quot;, &quot;A… ## $ Year &lt;chr&gt; &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;… ## $ rate &lt;int&gt; 1340, 71, 216, 1160, 72, 58, 8, 8, 64, 46, 26, 569, 58, 33, 9,… ## $ ci_low &lt;int&gt; 878, 58, 141, 627, 64, 51, 7, 7, 56, 34, 20, 446, 47, 28, 7, 4… ## $ ci_hi &lt;int&gt; 1950, 88, 327, 2020, 80, 65, 9, 10, 74, 61, 33, 715, 72, 38, 1… 4.8.4 All in one step We can chain all the steps for matmort above together, since we don’t need those intermediate data tables. matmort2&lt;- dataskills::matmort %&gt;% gather(&quot;Year&quot;, &quot;stats&quot;, `1990`:`2015`) %&gt;% mutate(stats = gsub(&quot; &quot;, &quot;&quot;, stats)) %&gt;% separate( col = stats, into = c(&quot;rate&quot;, &quot;ci_low&quot;, &quot;ci_hi&quot;), extra = &quot;drop&quot;, convert = TRUE ) %&gt;% glimpse() ## Rows: 543 ## Columns: 5 ## $ Country &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Albania&quot;, &quot;Algeria&quot;, &quot;Angola&quot;, &quot;Argentina&quot;, &quot;A… ## $ Year &lt;chr&gt; &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;, &quot;1990&quot;… ## $ rate &lt;int&gt; 1340, 71, 216, 1160, 72, 58, 8, 8, 64, 46, 26, 569, 58, 33, 9,… ## $ ci_low &lt;int&gt; 878, 58, 141, 627, 64, 51, 7, 7, 56, 34, 20, 446, 47, 28, 7, 4… ## $ ci_hi &lt;int&gt; 1950, 88, 327, 2020, 80, 65, 9, 10, 74, 61, 33, 715, 72, 38, 1… 4.8.5 Columns by Year Spread out the maternal mortality rate by year. matmort_wide &lt;- matmort2 %&gt;% spread(key = Year, value = rate) %&gt;% print() ## # A tibble: 542 x 6 ## Country ci_low ci_hi `1990` `2000` `2015` ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 253 620 NA NA 396 ## 2 Afghanistan 745 1570 NA 1100 NA ## 3 Afghanistan 878 1950 1340 NA NA ## 4 Albania 16 46 NA NA 29 ## 5 Albania 33 56 NA 43 NA ## 6 Albania 58 88 71 NA NA ## 7 Algeria 82 244 NA NA 140 ## 8 Algeria 118 241 NA 170 NA ## 9 Algeria 141 327 216 NA NA ## 10 Angola 221 988 NA NA 477 ## # … with 532 more rows Nope, that didn’t work at all, but it’s a really common mistake when spreading data. This is because spread matches on all the remaining columns, so Afghanistan with ci_low of 253 is treated as a different observation than Afghanistan with ci_low of 745. This is where pivot_wider() can be very useful. You can set values_from to multiple column names and their names will be added to the names_from values. matmort_wide &lt;- matmort2 %&gt;% pivot_wider( names_from = Year, values_from = c(rate, ci_low, ci_hi) ) glimpse(matmort_wide) ## Rows: 181 ## Columns: 10 ## $ Country &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Albania&quot;, &quot;Algeria&quot;, &quot;Angola&quot;, &quot;Argentina&quot;… ## $ rate_1990 &lt;int&gt; 1340, 71, 216, 1160, 72, 58, 8, 8, 64, 46, 26, 569, 58, 33… ## $ rate_2000 &lt;int&gt; 1100, 43, 170, 924, 60, 40, 9, 5, 48, 61, 21, 399, 48, 26,… ## $ rate_2015 &lt;int&gt; 396, 29, 140, 477, 52, 25, 6, 4, 25, 80, 15, 176, 27, 4, 7… ## $ ci_low_1990 &lt;int&gt; 878, 58, 141, 627, 64, 51, 7, 7, 56, 34, 20, 446, 47, 28, … ## $ ci_low_2000 &lt;int&gt; 745, 33, 118, 472, 54, 35, 8, 4, 42, 50, 18, 322, 38, 22, … ## $ ci_low_2015 &lt;int&gt; 253, 16, 82, 221, 44, 21, 5, 3, 17, 53, 12, 125, 19, 3, 5,… ## $ ci_hi_1990 &lt;int&gt; 1950, 88, 327, 2020, 80, 65, 9, 10, 74, 61, 33, 715, 72, 3… ## $ ci_hi_2000 &lt;int&gt; 1570, 56, 241, 1730, 65, 46, 10, 6, 55, 74, 26, 496, 58, 3… ## $ ci_hi_2015 &lt;int&gt; 620, 46, 244, 988, 63, 31, 7, 5, 35, 124, 19, 280, 37, 6, … 4.8.6 Experimentum Data Students in the Institute of Neuroscience and Psychology at the University of Glasgow can use the online experiment builder platform, Experimentum. The platform is also open source on github for anyone who can install it on a web server. It allows you to group questionnaires and experiments into projects with randomisation and counterbalancing. Data for questionnaires and experiments are downloadable in long format, but researchers often need to put them in wide format for analysis. Look at the help menu for built-in dataset dataskills::experimentum_quests to learn what each column is. Subjects are asked questions about dogs to test the different questionnaire response types. current: Do you own a dog? (yes/no) past: Have you ever owned a dog? (yes/no) name: What is the best name for a dog? (free short text) good: How good are dogs? (1=pretty good:7=very good) country: What country do borzois come from? good_borzoi: How good are borzois? (0=pretty good:100=very good) text: Write some text about dogs. (free long text) time: What time is it? (time) To get the dataset into wide format, where each question is in a separate column, use the following code: q &lt;- dataskills::experimentum_quests %&gt;% pivot_wider(id_cols = session_id:user_age, names_from = q_name, values_from = dv) %&gt;% type.convert(as.is = TRUE) %&gt;% print() ## # A tibble: 24 x 15 ## session_id project_id quest_id user_id user_sex user_status user_age current ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 34034 1 1 31105 female guest 28.2 1 ## 2 34104 1 1 31164 male registered 19.4 1 ## 3 34326 1 1 31392 female guest 17 0 ## 4 34343 1 1 31397 male guest 22 1 ## 5 34765 1 1 31770 female guest 44 1 ## 6 34796 1 1 31796 female guest 35.9 0 ## 7 34806 1 1 31798 female guest 35 0 ## 8 34822 1 1 31802 female guest 58 1 ## 9 34864 1 1 31820 male guest 20 0 ## 10 35014 1 1 31921 female student 39.2 1 ## # … with 14 more rows, and 7 more variables: past &lt;int&gt;, name &lt;chr&gt;, ## # good &lt;int&gt;, country &lt;chr&gt;, text &lt;chr&gt;, good_borzoi &lt;int&gt;, time &lt;chr&gt; The responses in the dv column have multiple types (e.g., r glossary(“integer”), r glossary(“double”), and r glossary(“character”)), but they are all represented as character strings when they’re in the same column. After you spread the data to wide format, each column should be given the ocrrect data type. The function type.convert() makes a best guess at what type each new column should be and converts it. The argument as.is = TRUE converts columns where none of the numbers have decimal places to integers. 4.9 Glossary term definition long Data where each observation is on a separate row observation All of the data about a single trial or question. value A single number or piece of data. variable A word that identifies and stores the value of some data for later use. wide Data where all of the observations about one subject are in the same row 4.10 Exercises Download the exercises. See the answers only after you’ve attempted all the questions. # run this to access the exercise dataskills::exercise(4) # run this to access the answers dataskills::exercise(4, answers = TRUE) "],["dplyr.html", "Chapter 5 Data Wrangling 5.1 Learning Objectives 5.2 Resources 5.3 Setup 5.4 Six main dplyr verbs 5.5 Additional dplyr one-table verbs 5.6 Window functions 5.7 Glossary 5.8 Exercises", " Chapter 5 Data Wrangling 5.1 Learning Objectives 5.1.1 Basic Be able to use the 6 main dplyr one-table verbs: (video) select() filter() arrange() mutate() summarise() group_by() Be able to wrangle data by chaining tidyr and dplyr functions (video) Be able to use these additional one-table verbs: (video) rename() distinct() count() slice() pull() 5.1.2 Intermediate Fine control of select() operations (video) Use window functions (video) 5.2 Resources Chapter 5: Data Transformation in R for Data Science Data transformation cheat sheet Chapter 16: Date and times in R for Data Science 5.3 Setup # libraries needed for these examples library(tidyverse) library(lubridate) library(dataskills) set.seed(8675309) # makes sure random numbers are reproducible 5.3.1 The disgust dataset These examples will use data from dataskills::disgust, which contains data from the Three Domain Disgust Scale. Each participant is identified by a unique user_id and each questionnaire completion has a unique id. Look at the Help for this dataset to see the individual questions. data(&quot;disgust&quot;, package = &quot;dataskills&quot;) #disgust &lt;- read_csv(&quot;https://psyteachr.github.io/msc-data-skills/data/disgust.csv&quot;) 5.4 Six main dplyr verbs Most of the data wrangling you’ll want to do with psychological data will involve the tidyr functions you learned in Chapter 4 and the six main dplyr verbs: select, filter, arrange, mutate, summarise, and group_by. 5.4.1 select() Select columns by name or number. You can select each column individually, separated by commas (e.g., col1, col2). You can also select all columns between two columns by separating them with a colon (e.g., start_col:end_col). moral &lt;- disgust %&gt;% select(user_id, moral1:moral7) names(moral) ## [1] &quot;user_id&quot; &quot;moral1&quot; &quot;moral2&quot; &quot;moral3&quot; &quot;moral4&quot; &quot;moral5&quot; &quot;moral6&quot; ## [8] &quot;moral7&quot; You can select columns by number, which is useful when the column names are long or complicated. sexual &lt;- disgust %&gt;% select(2, 11:17) names(sexual) ## [1] &quot;user_id&quot; &quot;sexual1&quot; &quot;sexual2&quot; &quot;sexual3&quot; &quot;sexual4&quot; &quot;sexual5&quot; &quot;sexual6&quot; ## [8] &quot;sexual7&quot; You can use a minus symbol to unselect columns, leaving all of the other columns. If you want to exclude a span of columns, put parentheses around the span first (e.g., -(moral1:moral7), not -moral1:moral7). pathogen &lt;- disgust %&gt;% select(-id, -date, -(moral1:sexual7)) names(pathogen) ## [1] &quot;user_id&quot; &quot;pathogen1&quot; &quot;pathogen2&quot; &quot;pathogen3&quot; &quot;pathogen4&quot; &quot;pathogen5&quot; ## [7] &quot;pathogen6&quot; &quot;pathogen7&quot; 5.4.1.1 Select helpers You can select columns based on criteria about the column names. 5.4.1.1.1 starts_with() Select columns that start with a character string. u &lt;- disgust %&gt;% select(starts_with(&quot;u&quot;)) names(u) ## [1] &quot;user_id&quot; 5.4.1.1.2 ends_with() Select columns that end with a character string. firstq &lt;- disgust %&gt;% select(ends_with(&quot;1&quot;)) names(firstq) ## [1] &quot;moral1&quot; &quot;sexual1&quot; &quot;pathogen1&quot; 5.4.1.1.3 contains() Select columns that contain a character string. pathogen &lt;- disgust %&gt;% select(contains(&quot;pathogen&quot;)) names(pathogen) ## [1] &quot;pathogen1&quot; &quot;pathogen2&quot; &quot;pathogen3&quot; &quot;pathogen4&quot; &quot;pathogen5&quot; &quot;pathogen6&quot; ## [7] &quot;pathogen7&quot; 5.4.1.1.4 num_range() Select columns with a name that matches the pattern prefix. moral2_4 &lt;- disgust %&gt;% select(num_range(&quot;moral&quot;, 2:4)) names(moral2_4) ## [1] &quot;moral2&quot; &quot;moral3&quot; &quot;moral4&quot; Use width to set the number of digits with leading zeros. For example, num_range(‘var_,’ 8:10, width=2) selects columns var_08, var_09, and var_10. 5.4.2 filter() Select rows by matching column criteria. Select all rows where the user_id is 1 (that’s Lisa). disgust %&gt;% filter(user_id == 1) id user_id date moral1 moral2 moral3 moral4 moral5 moral6 moral7 sexual1 sexual2 sexual3 sexual4 sexual5 sexual6 sexual7 pathogen1 pathogen2 pathogen3 pathogen4 pathogen5 pathogen6 pathogen7 1 1 2008-07-10 2 2 1 2 1 1 1 3 1 1 2 1 2 2 3 2 3 3 2 3 3 Remember to use == and not = to check if two things are equivalent. A single = assigns the righthand value to the lefthand variable and (usually) evaluates to TRUE. You can select on multiple criteria by separating them with commas. amoral &lt;- disgust %&gt;% filter( moral1 == 0, moral2 == 0, moral3 == 0, moral4 == 0, moral5 == 0, moral6 == 0, moral7 == 0 ) You can use the symbols &amp;, |, and ! to mean “and,” “or,” and “not.” You can also use other operators to make equations. # everyone who chose either 0 or 7 for question moral1 moral_extremes &lt;- disgust %&gt;% filter(moral1 == 0 | moral1 == 7) # everyone who chose the same answer for all moral questions moral_consistent &lt;- disgust %&gt;% filter( moral2 == moral1 &amp; moral3 == moral1 &amp; moral4 == moral1 &amp; moral5 == moral1 &amp; moral6 == moral1 &amp; moral7 == moral1 ) # everyone who did not answer 7 for all 7 moral questions moral_no_ceiling &lt;- disgust %&gt;% filter(moral1+moral2+moral3+moral4+moral5+moral6+moral7 != 7*7) 5.4.2.1 Match operator (%in%) Sometimes you need to exclude some participant IDs for reasons that can’t be described in code. The match operator (%in%) is useful here for testing if a column value is in a list. Surround the equation with parentheses and put ! in front to test that a value is not in the list. no_researchers &lt;- disgust %&gt;% filter(!(user_id %in% c(1,2))) 5.4.2.2 Dates You can use the lubridate package to work with dates. For example, you can use the year() function to return just the year from the date column and then select only data collected in 2010. disgust2010 &lt;- disgust %&gt;% filter(year(date) == 2010) Table 5.1: Rows 1-6 from disgust2010 id user_id date moral1 moral2 moral3 moral4 moral5 moral6 moral7 sexual1 sexual2 sexual3 sexual4 sexual5 sexual6 sexual7 pathogen1 pathogen2 pathogen3 pathogen4 pathogen5 pathogen6 pathogen7 6902 5469 2010-12-06 0 1 3 4 1 0 1 3 5 2 4 6 6 5 5 2 4 4 2 2 6 6158 6066 2010-04-18 4 5 6 5 5 4 4 3 0 1 6 3 5 3 6 5 5 5 5 5 5 6362 7129 2010-06-09 4 4 4 4 3 3 2 4 2 1 3 2 3 6 5 2 0 4 5 5 4 6302 39318 2010-05-20 2 4 1 4 5 6 0 1 0 0 1 0 0 1 3 2 3 2 3 2 4 5429 43029 2010-01-02 1 1 1 3 6 4 2 2 0 1 4 6 6 6 4 6 6 6 6 6 4 6732 71955 2010-10-15 2 5 3 6 3 2 5 4 3 3 6 6 6 5 4 2 6 5 6 6 3 Or select data from at least 5 years ago. You can use the range function to check the minimum and maximum dates in the resulting dataset. disgust_5ago &lt;- disgust %&gt;% filter(date &lt; today() - dyears(5)) range(disgust_5ago$date) ## [1] &quot;2008-07-10&quot; &quot;2016-08-04&quot; 5.4.3 arrange() Sort your dataset using arrange(). You will find yourself needing to sort data in R much less than you do in Excel, since you don’t need to have rows next to each other in order to, for example, calculate group means. But arrange() can be useful when preparing data from display in tables. disgust_order &lt;- disgust %&gt;% arrange(date, moral1) Table 5.2: Rows 1-6 from disgust_order id user_id date moral1 moral2 moral3 moral4 moral5 moral6 moral7 sexual1 sexual2 sexual3 sexual4 sexual5 sexual6 sexual7 pathogen1 pathogen2 pathogen3 pathogen4 pathogen5 pathogen6 pathogen7 1 1 2008-07-10 2 2 1 2 1 1 1 3 1 1 2 1 2 2 3 2 3 3 2 3 3 3 155324 2008-07-11 2 4 3 5 2 1 4 1 0 1 2 2 6 1 4 3 1 0 4 4 2 6 155386 2008-07-12 2 4 0 4 0 0 0 6 0 0 6 4 4 6 4 5 5 1 6 4 2 7 155409 2008-07-12 4 5 5 4 5 1 5 3 0 1 5 2 0 0 5 5 3 4 4 2 6 4 155366 2008-07-12 6 6 6 3 6 6 6 0 0 0 0 0 0 3 4 4 5 5 4 6 0 5 155370 2008-07-12 6 6 4 6 6 6 6 2 6 4 3 6 6 6 6 6 6 2 4 4 6 Reverse the order using desc() disgust_order_desc &lt;- disgust %&gt;% arrange(desc(date)) Table 5.3: Rows 1-6 from disgust_order_desc id user_id date moral1 moral2 moral3 moral4 moral5 moral6 moral7 sexual1 sexual2 sexual3 sexual4 sexual5 sexual6 sexual7 pathogen1 pathogen2 pathogen3 pathogen4 pathogen5 pathogen6 pathogen7 39456 356866 2017-08-21 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 39447 128727 2017-08-13 2 4 1 2 2 5 3 0 0 1 0 0 2 1 2 0 2 1 1 1 1 39371 152955 2017-06-13 6 6 3 6 6 6 6 1 0 0 2 1 4 4 5 0 5 4 3 6 3 39342 48303 2017-05-22 4 5 4 4 6 4 5 2 1 4 1 1 3 1 5 5 4 4 4 4 5 39159 151633 2017-04-04 4 5 6 5 3 6 2 6 4 0 4 0 3 6 4 4 6 6 6 6 4 38942 370464 2017-02-01 1 5 0 6 5 5 5 0 0 0 0 0 0 0 5 0 3 3 1 6 3 5.4.4 mutate() Add new columns. This is one of the most useful functions in the tidyverse. Refer to other columns by their names (unquoted). You can add more than one column in the same mutate function, just separate the columns with a comma. Once you make a new column, you can use it in further column definitions e.g., total below). disgust_total &lt;- disgust %&gt;% mutate( pathogen = pathogen1 + pathogen2 + pathogen3 + pathogen4 + pathogen5 + pathogen6 + pathogen7, moral = moral1 + moral2 + moral3 + moral4 + moral5 + moral6 + moral7, sexual = sexual1 + sexual2 + sexual3 + sexual4 + sexual5 + sexual6 + sexual7, total = pathogen + moral + sexual, user_id = paste0(&quot;U&quot;, user_id) ) Table 5.4: Rows 1-6 from disgust_total id user_id date moral1 moral2 moral3 moral4 moral5 moral6 moral7 sexual1 sexual2 sexual3 sexual4 sexual5 sexual6 sexual7 pathogen1 pathogen2 pathogen3 pathogen4 pathogen5 pathogen6 pathogen7 pathogen moral sexual total 1199 U0 2008-10-07 5 6 4 6 5 5 6 4 0 1 0 1 4 5 6 1 6 5 4 5 6 33 37 15 85 1 U1 2008-07-10 2 2 1 2 1 1 1 3 1 1 2 1 2 2 3 2 3 3 2 3 3 19 10 12 41 1599 U2 2008-10-27 1 1 1 1 NA NA 1 1 NA 1 NA 1 NA NA NA NA 1 NA NA NA NA NA NA NA NA 13332 U2118 2012-01-02 0 1 1 1 1 2 1 4 3 0 6 0 3 5 5 6 4 6 5 5 4 35 7 21 63 23 U2311 2008-07-15 4 4 4 4 4 4 4 2 1 2 1 1 1 5 5 5 4 4 5 4 3 30 28 13 71 1160 U3630 2008-10-06 1 5 NA 5 5 5 1 0 5 0 2 0 1 0 6 3 1 1 3 1 0 15 NA 8 NA You can overwrite a column by giving a new column the same name as the old column (see user_id) above. Make sure that you mean to do this and that you aren’t trying to use the old column value after you redefine it. 5.4.5 summarise() Create summary statistics for the dataset. Check the Data Wrangling Cheat Sheet or the Data Transformation Cheat Sheet for various summary functions. Some common ones are: mean(), sd(), n(), sum(), and quantile(). disgust_summary&lt;- disgust_total %&gt;% summarise( n = n(), q25 = quantile(total, .25, na.rm = TRUE), q50 = quantile(total, .50, na.rm = TRUE), q75 = quantile(total, .75, na.rm = TRUE), avg_total = mean(total, na.rm = TRUE), sd_total = sd(total, na.rm = TRUE), min_total = min(total, na.rm = TRUE), max_total = max(total, na.rm = TRUE) ) Table 5.5: All rows from disgust_summary n q25 q50 q75 avg_total sd_total min_total max_total 20000 59 71 83 70.6868 18.24253 0 126 5.4.6 group_by() Create subsets of the data. You can use this to create summaries, like the mean value for all of your experimental groups. Here, we’ll use mutate to create a new column called year, group by year, and calculate the average scores. disgust_groups &lt;- disgust_total %&gt;% mutate(year = year(date)) %&gt;% group_by(year) %&gt;% summarise( n = n(), avg_total = mean(total, na.rm = TRUE), sd_total = sd(total, na.rm = TRUE), min_total = min(total, na.rm = TRUE), max_total = max(total, na.rm = TRUE), .groups = &quot;drop&quot; ) Table 5.6: All rows from disgust_groups year n avg_total sd_total min_total max_total 2008 2578 70.29975 18.46251 0 126 2009 2580 69.74481 18.61959 3 126 2010 1514 70.59238 18.86846 6 126 2011 6046 71.34425 17.79446 0 126 2012 5938 70.42530 18.35782 0 126 2013 1251 71.59574 17.61375 0 126 2014 58 70.46296 17.23502 19 113 2015 21 74.26316 16.89787 43 107 2016 8 67.87500 32.62531 0 110 2017 6 57.16667 27.93862 21 90 If you don’t add .groups = “drop” at the end of the summarise() function, you will get the following message: “summarise() ungrouping output (override with .groups argument).” This just reminds you that the groups are still in effect and any further functions will also be grouped. Older versions of dplyr didn’t do this, so older code will generate this warning if you run it with newer version of dplyr. Older code might ungroup() after summarise() to indicate that groupings should be dropped. The default behaviour is usually correct, so you don’t need to worry, but it’s best to explicitly set .groups in a summarise() function after group_by() if you want to “keep” or “drop” the groupings. You can use filter after group_by. The following example returns the lowest total score from each year (i.e., the row where the rank() of the value in the column total is equivalent to 1). disgust_lowest &lt;- disgust_total %&gt;% mutate(year = year(date)) %&gt;% select(user_id, year, total) %&gt;% group_by(year) %&gt;% filter(rank(total) == 1) %&gt;% arrange(year) Table 5.7: All rows from disgust_lowest user_id year total U236585 2009 3 U292359 2010 6 U245384 2013 0 U206293 2014 19 U407089 2015 43 U453237 2016 0 U356866 2017 21 You can also use mutate after group_by. The following example calculates subject-mean-centered scores by grouping the scores by user_id and then subtracting the group-specific mean from each score. Note the use of gather to tidy the data into a long format first. disgust_smc &lt;- disgust %&gt;% gather(&quot;question&quot;, &quot;score&quot;, moral1:pathogen7) %&gt;% group_by(user_id) %&gt;% mutate(score_smc = score - mean(score, na.rm = TRUE)) %&gt;% ungroup() Use ungroup() as soon as you are done with grouped functions, otherwise the data table will still be grouped when you use it in the future. Table 5.8: Rows 1-6 from disgust_smc id user_id date question score score_smc 1199 0 2008-10-07 moral1 5 0.9523810 1 1 2008-07-10 moral1 2 0.0476190 1599 2 2008-10-27 moral1 1 0.0000000 13332 2118 2012-01-02 moral1 0 -3.0000000 23 2311 2008-07-15 moral1 4 0.6190476 1160 3630 2008-10-06 moral1 1 -1.2500000 5.4.7 All Together A lot of what we did above would be easier if the data were tidy, so let’s do that first. Then we can use group_by to calculate the domain scores. After that, we can spread out the 3 domains, calculate the total score, remove any rows with a missing (NA) total, and calculate mean values by year. disgust_tidy &lt;- dataskills::disgust %&gt;% gather(&quot;question&quot;, &quot;score&quot;, moral1:pathogen7) %&gt;% separate(question, c(&quot;domain&quot;,&quot;q_num&quot;), sep = -1) %&gt;% group_by(id, user_id, date, domain) %&gt;% summarise(score = mean(score), .groups = &quot;drop&quot;) Table 5.9: Rows 1-6 from disgust_tidy id user_id date domain score 1 1 2008-07-10 moral 1.428571 1 1 2008-07-10 pathogen 2.714286 1 1 2008-07-10 sexual 1.714286 3 155324 2008-07-11 moral 3.000000 3 155324 2008-07-11 pathogen 2.571429 3 155324 2008-07-11 sexual 1.857143 disgust_scored &lt;- disgust_tidy %&gt;% spread(domain, score) %&gt;% mutate( total = moral + sexual + pathogen, year = year(date) ) %&gt;% filter(!is.na(total)) %&gt;% arrange(user_id) Table 5.10: Rows 1-6 from disgust_scored id user_id date moral pathogen sexual total year 1199 0 2008-10-07 5.285714 4.714286 2.142857 12.142857 2008 1 1 2008-07-10 1.428571 2.714286 1.714286 5.857143 2008 13332 2118 2012-01-02 1.000000 5.000000 3.000000 9.000000 2012 23 2311 2008-07-15 4.000000 4.285714 1.857143 10.142857 2008 7980 4458 2011-09-05 3.428571 3.571429 3.000000 10.000000 2011 552 4651 2008-08-23 3.857143 4.857143 4.285714 13.000000 2008 disgust_summarised &lt;- disgust_scored %&gt;% group_by(year) %&gt;% summarise( n = n(), avg_pathogen = mean(pathogen), avg_moral = mean(moral), avg_sexual = mean(sexual), first_user = first(user_id), last_user = last(user_id), .groups = &quot;drop&quot; ) Table 5.11: Rows 1-6 from disgust_summarised year n avg_pathogen avg_moral avg_sexual first_user last_user 2008 2392 3.697265 3.806259 2.539298 0 188708 2009 2410 3.674333 3.760937 2.528275 6093 251959 2010 1418 3.731412 3.843139 2.510075 5469 319641 2011 5586 3.756918 3.806506 2.628612 4458 406569 2012 5375 3.740465 3.774591 2.545701 2118 458194 2013 1222 3.771920 3.906944 2.549100 7646 462428 2014 54 3.759259 4.000000 2.306878 11090 461307 2015 19 3.781955 4.451128 2.375940 102699 460283 2016 8 3.696429 3.625000 2.375000 4976 453237 2017 6 3.071429 3.690476 1.404762 48303 370464 5.5 Additional dplyr one-table verbs Use the code examples below and the help pages to figure out what the following one-table verbs do. Most have pretty self-explanatory names. 5.5.1 rename() You can rename columns with rename(). Set the argument name to the new name, and the value to the old name. You need to put a name in quotes or backticks if it doesn’t follow the rules for a good variable name (contains only letter, numbers, underscores, and full stops; and doesn’t start with a number). sw &lt;- starwars %&gt;% rename(Name = name, Height = height, Mass = mass, `Hair Colour` = hair_color, `Skin Colour` = skin_color, `Eye Colour` = eye_color, `Birth Year` = birth_year) names(sw) ## [1] &quot;Name&quot; &quot;Height&quot; &quot;Mass&quot; &quot;Hair Colour&quot; &quot;Skin Colour&quot; ## [6] &quot;Eye Colour&quot; &quot;Birth Year&quot; &quot;sex&quot; &quot;gender&quot; &quot;homeworld&quot; ## [11] &quot;species&quot; &quot;films&quot; &quot;vehicles&quot; &quot;starships&quot; Almost everyone gets confused at some point with rename() and tries to put the original names on the left and the new names on the right. Try it and see what the error message looks like. 5.5.2 distinct() Get rid of exactly duplicate rows with distinct(). This can be helpful if, for example, you are merging data from multiple computers and some of the data got copied from one computer to another, creating duplicate rows. # create a data table with duplicated values dupes &lt;- tibble( id = c( 1, 2, 1, 2, 1, 2), dv = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;A&quot;, &quot;B&quot;) ) distinct(dupes) id dv 1 A 2 B 1 C 2 D 5.5.3 count() The function count() is a quick shortcut for the common combination of group_by() and summarise() used to count the number of rows per group. starwars %&gt;% group_by(sex) %&gt;% summarise(n = n(), .groups = &quot;drop&quot;) sex n female 16 hermaphroditic 1 male 60 none 6 NA 4 count(starwars, sex) sex n female 16 hermaphroditic 1 male 60 none 6 NA 4 5.5.4 slice() slice(starwars, 1:3, 10) name height mass hair_color skin_color eye_color birth_year sex gender homeworld species films vehicles starships Luke Skywalker 172 77 blond fair blue 19 male masculine Tatooine Human The Empire Strikes Back, Revenge of the Sith , Return of the Jedi , A New Hope , The Force Awakens Snowspeeder , Imperial Speeder Bike X-wing , Imperial shuttle C-3PO 167 75 NA gold yellow 112 none masculine Tatooine Droid The Empire Strikes Back, Attack of the Clones , The Phantom Menace , Revenge of the Sith , Return of the Jedi , A New Hope R2-D2 96 32 NA white, blue red 33 none masculine Naboo Droid The Empire Strikes Back, Attack of the Clones , The Phantom Menace , Revenge of the Sith , Return of the Jedi , A New Hope , The Force Awakens Obi-Wan Kenobi 182 77 auburn, white fair blue-gray 57 male masculine Stewjon Human The Empire Strikes Back, Attack of the Clones , The Phantom Menace , Revenge of the Sith , Return of the Jedi , A New Hope Tribubble bongo Jedi starfighter , Trade Federation cruiser, Naboo star skiff , Jedi Interceptor , Belbullab-22 starfighter 5.5.5 pull() starwars %&gt;% filter(species == &quot;Droid&quot;) %&gt;% pull(name) ## [1] &quot;C-3PO&quot; &quot;R2-D2&quot; &quot;R5-D4&quot; &quot;IG-88&quot; &quot;R4-P17&quot; &quot;BB8&quot; 5.6 Window functions Window functions use the order of rows to calculate values. You can use them to do things that require ranking or ordering, like choose the top scores in each class, or accessing the previous and next rows, like calculating cumulative sums or means. The dplyr window functions vignette has very good detailed explanations of these functions, but we’ve described a few of the most useful ones below. 5.6.1 Ranking functions grades &lt;- tibble( id = 1:5, &quot;Data Skills&quot; = c(16, 17, 17, 19, 20), &quot;Statistics&quot; = c(14, 16, 18, 18, 19) ) %&gt;% gather(class, grade, 2:3) %&gt;% group_by(class) %&gt;% mutate(row_number = row_number(), rank = rank(grade), min_rank = min_rank(grade), dense_rank = dense_rank(grade), quartile = ntile(grade, 4), percentile = ntile(grade, 100)) Table 5.12: All rows from grades id class grade row_number rank min_rank dense_rank quartile percentile 1 Data Skills 16 1 1.0 1 1 1 1 2 Data Skills 17 2 2.5 2 2 1 2 3 Data Skills 17 3 2.5 2 2 2 3 4 Data Skills 19 4 4.0 4 3 3 4 5 Data Skills 20 5 5.0 5 4 4 5 1 Statistics 14 1 1.0 1 1 1 1 2 Statistics 16 2 2.0 2 2 1 2 3 Statistics 18 3 3.5 3 3 2 3 4 Statistics 18 4 3.5 3 3 3 4 5 Statistics 19 5 5.0 5 4 4 5 What are the differences among row_number(), rank(), min_rank(), dense_rank(), and ntile()? Why doesn’t row_number() need an argument? What would happen if you gave it the argument grade or class? What do you think would happen if you removed the group_by(class) line above? What if you added id to the grouping? What happens if you change the order of the rows? What does the second argument in ntile() do? You can use window functions to group your data into quantiles. sw_mass &lt;- starwars %&gt;% group_by(tertile = ntile(mass, 3)) %&gt;% summarise(min = min(mass), max = max(mass), mean = mean(mass), .groups = &quot;drop&quot;) Table 5.13: All rows from sw_mass tertile min max mean 1 15 68 45.6600 2 74 82 78.4100 3 83 1358 171.5789 NA NA NA NA Why is there a row of NA values? How would you get rid of them? 5.6.2 Offset functions The function lag() gives a previous row’s value. It defaults to 1 row back, but you can change that with the n argument. The function lead() gives values ahead of the current row. lag_lead &lt;- tibble(x = 1:6) %&gt;% mutate(lag = lag(x), lag2 = lag(x, n = 2), lead = lead(x, default = 0)) Table 5.14: All rows from lag_lead x lag lag2 lead 1 NA NA 2 2 1 NA 3 3 2 1 4 4 3 2 5 5 4 3 6 6 5 4 0 You can use offset functions to calculate change between trials or where a value changes. Use the order_by argument to specify the order of the rows. Alternatively, you can use arrange() before the offset functions. trials &lt;- tibble( trial = sample(1:10, 10), cond = sample(c(&quot;exp&quot;, &quot;ctrl&quot;), 10, T), score = rpois(10, 4) ) %&gt;% mutate( score_change = score - lag(score, order_by = trial), change_cond = cond != lag(cond, order_by = trial, default = &quot;no condition&quot;) ) %&gt;% arrange(trial) Table 5.15: All rows from trials trial cond score score_change change_cond 1 ctrl 8 NA TRUE 2 ctrl 4 -4 FALSE 3 exp 6 2 TRUE 4 ctrl 2 -4 TRUE 5 ctrl 3 1 FALSE 6 ctrl 6 3 FALSE 7 ctrl 2 -4 FALSE 8 exp 4 2 TRUE 9 ctrl 4 0 TRUE 10 exp 3 -1 TRUE Look at the help pages for lag() and lead(). What happens if you remove the order_by argument or change it to cond? What does the default argument do? Can you think of circumstances in your own data where you might need to use lag() or lead()? 5.6.3 Cumulative aggregates cumsum(), cummin(), and cummax() are base R functions for calculating cumulative means, minimums, and maximums. The dplyr package introduces cumany() and cumall(), which return TRUE if any or all of the previous values meet their criteria. cumulative &lt;- tibble( time = 1:10, obs = c(2, 2, 1, 2, 4, 3, 1, 0, 3, 5) ) %&gt;% mutate( cumsum = cumsum(obs), cummin = cummin(obs), cummax = cummax(obs), cumany = cumany(obs == 3), cumall = cumall(obs &lt; 4) ) Table 5.16: All rows from cumulative time obs cumsum cummin cummax cumany cumall 1 2 2 2 2 FALSE TRUE 2 2 4 2 2 FALSE TRUE 3 1 5 1 2 FALSE TRUE 4 2 7 1 2 FALSE TRUE 5 4 11 1 4 FALSE FALSE 6 3 14 1 4 TRUE FALSE 7 1 15 1 4 TRUE FALSE 8 0 15 0 4 TRUE FALSE 9 3 18 0 4 TRUE FALSE 10 5 23 0 5 TRUE FALSE What would happen if you change cumany(obs == 3) to cumany(obs &gt; 2)? What would happen if you change cumall(obs &lt; 4) to cumall(obs &lt; 2)? Can you think of circumstances in your own data where you might need to use cumany() or cumall()? 5.7 Glossary term definition data wrangling The process of preparing data for visualisation and statistical analysis. 5.8 Exercises Download the exercises. See the answers only after you’ve attempted all the questions. # run this to access the exercise dataskills::exercise(5) # run this to access the answers dataskills::exercise(5, answers = TRUE) "],["joins.html", "Chapter 6 Data Relations 6.1 Learning Objectives 6.2 Resources 6.3 Setup 6.4 Data 6.5 Mutating Joins 6.6 Filtering Joins 6.7 Binding Joins 6.8 Set Operations 6.9 Glossary 6.10 Exercises", " Chapter 6 Data Relations 6.1 Learning Objectives Be able to use the 4 mutating join verbs: (video) left_join() right_join() inner_join() full_join() Be able to use the 2 filtering join verbs: (video) semi_join() anti_join() Be able to use the 2 binding join verbs: (video) bind_rows() bind_cols() Be able to use the 3 set operations: (video) intersect() union() setdiff() 6.2 Resources Chapter 13: Relational Data in R for Data Science Cheatsheet for dplyr join functions Lecture slides on dplyr two-table verbs 6.3 Setup # libraries needed library(tidyverse) 6.4 Data First, we’ll create two small data tables. subject has id, gender and age for subjects 1-5. Age and gender are missing for subject 3. subject &lt;- tibble( id = 1:5, gender = c(&quot;m&quot;, &quot;m&quot;, NA, &quot;nb&quot;, &quot;f&quot;), age = c(19, 22, NA, 19, 18) ) id gender age 1 m 19 2 m 22 3 NA NA 4 nb 19 5 f 18 exp has subject id and the score from an experiment. Some subjects are missing, some completed twice, and some are not in the subject table. exp &lt;- tibble( id = c(2, 3, 4, 4, 5, 5, 6, 6, 7), score = c(10, 18, 21, 23, 9, 11, 11, 12, 3) ) id score 2 10 3 18 4 21 4 23 5 9 5 11 6 11 6 12 7 3 6.5 Mutating Joins Mutating joins act like the mutate() function in that they add new columns to one table based on values in another table. All the mutating joins have this basic syntax: ****_join(x, y, by = NULL, suffix = c(\".x\", \".y\") x = the first (left) table y = the second (right) table by = what columns to match on. If you leave this blank, it will match on all columns with the same names in the two tables. suffix = if columns have the same name in the two tables, but you aren’t joining by them, they get a suffix to make them unambiguous. This defaults to “.x” and “.y,” but you can change it to something more meaningful. You can leave out the by argument if you’re matching on all of the columns with the same name, but it’s good practice to always specify it so your code is robust to changes in the loaded data. 6.5.1 left_join() Figure 6.1: Left Join A left_join keeps all the data from the first (left) table and joins anything that matches from the second (right) table. If the right table has more than one match for a row in the right table, there will be more than one row in the joined table (see ids 4 and 5). left_join(subject, exp, by = &quot;id&quot;) id gender age score 1 m 19 NA 2 m 22 10 3 NA NA 18 4 nb 19 21 4 nb 19 23 5 f 18 9 5 f 18 11 Figure 6.2: Left Join (reversed) The order of tables is swapped here, so the result is all rows from the exp table joined to any matching rows from the subject table. left_join(exp, subject, by = &quot;id&quot;) id score gender age 2 10 m 22 3 18 NA NA 4 21 nb 19 4 23 nb 19 5 9 f 18 5 11 f 18 6 11 NA NA 6 12 NA NA 7 3 NA NA 6.5.2 right_join() Figure 6.3: Right Join A right_join keeps all the data from the second (right) table and joins anything that matches from the first (left) table. right_join(subject, exp, by = &quot;id&quot;) id gender age score 2 m 22 10 3 NA NA 18 4 nb 19 21 4 nb 19 23 5 f 18 9 5 f 18 11 6 NA NA 11 6 NA NA 12 7 NA NA 3 This table has the same information as left_join(exp, subject, by = \"id\"), but the columns are in a different order (left table, then right table). 6.5.3 inner_join() Figure 6.4: Inner Join An inner_join returns all the rows that have a match in the other table. inner_join(subject, exp, by = &quot;id&quot;) id gender age score 2 m 22 10 3 NA NA 18 4 nb 19 21 4 nb 19 23 5 f 18 9 5 f 18 11 6.5.4 full_join() Figure 6.5: Full Join A full_join lets you join up rows in two tables while keeping all of the information from both tables. If a row doesn’t have a match in the other table, the other table’s column values are set to NA. full_join(subject, exp, by = &quot;id&quot;) id gender age score 1 m 19 NA 2 m 22 10 3 NA NA 18 4 nb 19 21 4 nb 19 23 5 f 18 9 5 f 18 11 6 NA NA 11 6 NA NA 12 7 NA NA 3 6.6 Filtering Joins Filtering joins act like the filter() function in that they remove rows from the data in one table based on the values in another table. The result of a filtering join will only contain rows from the left table and have the same number or fewer rows than the left table. 6.6.1 semi_join() Figure 6.6: Semi Join A semi_join returns all rows from the left table where there are matching values in the right table, keeping just columns from the left table. semi_join(subject, exp, by = &quot;id&quot;) id gender age 2 m 22 3 NA NA 4 nb 19 5 f 18 Unlike an inner join, a semi join will never duplicate the rows in the left table if there is more than one matching row in the right table. Figure 6.7: Semi Join (Reversed) Order matters in a semi join. semi_join(exp, subject, by = &quot;id&quot;) id score 2 10 3 18 4 21 4 23 5 9 5 11 6.6.2 anti_join() Figure 6.8: Anti Join An anti_join return all rows from the left table where there are not matching values in the right table, keeping just columns from the left table. anti_join(subject, exp, by = &quot;id&quot;) id gender age 1 m 19 Figure 6.9: Anti Join (Reversed) Order matters in an anti join. anti_join(exp, subject, by = &quot;id&quot;) id score 6 11 6 12 7 3 6.7 Binding Joins Binding joins bind one table to another by adding their rows or columns together. 6.7.1 bind_rows() You can combine the rows of two tables with bind_rows. Here we’ll add subject data for subjects 6-9 and bind that to the original subject table. new_subjects &lt;- tibble( id = 6:9, gender = c(&quot;nb&quot;, &quot;m&quot;, &quot;f&quot;, &quot;f&quot;), age = c(19, 16, 20, 19) ) bind_rows(subject, new_subjects) id gender age 1 m 19 2 m 22 3 NA NA 4 nb 19 5 f 18 6 nb 19 7 m 16 8 f 20 9 f 19 The columns just have to have the same names, they don’t have to be in the same order. Any columns that differ between the two tables will just have NA values for entries from the other table. If a row is duplicated between the two tables (like id 5 below), the row will also be duplicated in the resulting table. If your tables have the exact same columns, you can use union() (see below) to avoid duplicates. new_subjects &lt;- tibble( id = 5:9, age = c(18, 19, 16, 20, 19), gender = c(&quot;f&quot;, &quot;nb&quot;, &quot;m&quot;, &quot;f&quot;, &quot;f&quot;), new = c(1,2,3,4,5) ) bind_rows(subject, new_subjects) id gender age new 1 m 19 NA 2 m 22 NA 3 NA NA NA 4 nb 19 NA 5 f 18 NA 5 f 18 1 6 nb 19 2 7 m 16 3 8 f 20 4 9 f 19 5 6.7.2 bind_cols() You can merge two tables with the same number of rows using bind_cols. This is only useful if the two tables have their rows in the exact same order. The only advantage over a left join is when the tables don’t have any IDs to join by and you have to rely solely on their order. new_info &lt;- tibble( colour = c(&quot;red&quot;, &quot;orange&quot;, &quot;yellow&quot;, &quot;green&quot;, &quot;blue&quot;) ) bind_cols(subject, new_info) id gender age colour 1 m 19 red 2 m 22 orange 3 NA NA yellow 4 nb 19 green 5 f 18 blue 6.8 Set Operations Set operations compare two tables and return rows that match (intersect), are in either table (union), or are in one table but not the other (setdiff). 6.8.1 intersect() intersect() returns all rows in two tables that match exactly. The columns don’t have to be in the same order. new_subjects &lt;- tibble( id = seq(4, 9), age = c(19, 18, 19, 16, 20, 19), gender = c(&quot;f&quot;, &quot;f&quot;, &quot;m&quot;, &quot;m&quot;, &quot;f&quot;, &quot;f&quot;) ) intersect(subject, new_subjects) id gender age 5 f 18 If you’ve forgotten to load dplyr or the tidyverse, base R also has an intersect() function. The error message can be confusing and looks something like this: base::intersect(subject, new_subjects) ## Error: Must subset rows with a valid subscript vector. ## ℹ Logical subscripts must match the size of the indexed input. ## x Input has size 6 but subscript `!duplicated(x, fromLast = fromLast, ...)` has size 0. 6.8.2 union() union() returns all the rows from both tables, removing duplicate rows. union(subject, new_subjects) id gender age 1 m 19 2 m 22 3 NA NA 4 nb 19 5 f 18 4 f 19 6 m 19 7 m 16 8 f 20 9 f 19 If you’ve forgotten to load dplyr or the tidyverse, base R also has a union() function. You usually won’t get an error message, but the output won’t be what you expect. base::union(subject, new_subjects) ## [[1]] ## [1] 1 2 3 4 5 ## ## [[2]] ## [1] &quot;m&quot; &quot;m&quot; NA &quot;nb&quot; &quot;f&quot; ## ## [[3]] ## [1] 19 22 NA 19 18 ## ## [[4]] ## [1] 4 5 6 7 8 9 ## ## [[5]] ## [1] 19 18 19 16 20 19 ## ## [[6]] ## [1] &quot;f&quot; &quot;f&quot; &quot;m&quot; &quot;m&quot; &quot;f&quot; &quot;f&quot; 6.8.3 setdiff() setdiff returns rows that are in the first table, but not in the second table. setdiff(subject, new_subjects) id gender age 1 m 19 2 m 22 3 NA NA 4 nb 19 Order matters for setdiff. setdiff(new_subjects, subject) id age gender 4 19 f 6 19 m 7 16 m 8 20 f 9 19 f If you’ve forgotten to load dplyr or the tidyverse, base R also has a setdiff() function. You usually won’t get an error message, but the output might not be what you expect because the base R setdiff() expects columns to be in the same order, so id 5 here registers as different between the two tables. base::setdiff(subject, new_subjects) id gender age 1 m 19 2 m 22 3 NA NA 4 nb 19 5 f 18 6.9 Glossary term definition base r The set of R functions that come with a basic installation of R, before you add external packages binding joins Joins that bind one table to another by adding their rows or columns together. filtering joins Joins that act like the dplyr::filter() function in that they remove rows from the data in one table based on the values in another table. mutating joins Joins that act like the dplyr::mutate() function in that they add new columns to one table based on values in another table. set operations Functions that compare two tables and return rows that match (intersect), are in either table (union), or are in one table but not the other (setdiff). 6.10 Exercises Download the exercises. See the answers only after you’ve attempted all the questions. # run this to access the exercise dataskills::exercise(6) # run this to access the answers dataskills::exercise(6, answers = TRUE) "],["func.html", "Chapter 7 Iteration &amp; Functions 7.1 Learning Objectives 7.2 Resources 7.3 Setup 7.4 Iteration functions 7.5 Custom functions 7.6 Iterating your own functions 7.7 Glossary 7.8 Exercises", " Chapter 7 Iteration &amp; Functions 7.1 Learning Objectives You will learn about functions and iteration by using simulation to calculate a power analysis for an independent samples t-test. 7.1.1 Basic Work with basic iteration functions rep, seq, replicate (video) Use map() and apply() functions (video) Write your own custom functions with function() (video) Set default values for the arguments in your functions 7.1.2 Intermediate Understand scope Use error handling and warnings in a function 7.1.3 Advanced The topics below are not (yet) covered in these materials, but they are directions for independent learning. Repeat commands having multiple arguments using purrr::map2_*() and purrr::pmap_*() Create nested data frames using dplyr::group_by() and tidyr::nest() Work with nested data frames in dplyr Capture and deal with errors using ‘adverb’ functions purrr::safely() and purrr::possibly() 7.2 Resources Chapters 19 and 21 of R for Data Science RStudio Apply Functions Cheatsheet In the next two lectures, we are going to learn more about iteration (doing the same commands over and over) and custom functions through a data simulation exercise, which will also lead us into more traditional statistical topics. 7.3 Setup # libraries needed for these examples library(tidyverse) ## contains purrr, tidyr, dplyr library(broom) ## converts test output to tidy tables set.seed(8675309) # makes sure random numbers are reproducible 7.4 Iteration functions We first learned about the two basic iteration functions, rep() and seq() in the Working with Data chapter. 7.4.1 rep() The function rep() lets you repeat the first argument a number of times. Use rep() to create a vector of alternating \"A\" and \"B\" values of length 24. rep(c(&quot;A&quot;, &quot;B&quot;), 12) ## [1] &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; ## [20] &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; If you don’t specify what the second argument is, it defaults to times, repeating the vector in the first argument that many times. Make the same vector as above, setting the second argument explicitly. rep(c(&quot;A&quot;, &quot;B&quot;), times = 12) ## [1] &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; ## [20] &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; If the second argument is a vector that is the same length as the first argument, each element in the first vector is repeated than many times. Use rep() to create a vector of 11 \"A\" values followed by 3 \"B\" values. rep(c(&quot;A&quot;, &quot;B&quot;), c(11, 3)) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; You can repeat each element of the vector a sepcified number of times using the each argument, Use rep() to create a vector of 12 \"A\" values followed by 12 \"B\" values. rep(c(&quot;A&quot;, &quot;B&quot;), each = 12) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; ## [20] &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; What do you think will happen if you set both times to 3 and each to 2? rep(c(&quot;A&quot;, &quot;B&quot;), times = 3, each = 2) ## [1] &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; 7.4.2 seq() The function seq() is useful for generating a sequence of numbers with some pattern. Use seq() to create a vector of the integers 0 to 10. seq(0, 10) ## [1] 0 1 2 3 4 5 6 7 8 9 10 You can set the by argument to count by numbers other than 1 (the default). Use seq() to create a vector of the numbers 0 to 100 by 10s. seq(0, 100, by = 10) ## [1] 0 10 20 30 40 50 60 70 80 90 100 The argument length.out is useful if you know how many steps you want to divide something into. Use seq() to create a vector that starts with 0, ends with 100, and has 12 equally spaced steps (hint: how many numbers would be in a vector with 2 steps?). seq(0, 100, length.out = 13) ## [1] 0.000000 8.333333 16.666667 25.000000 33.333333 41.666667 ## [7] 50.000000 58.333333 66.666667 75.000000 83.333333 91.666667 ## [13] 100.000000 7.4.3 replicate() You can use the replicate() function to run a function n times. For example, you can get 3 sets of 5 numbers from a random normal distribution by setting n to 3 and expr to rnorm(5). replicate(n = 3, expr = rnorm(5)) ## [,1] [,2] [,3] ## [1,] -0.9965824 0.98721974 -1.5495524 ## [2,] 0.7218241 0.02745393 1.0226378 ## [3,] -0.6172088 0.67287232 0.1500832 ## [4,] 2.0293916 0.57206650 -0.6599640 ## [5,] 1.0654161 0.90367770 -0.9945890 By default, replicate() simplifies your result into a matrix that is easy to convert into a table if your function returns vectors that are the same length. If you’d rather have a list of vectors, set simplify = FALSE. replicate(n = 3, expr = rnorm(5), simplify = FALSE) ## [[1]] ## [1] 1.9724587 -0.4418016 -0.9006372 -0.1505882 -0.8278942 ## ## [[2]] ## [1] 1.98582582 0.04400503 -0.40428231 -0.47299855 -0.41482324 ## ## [[3]] ## [1] 0.6832342 0.6902011 0.5334919 -0.1861048 0.3829458 7.4.4 map() and apply() functions purrr::map() and lapply() return a list of the same length as a vector or list, each element of which is the result of applying a function to the corresponding element. They function much the same, but purrr functions have some optimisations for working with the tidyverse. We’ll be working mostly with purrr functions in this course, but apply functions are very common in code that you might see in examples on the web. Imagine you want to calculate the power for a two-sample t-test with a mean difference of 0.2 and SD of 1, for all the sample sizes 100 to 1000 (by 100s). You could run the power.t.test() function 20 times and extract the values for “power” from the resulting list and put it in a table. p100 &lt;- power.t.test(n = 100, delta = 0.2, sd = 1, type=&quot;two.sample&quot;) # 18 more lines p1000 &lt;- power.t.test(n = 500, delta = 0.2, sd = 1, type=&quot;two.sample&quot;) tibble( n = c(100, &quot;...&quot;, 1000), power = c(p100$power, &quot;...&quot;, p1000$power) ) n power 100 0.290266404572217 … … 1000 0.884788352886661 However, the apply() and map() functions allow you to perform a function on each item in a vector or list. First make an object n that is the vector of the sample sizes you want to test, then use lapply() or map() to run the function power.t.test() on each item. You can set other arguments to power.t.test() after the function argument. n &lt;- seq(100, 1000, 100) pcalc &lt;- lapply(n, power.t.test, delta = 0.2, sd = 1, type=&quot;two.sample&quot;) # or pcalc &lt;- purrr::map(n, power.t.test, delta = 0.2, sd = 1, type=&quot;two.sample&quot;) These functions return a list where each item is the result of power.t.test(), which returns a list of results that includes the named item “power.” This is a special list that has a summary format if you just print it directly: pcalc[[1]] ## ## Two-sample t test power calculation ## ## n = 100 ## delta = 0.2 ## sd = 1 ## sig.level = 0.05 ## power = 0.2902664 ## alternative = two.sided ## ## NOTE: n is number in *each* group But you can see the individual items using the str() function. pcalc[[1]] %&gt;% str() ## List of 8 ## $ n : num 100 ## $ delta : num 0.2 ## $ sd : num 1 ## $ sig.level : num 0.05 ## $ power : num 0.29 ## $ alternative: chr &quot;two.sided&quot; ## $ note : chr &quot;n is number in *each* group&quot; ## $ method : chr &quot;Two-sample t test power calculation&quot; ## - attr(*, &quot;class&quot;)= chr &quot;power.htest&quot; sapply() is a version of lapply() that returns a vector or array instead of a list, where appropriate. The corresponding purrr functions are map_dbl(), map_chr(), map_int() and map_lgl(), which return vectors with the corresponding data type. You can extract a value from a list with the function [[. You usually see this written as pcalc[[1]], but if you put it inside backticks, you can use it in apply and map functions. sapply(pcalc, `[[`, &quot;power&quot;) ## [1] 0.2902664 0.5140434 0.6863712 0.8064964 0.8847884 0.9333687 0.9623901 ## [8] 0.9792066 0.9887083 0.9939638 We use map_dbl() here because the value for “power” is a double. purrr::map_dbl(pcalc, `[[`, &quot;power&quot;) ## [1] 0.2902664 0.5140434 0.6863712 0.8064964 0.8847884 0.9333687 0.9623901 ## [8] 0.9792066 0.9887083 0.9939638 We can use the map() functions inside a mutate() function to run the power.t.test() function on the value of n from each row of a table, then extract the value for “power,” and delete the column with the power calculations. mypower &lt;- tibble( n = seq(100, 1000, 100)) %&gt;% mutate(pcalc = purrr::map(n, power.t.test, delta = 0.2, sd = 1, type=&quot;two.sample&quot;), power = purrr::map_dbl(pcalc, `[[`, &quot;power&quot;)) %&gt;% select(-pcalc) Figure 7.1: Power for a two-sample t-test with d = 0.2 7.5 Custom functions In addition to the built-in functions and functions you can access from packages, you can also write your own functions (and eventually even packages!). 7.5.1 Structuring a function The general structure of a function is as follows: function_name &lt;- function(my_args) { # process the arguments # return some value } Here is a very simple function. Can you guess what it does? add1 &lt;- function(my_number) { my_number + 1 } add1(10) ## [1] 11 Let’s make a function that reports p-values in APA format (with “p = [rounded value]” when p &gt;= .001 and “p &lt; .001” when p &lt; .001). First, we have to name the function. You can name it anything, but try not to duplicate existing functions or you will overwrite them. For example, if you call your function rep, then you will need to use base::rep() to access the normal rep function. Let’s call our p-value function report_p and set up the framework of the function. report_p &lt;- function() { } 7.5.2 Arguments We need to add one argument, the p-value you want to report. The names you choose for the arguments are private to that argument, so it is not a problem if they conflict with other variables in your script. You put the arguments in the parentheses of function() in the order you want them to default (just like the built-in functions you’ve used before). report_p &lt;- function(p) { } 7.5.3 Argument defaults You can add a default value to any argument. If that argument is skipped, then the function uses the default argument. It probably doesn’t make sense to run this function without specifying the p-value, but we can add a second argument called digits that defaults to 3, so we can round p-values to any number of digits. report_p &lt;- function(p, digits = 3) { } Now we need to write some code inside the function to process the input arguments and turn them into a returned output. Put the output as the last item in function. report_p &lt;- function(p, digits = 3) { if (p &lt; .001) { reported = &quot;p &lt; .001&quot; } else { roundp &lt;- round(p, digits) reported = paste(&quot;p =&quot;, roundp) } reported } You might also see the returned output inside of the return() function. This does the same thing. report_p &lt;- function(p, digits = 3) { if (p &lt; .001) { reported = &quot;p &lt; .001&quot; } else { roundp &lt;- round(p, digits) reported = paste(&quot;p =&quot;, roundp) } return(reported) } When you run the code defining your function, it doesn’t output anything, but makes a new object in the Environment tab under Functions. Now you can run the function. report_p(0.04869) report_p(0.0000023) ## [1] &quot;p = 0.049&quot; ## [1] &quot;p &lt; .001&quot; 7.5.4 Scope What happens in a function stays in a function. You can change the value of a variable passed to a function, but that won’t change the value of the variable outside of the function, even if that variable has the same name as the one in the function. reported &lt;- &quot;not changed&quot; # inside this function, reported == &quot;p = 0.002&quot; report_p(0.0023) reported # still &quot;not changed&quot; ## [1] &quot;p = 0.002&quot; ## [1] &quot;not changed&quot; 7.5.5 Warnings and errors What happens when you omit the argument for p? Or if you set p to 1.5 or “a?” You might want to add a more specific warning and stop running the function code if someone enters a value that isn’t a number. You can do this with the stop() function. If someone enters a number that isn’t possible for a p-value (0-1), you might want to warn them that this is probably not what they intended, but still continue with the function. You can do this with warning(). report_p &lt;- function(p, digits = 3) { if (!is.numeric(p)) stop(&quot;p must be a number&quot;) if (p &lt;= 0) warning(&quot;p-values are normally greater than 0&quot;) if (p &gt;= 1) warning(&quot;p-values are normally less than 1&quot;) if (p &lt; .001) { reported = &quot;p &lt; .001&quot; } else { roundp &lt;- round(p, digits) reported = paste(&quot;p =&quot;, roundp) } reported } report_p() ## Error in report_p(): argument &quot;p&quot; is missing, with no default report_p(&quot;a&quot;) ## Error in report_p(&quot;a&quot;): p must be a number report_p(-2) ## Warning in report_p(-2): p-values are normally greater than 0 report_p(2) ## Warning in report_p(2): p-values are normally less than 1 ## [1] &quot;p &lt; .001&quot; ## [1] &quot;p = 2&quot; 7.6 Iterating your own functions First, let’s build up the code that we want to iterate. 7.6.1 rnorm() Create a vector of 20 random numbers drawn from a normal distribution with a mean of 5 and standard deviation of 1 using the rnorm() function and store them in the variable A. A &lt;- rnorm(20, mean = 5, sd = 1) 7.6.2 tibble::tibble() A tibble is a type of table or data.frame. The function tibble::tibble() creates a tibble with a column for each argument. Each argument takes the form column_name = data_vector. Create a table called dat including two vectors: A that is a vector of 20 random normally distributed numbers with a mean of 5 and SD of 1, and B that is a vector of 20 random normally distributed numbers with a mean of 5.5 and SD of 1. dat &lt;- tibble( A = rnorm(20, 5, 1), B = rnorm(20, 5.5, 1) ) 7.6.3 t.test() You can run a Welch two-sample t-test by including the two samples you made as the first two arguments to the function t.test. You can reference one column of a table by its names using the format table_name$column_name t.test(dat$A, dat$B) ## ## Welch Two Sample t-test ## ## data: dat$A and dat$B ## t = -1.7716, df = 36.244, p-value = 0.08487 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.2445818 0.0838683 ## sample estimates: ## mean of x mean of y ## 4.886096 5.466453 You can also convert the table to long format using the gather function and specify the t-test using the format dv_column~grouping_column. longdat &lt;- gather(dat, group, score, A:B) t.test(score~group, data = longdat) ## ## Welch Two Sample t-test ## ## data: score by group ## t = -1.7716, df = 36.244, p-value = 0.08487 ## alternative hypothesis: true difference in means between group A and group B is not equal to 0 ## 95 percent confidence interval: ## -1.2445818 0.0838683 ## sample estimates: ## mean in group A mean in group B ## 4.886096 5.466453 7.6.4 broom::tidy() You can use the function broom::tidy() to extract the data from a statistical test in a table format. The example below pipes everything together. tibble( A = rnorm(20, 5, 1), B = rnorm(20, 5.5, 1) ) %&gt;% gather(group, score, A:B) %&gt;% t.test(score~group, data = .) %&gt;% broom::tidy() estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high method alternative -0.6422108 5.044009 5.68622 -2.310591 0.0264905 37.27083 -1.205237 -0.0791844 Welch Two Sample t-test two.sided In the pipeline above, t.test(score~group, data = .) uses the . notation to change the location of the piped-in data table from it’s default position as the first argument to a different position. Finally, we can extract a single value from this results table using pull(). tibble( A = rnorm(20, 5, 1), B = rnorm(20, 5.5, 1) ) %&gt;% gather(group, score, A:B) %&gt;% t.test(score~group, data = .) %&gt;% broom::tidy() %&gt;% pull(p.value) ## [1] 0.7075268 7.6.5 Custom function: t_sim() First, name your function t_sim and wrap the code above in a function with no arguments. t_sim &lt;- function() { tibble( A = rnorm(20, 5, 1), B = rnorm(20, 5.5, 1) ) %&gt;% gather(group, score, A:B) %&gt;% t.test(score~group, data = .) %&gt;% broom::tidy() %&gt;% pull(p.value) } Run it a few times to see what happens. t_sim() ## [1] 0.00997552 7.6.6 Iterate t_sim() Let’s run the t_sim function 1000 times, assign the resulting p-values to a vector called reps, and check what proportion of p-values are lower than alpha (e.g., .05). This number is the power for this analysis. reps &lt;- replicate(1000, t_sim()) alpha &lt;- .05 power &lt;- mean(reps &lt; alpha) power ## [1] 0.328 7.6.7 Set seed You can use the set.seed function before you run a function that uses random numbers to make sure that you get the same random data back each time. You can use any integer you like as the seed. set.seed(90201) Make sure you don’t ever use set.seed() inside of a simulation function, or you will just simulate the exact same data over and over again. Figure 7.2: @KellyBodwin 7.6.8 Add arguments You can just edit your function each time you want to cacluate power for a different sample n, but it is more efficent to build this into your fuction as an arguments. Redefine t_sim, setting arguments for the mean and SD of group A, the mean and SD of group B, and the number of subjects per group. Give them all default values. t_sim &lt;- function(n = 10, m1=0, sd1=1, m2=0, sd2=1) { tibble( A = rnorm(n, m1, sd1), B = rnorm(n, m2, sd2) ) %&gt;% gather(group, score, A:B) %&gt;% t.test(score~group, data = .) %&gt;% broom::tidy() %&gt;% pull(p.value) } Test your function with some different values to see if the results make sense. t_sim(100) t_sim(100, 0, 1, 0.5, 1) ## [1] 0.5065619 ## [1] 0.001844064 Use replicate to calculate power for 100 subjects/group with an effect size of 0.2 (e.g., A: m = 0, SD = 1; B: m = 0.2, SD = 1). Use 1000 replications. reps &lt;- replicate(1000, t_sim(100, 0, 1, 0.2, 1)) power &lt;- mean(reps &lt; .05) power ## [1] 0.268 Compare this to power calculated from the power.t.test function. power.t.test(n = 100, delta = 0.2, sd = 1, type=&quot;two.sample&quot;) ## ## Two-sample t test power calculation ## ## n = 100 ## delta = 0.2 ## sd = 1 ## sig.level = 0.05 ## power = 0.2902664 ## alternative = two.sided ## ## NOTE: n is number in *each* group Calculate power via simulation and power.t.test for the following tests: 20 subjects/group, A: m = 0, SD = 1; B: m = 0.2, SD = 1 40 subjects/group, A: m = 0, SD = 1; B: m = 0.2, SD = 1 20 subjects/group, A: m = 10, SD = 1; B: m = 12, SD = 1.5 7.7 Glossary term definition argument A variable that provides input to a function. data type The kind of data represented by an object. double A data type representing a real decimal number function A named section of code that can be reused. iteration Repeating a process or function matrix A container data type consisting of numbers arranged into a fixed number of rows and columns 7.8 Exercises Download the exercises. See the answers only after you’ve attempted all the questions. # run this to access the exercise dataskills::exercise(7) # run this to access the answers dataskills::exercise(7, answers = TRUE) "],["sim.html", "Chapter 8 Probability &amp; Simulation 8.1 Learning Objectives 8.2 Resources 8.3 Setup 8.4 Univariate Distributions 8.5 Multivariate Distributions 8.6 Statistical terms 8.7 Tests 8.8 Example 8.9 Glossary 8.10 Exercises", " Chapter 8 Probability &amp; Simulation 8.1 Learning Objectives 8.1.1 Basic Generate and plot data randomly sampled from common distributions (video) uniform binomial normal poisson Generate related variables from a multivariate distribution (video) Define the following statistical terms: p-value alpha power smallest effect size of interest (SESOI) false positive (type I error) false negative (type II error) confidence interval (CI) Test sampled distributions against a null hypothesis (video) exact binomial test t-test (1-sample, independent samples, paired samples) correlation (pearson, kendall and spearman) Calculate power using iteration and a sampling function 8.1.2 Intermediate Calculate the minimum sample size for a specific power level and design 8.2 Resources Stub for this lesson Distribution Shiny App (or run dataskills::app(\"simulate\") Simulation tutorials Chapter 21: Iteration of R for Data Science Improving your statistical inferences on Coursera (week 1) Faux package for data simulation Simulation-Based Power-Analysis for Factorial ANOVA Designs (Daniel Lakens and Caldwell 2019) Understanding mixed effects models through data simulation (DeBruine and Barr 2019) 8.3 Setup # libraries needed for these examples library(tidyverse) library(plotly) library(faux) set.seed(8675309) # makes sure random numbers are reproducible Simulating data is a very powerful way to test your understanding of statistical concepts. We are going to use simulations to learn the basics of probability. 8.4 Univariate Distributions First, we need to understand some different ways data might be distributed and how to simulate data from these distributions. A univariate distribution is the distribution of a single variable. 8.4.1 Uniform Distribution The uniform distribution is the simplest distribution. All numbers in the range have an equal probability of being sampled. Take a minute to think of things in your own research that are uniformly distributed. 8.4.1.1 Continuous distribution runif(n, min=0, max=1) Use runif() to sample from a continuous uniform distribution. u &lt;- runif(100000, min = 0, max = 1) # plot to visualise ggplot() + geom_histogram(aes(u), binwidth = 0.05, boundary = 0, fill = &quot;white&quot;, colour = &quot;black&quot;) 8.4.1.2 Discrete sample(x, size, replace = FALSE, prob = NULL) Use sample() to sample from a discrete distribution. You can use sample() to simulate events like rolling dice or choosing from a deck of cards. The code below simulates rolling a 6-sided die 10000 times. We set replace to TRUE so that each event is independent. See what happens if you set replace to FALSE. rolls &lt;- sample(1:6, 10000, replace = TRUE) # plot the results ggplot() + geom_histogram(aes(rolls), binwidth = 1, fill = &quot;white&quot;, color = &quot;black&quot;) Figure 8.1: Distribution of dice rolls. You can also use sample to sample from a list of named outcomes. pet_types &lt;- c(&quot;cat&quot;, &quot;dog&quot;, &quot;ferret&quot;, &quot;bird&quot;, &quot;fish&quot;) sample(pet_types, 10, replace = TRUE) ## [1] &quot;cat&quot; &quot;cat&quot; &quot;cat&quot; &quot;cat&quot; &quot;ferret&quot; &quot;dog&quot; &quot;bird&quot; &quot;cat&quot; ## [9] &quot;dog&quot; &quot;fish&quot; Ferrets are a much less common pet than cats and dogs, so our sample isn’t very realistic. You can set the probabilities of each item in the list with the prob argument. pet_types &lt;- c(&quot;cat&quot;, &quot;dog&quot;, &quot;ferret&quot;, &quot;bird&quot;, &quot;fish&quot;) pet_prob &lt;- c(0.3, 0.4, 0.1, 0.1, 0.1) sample(pet_types, 10, replace = TRUE, prob = pet_prob) ## [1] &quot;fish&quot; &quot;dog&quot; &quot;cat&quot; &quot;dog&quot; &quot;cat&quot; &quot;dog&quot; &quot;fish&quot; &quot;dog&quot; &quot;cat&quot; &quot;fish&quot; 8.4.2 Binomial Distribution The binomial distribution is useful for modelling binary data, where each observation can have one of two outcomes, like success/failure, yes/no or head/tails. rbinom(n, size, prob) The rbinom function will generate a random binomial distribution. n = number of observations size = number of trials prob = probability of success on each trial Coin flips are a typical example of a binomial distribution, where we can assign heads to 1 and tails to 0. # 20 individual coin flips of a fair coin rbinom(20, 1, 0.5) ## [1] 1 1 1 0 1 1 0 1 0 0 1 1 1 0 0 0 1 0 0 0 # 20 individual coin flips of a baised (0.75) coin rbinom(20, 1, 0.75) ## [1] 1 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 You can generate the total number of heads in 1 set of 20 coin flips by setting size to 20 and n to 1. rbinom(1, 20, 0.75) ## [1] 13 You can generate more sets of 20 coin flips by increasing the n. rbinom(10, 20, 0.5) ## [1] 10 14 11 7 11 13 6 10 9 9 You should always check your randomly generated data to check that it makes sense. For large samples, it’s easiest to do that graphically. A histogram is usually the best choice for plotting binomial data. flips &lt;- rbinom(1000, 20, 0.5) ggplot() + geom_histogram( aes(flips), binwidth = 1, fill = &quot;white&quot;, color = &quot;black&quot; ) Run the simulation above several times, noting how the histogram changes. Try changing the values of n, size, and prob. 8.4.3 Normal Distribution rnorm(n, mean, sd) We can simulate a normal distribution of size n if we know the mean and standard deviation (sd). A density plot is usually the best way to visualise this type of data if your n is large. dv &lt;- rnorm(1e5, 10, 2) # proportions of normally-distributed data # within 1, 2, or 3 SD of the mean sd1 &lt;- .6827 sd2 &lt;- .9545 sd3 &lt;- .9973 ggplot() + geom_density(aes(dv), fill = &quot;white&quot;) + geom_vline(xintercept = mean(dv), color = &quot;red&quot;) + geom_vline(xintercept = quantile(dv, .5 - sd1/2), color = &quot;darkgreen&quot;) + geom_vline(xintercept = quantile(dv, .5 + sd1/2), color = &quot;darkgreen&quot;) + geom_vline(xintercept = quantile(dv, .5 - sd2/2), color = &quot;blue&quot;) + geom_vline(xintercept = quantile(dv, .5 + sd2/2), color = &quot;blue&quot;) + geom_vline(xintercept = quantile(dv, .5 - sd3/2), color = &quot;purple&quot;) + geom_vline(xintercept = quantile(dv, .5 + sd3/2), color = &quot;purple&quot;) + scale_x_continuous( limits = c(0,20), breaks = seq(0,20) ) Run the simulation above several times, noting how the density plot changes. What do the vertical lines represent? Try changing the values of n, mean, and sd. 8.4.4 Poisson Distribution The Poisson distribution is useful for modelling events, like how many times something happens over a unit of time, as long as the events are independent (e.g., an event having happened in one time period doesn’t make it more or less likely to happen in the next). rpois(n, lambda) The rpois function will generate a random Poisson distribution. n = number of observations lambda = the mean number of events per observation Let’s say we want to model how many texts you get each day for a whole. You know that you get an average of 20 texts per day. So we set n = 365 and lambda = 20. Lambda is a parameter that describes the Poisson distribution, just like mean and standard deviation are parameters that describe the normal distribution. texts &lt;- rpois(n = 365, lambda = 20) ggplot() + geom_histogram( aes(texts), binwidth = 1, fill = &quot;white&quot;, color = &quot;black&quot; ) So we can see that over a year, you’re unlikely to get fewer than 5 texts in a day, or more than 35 (although it’s not impossible). 8.5 Multivariate Distributions 8.5.1 Bivariate Normal A bivariate normal distribution is two normally distributed vectors that have a specified relationship, or correlation to each other. What if we want to sample from a population with specific relationships between variables? We can sample from a bivariate normal distribution using mvrnorm() from the MASS package. Don’t load MASS with the library() function because it will create a conflict with the select() function from dplyr and you will always need to preface it with dplyr::. Just use MASS::mvrnorm(). You need to know how many observations you want to simulate (n) the means of the two variables (mu) and you need to calculate a covariance matrix (sigma) from the correlation between the variables (rho) and their standard deviations (sd). n &lt;- 1000 # number of random samples # name the mu values to give the resulting columns names mu &lt;- c(x = 10, y = 20) # the means of the samples sd &lt;- c(5, 6) # the SDs of the samples rho &lt;- 0.5 # population correlation between the two variables # correlation matrix cor_mat &lt;- matrix(c( 1, rho, rho, 1), 2) # create the covariance matrix sigma &lt;- (sd %*% t(sd)) * cor_mat # sample from bivariate normal distribution bvn &lt;- MASS::mvrnorm(n, mu, sigma) Plot your sampled variables to check everything worked like you expect. It’s easiest to convert the output of mvnorm into a tibble in order to use it in ggplot. bvn %&gt;% as_tibble() %&gt;% ggplot(aes(x, y)) + geom_point(alpha = 0.5) + geom_smooth(method = &quot;lm&quot;) + geom_density2d() ## `geom_smooth()` using formula &#39;y ~ x&#39; 8.5.2 Multivariate Normal You can generate more than 2 correlated variables, but it gets a little trickier to create the correlation matrix. n &lt;- 200 # number of random samples mu &lt;- c(x = 10, y = 20, z = 30) # the means of the samples sd &lt;- c(8, 9, 10) # the SDs of the samples rho1_2 &lt;- 0.5 # correlation between x and y rho1_3 &lt;- 0 # correlation between x and z rho2_3 &lt;- 0.7 # correlation between y and z # correlation matrix cor_mat &lt;- matrix(c( 1, rho1_2, rho1_3, rho1_2, 1, rho2_3, rho1_3, rho2_3, 1), 3) sigma &lt;- (sd %*% t(sd)) * cor_mat bvn3 &lt;- MASS::mvrnorm(n, mu, sigma) cor(bvn3) # check correlation matrix ## x y z ## x 1.0000000 0.5896674 0.1513108 ## y 0.5896674 1.0000000 0.7468737 ## z 0.1513108 0.7468737 1.0000000 You can use the plotly library to make a 3D graph. #set up the marker style marker_style = list( color = &quot;#ff0000&quot;, line = list( color = &quot;#444&quot;, width = 1 ), opacity = 0.5, size = 5 ) # convert bvn3 to a tibble, plot and add markers bvn3 %&gt;% as_tibble() %&gt;% plot_ly(x = ~x, y = ~y, z = ~z, marker = marker_style) %&gt;% add_markers() 8.5.3 Faux Alternatively, you can use the package faux to generate any number of correlated variables. It also has a function for checking the parameters of your new simulated data (check_sim_stats()). bvn3 &lt;- rnorm_multi( n = n, vars = 3, mu = mu, sd = sd, r = c(rho1_2, rho1_3, rho2_3), varnames = c(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;) ) check_sim_stats(bvn3) n var x y z mean sd 200 x 1.00 0.54 0.10 10.35 7.66 200 y 0.54 1.00 0.67 20.01 8.77 200 z 0.10 0.67 1.00 30.37 9.59 You can also use faux to simulate data for factorial designs. Set up the between-subject and within-subject factors as lists with the levels as (named) vectors. Means and standard deviations can be included as vectors or data frames. The function calculates sigma for you, structures your dataset, and outputs a plot of the design. b &lt;- list(pet = c(cat = &quot;Cat Owners&quot;, dog = &quot;Dog Owners&quot;)) w &lt;- list(time = c(&quot;morning&quot;, &quot;noon&quot;, &quot;night&quot;)) mu &lt;- data.frame( cat = c(10, 12, 14), dog = c(10, 15, 20), row.names = w$time ) sd &lt;- c(3, 3, 3, 5, 5, 5) pet_data &lt;- sim_design( within = w, between = b, n = 100, mu = mu, sd = sd, r = .5) You can use the check_sim_stats() function, but you need to set the argument between to a vector of all the between-subject factor columns. check_sim_stats(pet_data, between = &quot;pet&quot;) pet n var morning noon night mean sd cat 100 morning 1.00 0.57 0.51 10.62 3.48 cat 100 noon 0.57 1.00 0.59 12.44 3.01 cat 100 night 0.51 0.59 1.00 14.61 3.14 dog 100 morning 1.00 0.55 0.50 9.44 4.92 dog 100 noon 0.55 1.00 0.48 14.18 5.90 dog 100 night 0.50 0.48 1.00 19.42 5.36 See the faux website for more detailed tutorials. 8.6 Statistical terms Let’s review some important statistical terms before we review tests of distributions. 8.6.1 Effect The effect is some measure of your data. This will depend on the type of data you have and the type of statistical test you are using. For example, if you flipped a coin 100 times and it landed heads 66 times, the effect would be 66/100. You can then use the exact binomial test to compare this effect to the null effect you would expect from a fair coin (50/100) or to any other effect you choose. The effect size refers to the difference between the effect in your data and the null effect (usually a chance value). 8.6.2 P-value The p-value of a test is the probability of seeing an effect at least as extreme as what you have, if the real effect was the value you are testing against (e.g., a null effect). So if you used a binomial test to test against a chance probability of 1/6 (e.g., the probability of rolling 1 with a 6-sided die), then a p-value of 0.17 means that you could expect to see effects at least as extreme as your data 17% of the time just by chance alone. 8.6.3 Alpha If you are using null hypothesis significance testing (NHST), then you need to decide on a cutoff value (alpha) for making a decision to reject the null hypothesis. We call p-values below the alpha cutoff significant. In psychology, alpha is traditionally set at 0.05, but there are good arguments for setting a different criterion in some circumstances. 8.6.4 False Positive/Negative The probability that a test concludes there is an effect when there is really no effect (e.g., concludes a fair coin is biased) is called the false positive rate (or Type I Error Rate). The alpha is the false positive rate we accept for a test. The probability that a test concludes there is no effect when there really is one (e.g., concludes a biased coin is fair) is called the false negative rate (or Type II Error Rate). The beta is the false negative rate we accept for a test. The false positive rate is not the overall probability of getting a false positive, but the probability of a false positive under the null hypothesis. Similarly, the false negative rate is the probability of a false negative under the alternative hypothesis. Unless we know the probability that we are testing a null effect, we can’t say anything about the overall probability of false positives or negatives. If 100% of the hypotheses we test are false, then all significant effects are false positives, but if all of the hypotheses we test are true, then all of the positives are true positives and the overall false positive rate is 0. 8.6.5 Power and SESOI Power is equal to 1 minus beta (i.e., the true positive rate), and depends on the effect size, how many samples we take (n), and what we set alpha to. For any test, if you specify all but one of these values, you can calculate the last. The effect size you use in power calculations should be the smallest effect size of interest (SESOI). See (Daniël Lakens, Scheel, and Isager 2018)(https://doi.org/10.1177/2515245918770963) for a tutorial on methods for choosing an SESOI. Let’s say you want to be able to detect at least a 15% difference from chance (50%) in a coin’s fairness, and you want your test to have a 5% chance of false positives and a 10% chance of false negatives. What are the following values? alpha = beta = false positive rate = false negative rate = power = SESOI = 8.6.6 Confidence Intervals The confidence interval is a range around some value (such as a mean) that has some probability of containing the parameter, if you repeated the process many times. Traditionally in psychology, we use 95% confidence intervals, but you can calculate CIs for any percentage. A 95% CI does not mean that there is a 95% probability that the true mean lies within this range, but that, if you repeated the study many times and calculated the CI this same way every time, you’d expect the true mean to be inside the CI in 95% of the studies. This seems like a subtle distinction, but can lead to some misunderstandings. See (Morey et al. 2016)(https://link.springer.com/article/10.3758/s13423-015-0947-8) for more detailed discussion. 8.7 Tests 8.7.1 Exact binomial test binom.test(x, n, p) You can test a binomial distribution against a specific probability using the exact binomial test. x = the number of successes n = the number of trials p = hypothesised probability of success Here we can test a series of 10 coin flips from a fair coin and a biased coin against the hypothesised probability of 0.5 (even odds). n &lt;- 10 fair_coin &lt;- rbinom(1, n, 0.5) biased_coin &lt;- rbinom(1, n, 0.6) binom.test(fair_coin, n, p = 0.5) binom.test(biased_coin, n, p = 0.5) ## ## Exact binomial test ## ## data: fair_coin and n ## number of successes = 6, number of trials = 10, p-value = 0.7539 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.2623781 0.8784477 ## sample estimates: ## probability of success ## 0.6 ## ## ## Exact binomial test ## ## data: biased_coin and n ## number of successes = 8, number of trials = 10, p-value = 0.1094 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.4439045 0.9747893 ## sample estimates: ## probability of success ## 0.8 Run the code above several times, noting the p-values for the fair and biased coins. Alternatively, you can simulate coin flips online and build up a graph of results and p-values. How does the p-value vary for the fair and biased coins? What happens to the confidence intervals if you increase n from 10 to 100? What criterion would you use to tell if the observed data indicate the coin is fair or biased? How often do you conclude the fair coin is biased (false positives)? How often do you conclude the biased coin is fair (false negatives)? 8.7.1.1 Sampling function To estimate these rates, we need to repeat the sampling above many times. A function is ideal for repeating the exact same procedure over and over. Set the arguments of the function to variables that you might want to change. Here, we will want to estimate power for: different sample sizes (n) different effects (bias) different hypothesised probabilities (p, defaults to 0.5) sim_binom_test &lt;- function(n, bias, p = 0.5) { # simulate 1 coin flip n times with the specified bias coin &lt;- rbinom(1, n, bias) # run a binomial test on the simulated data for the specified p btest &lt;- binom.test(coin, n, p) # return the p-value of this test btest$p.value } Once you’ve created your function, test it a few times, changing the values. sim_binom_test(100, 0.6) ## [1] 0.1332106 8.7.1.2 Calculate power Then you can use the replicate() function to run it many times and save all the output values. You can calculate the power of your analysis by checking the proportion of your simulated analyses that have a p-value less than your alpha (the probability of rejecting the null hypothesis when the null hypothesis is true). my_reps &lt;- replicate(1e4, sim_binom_test(100, 0.6)) alpha &lt;- 0.05 # this does not always have to be 0.05 mean(my_reps &lt; alpha) ## [1] 0.4561 1e4 is just scientific notation for a 1 followed by 4 zeros (10000). When you’re running simulations, you usually want to run a lot of them. It’s a pain to keep track of whether you’ve typed 5 or 6 zeros (100000 vs 1000000) and this will change your running time by an order of magnitude. You can plot the distribution of p-values. ggplot() + geom_histogram( aes(my_reps), binwidth = 0.05, boundary = 0, fill = &quot;white&quot;, color = &quot;black&quot; ) 8.7.2 T-test t.test(x, y, alternative, mu, paired) Use a t-test to compare the mean of one distribution to a null hypothesis (one-sample t-test), compare the means of two samples (independent-samples t-test), or compare pairs of values (paired-samples t-test). You can run a one-sample t-test comparing the mean of your data to mu. Here is a simulated distribution with a mean of 0.5 and an SD of 1, creating an effect size of 0.5 SD when tested against a mu of 0. Run the simulation a few times to see how often the t-test returns a significant p-value (or run it in the shiny app). sim_norm &lt;- rnorm(100, 0.5, 1) t.test(sim_norm, mu = 0) ## ## One Sample t-test ## ## data: sim_norm ## t = 6.2874, df = 99, p-value = 8.758e-09 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 0.4049912 0.7784761 ## sample estimates: ## mean of x ## 0.5917337 Run an independent-samples t-test by comparing two lists of values. a &lt;- rnorm(100, 0.5, 1) b &lt;- rnorm(100, 0.7, 1) t_ind &lt;- t.test(a, b, paired = FALSE) t_ind ## ## Welch Two Sample t-test ## ## data: a and b ## t = -1.8061, df = 197.94, p-value = 0.07243 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.54825320 0.02408469 ## sample estimates: ## mean of x mean of y ## 0.4585985 0.7206828 The paired argument defaults to FALSE, but it’s good practice to always explicitly set it so you are never confused about what type of test you are performing. 8.7.2.1 Sampling function We can use the names() function to find out the names of all the t.test parameters and use this to just get one type of data, like the test statistic (e.g., t-value). names(t_ind) t_ind$statistic ## [1] &quot;statistic&quot; &quot;parameter&quot; &quot;p.value&quot; &quot;conf.int&quot; &quot;estimate&quot; ## [6] &quot;null.value&quot; &quot;stderr&quot; &quot;alternative&quot; &quot;method&quot; &quot;data.name&quot; ## t ## -1.806051 If you want to run the simulation many times and record information each time, first you need to turn your simulation into a function. sim_t_ind &lt;- function(n, m1, sd1, m2, sd2) { # simulate v1 v1 &lt;- rnorm(n, m1, sd1) #simulate v2 v2 &lt;- rnorm(n, m2, sd2) # compare using an independent samples t-test t_ind &lt;- t.test(v1, v2, paired = FALSE) # return the p-value return(t_ind$p.value) } Run it a few times to check that it gives you sensible values. sim_t_ind(100, 0.7, 1, 0.5, 1) ## [1] 0.362521 8.7.2.2 Calculate power Now replicate the simulation 1000 times. my_reps &lt;- replicate(1e4, sim_t_ind(100, 0.7, 1, 0.5, 1)) alpha &lt;- 0.05 power &lt;- mean(my_reps &lt; alpha) power ## [1] 0.2925 Run the code above several times. How much does the power value fluctuate? How many replications do you need to run to get a reliable estimate of power? Compare your power estimate from simluation to a power calculation using power.t.test(). Here, delta is the difference between m1 and m2 above. power.t.test(n = 100, delta = 0.2, sd = 1, sig.level = alpha, type = &quot;two.sample&quot;) ## ## Two-sample t test power calculation ## ## n = 100 ## delta = 0.2 ## sd = 1 ## sig.level = 0.05 ## power = 0.2902664 ## alternative = two.sided ## ## NOTE: n is number in *each* group You can plot the distribution of p-values. ggplot() + geom_histogram( aes(my_reps), binwidth = 0.05, boundary = 0, fill = &quot;white&quot;, color = &quot;black&quot; ) What do you think the distribution of p-values is when there is no effect (i.e., the means are identical)? Check this yourself. Make sure the boundary argument is set to 0 for p-value histograms. See what happens with a null effect if boundary is not set. 8.7.3 Correlation You can test if continuous variables are related to each other using the cor() function. Let’s use rnorm_multi() to make a quick table of correlated values. dat &lt;- rnorm_multi( n = 100, vars = 2, r = -0.5, varnames = c(&quot;x&quot;, &quot;y&quot;) ) cor(dat$x, dat$y) ## [1] -0.4960331 Set n to a large number like 1e6 so that the correlations are less affected by chance. Change the value of the mean for a, x, or y. Does it change the correlation between x and y? What happens when you increase or decrease the sd? Can you work out any rules here? cor() defaults to Pearson’s correlations. Set the method argument to use Kendall or Spearman correlations. cor(dat$x, dat$y, method = &quot;spearman&quot;) ## [1] -0.4724992 8.7.3.1 Sampling function Create a function that creates two variables with n observations and r correlation. Use the function cor.test() to give you p-values for the correlation. sim_cor_test &lt;- function(n = 100, r = 0) { dat &lt;- rnorm_multi( n = n, vars = 2, r = r, varnames = c(&quot;x&quot;, &quot;y&quot;) ) ctest &lt;- cor.test(dat$x, dat$y) ctest$p.value } Once you’ve created your function, test it a few times, changing the values. sim_cor_test(50, .5) ## [1] 0.001354836 8.7.3.2 Calculate power Now replicate the simulation 1000 times. my_reps &lt;- replicate(1e4, sim_cor_test(50, 0.5)) alpha &lt;- 0.05 power &lt;- mean(my_reps &lt; alpha) power ## [1] 0.965 Compare to the value calcuated by the pwr package. pwr::pwr.r.test(n = 50, r = 0.5) ## ## approximate correlation power calculation (arctangh transformation) ## ## n = 50 ## r = 0.5 ## sig.level = 0.05 ## power = 0.9669813 ## alternative = two.sided 8.8 Example This example uses the Growth Chart Data Tables from the US CDC. The data consist of height in centimeters for the z-scores of –2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, and 2 by sex (1=male; 2=female) and half-month of age (from 24.0 to 240.5 months). 8.8.1 Load &amp; wrangle We have to do a little data wrangling first. Have a look at the data after you import it and relabel Sex to male and female instead of 1 and 2. Also convert Agemos (age in months) to years. Relabel the column 0 as mean and calculate a new column named sd as the difference between columns 1 and 0. orig_height_age &lt;- read_csv(&quot;https://www.cdc.gov/growthcharts/data/zscore/zstatage.csv&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## Sex = col_character(), ## Agemos = col_character(), ## `-2` = col_double(), ## `-1.5` = col_double(), ## `-1` = col_double(), ## `-0.5` = col_double(), ## `0` = col_double(), ## `0.5` = col_double(), ## `1` = col_double(), ## `1.5` = col_double(), ## `2` = col_double() ## ) height_age &lt;- orig_height_age %&gt;% filter(Sex %in% c(1,2)) %&gt;% mutate( sex = recode(Sex, &quot;1&quot; = &quot;male&quot;, &quot;2&quot; = &quot;female&quot;), age = as.numeric(Agemos)/12, sd = `1` - `0` ) %&gt;% select(sex, age, mean = `0`, sd) 8.8.2 Plot Plot your new data frame to see how mean height changes with age for boys and girls. ggplot(height_age, aes(age, mean, color = sex)) + geom_smooth(aes(ymin = mean - sd, ymax = mean + sd), stat=&quot;identity&quot;) 8.8.3 Simulate a population Simulate 50 random male heights and 50 random female heights for 20-year-olds using the rnorm() function and the means and SDs from the height_age table. Plot the data. age_filter &lt;- 20 m &lt;- filter(height_age, age == age_filter, sex == &quot;male&quot;) f &lt;- filter(height_age, age == age_filter, sex == &quot;female&quot;) sim_height &lt;- tibble( male = rnorm(50, m$mean, m$sd), female = rnorm(50, f$mean, f$sd) ) %&gt;% gather(&quot;sex&quot;, &quot;height&quot;, male:female) ggplot(sim_height) + geom_density(aes(height, fill = sex), alpha = 0.5) + xlim(125, 225) Run the simulation above several times, noting how the density plot changes. Try changing the age you’re simulating. 8.8.4 Analyse simulated data Use the sim_t_ind(n, m1, sd1, m2, sd2) function we created above to generate one simulation with a sample size of 50 in each group using the means and SDs of male and female 14-year-olds. age_filter &lt;- 14 m &lt;- filter(height_age, age == age_filter, sex == &quot;male&quot;) f &lt;- filter(height_age, age == age_filter, sex == &quot;female&quot;) sim_t_ind(50, m$mean, m$sd, f$mean, f$sd) ## [1] 0.0005255744 8.8.5 Replicate simulation Now replicate this 1e4 times using the replicate() function. This function will save the returned p-values in a list (my_reps). We can then check what proportion of those p-values are less than our alpha value. This is the power of our test. my_reps &lt;- replicate(1e4, sim_t_ind(50, m$mean, m$sd, f$mean, f$sd)) alpha &lt;- 0.05 power &lt;- mean(my_reps &lt; alpha) power ## [1] 0.6403 8.8.6 One-tailed prediction This design has about 65% power to detect the sex difference in height (with a 2-tailed test). Modify the sim_t_ind function for a 1-tailed prediction. You could just set alternative equal to “greater” in the function, but it might be better to add the alt argument to your function (giving it the same default value as t.test) and change the value of alternative in the function to alt. sim_t_ind &lt;- function(n, m1, sd1, m2, sd2, alt = &quot;two.sided&quot;) { v1 &lt;- rnorm(n, m1, sd1) v2 &lt;- rnorm(n, m2, sd2) t_ind &lt;- t.test(v1, v2, paired = FALSE, alternative = alt) return(t_ind$p.value) } alpha &lt;- 0.05 my_reps &lt;- replicate(1e4, sim_t_ind(50, m$mean, m$sd, f$mean, f$sd, &quot;greater&quot;)) mean(my_reps &lt; alpha) ## [1] 0.752 8.8.7 Range of sample sizes What if we want to find out what sample size will give us 80% power? We can try trial and error. We know the number should be slightly larger than 50. But you can search more systematically by repeating your power calculation for a range of sample sizes. This might seem like overkill for a t-test, where you can easily look up sample size calculators online, but it is a valuable skill to learn for when your analyses become more complicated. Start with a relatively low number of replications and/or more spread-out samples to estimate where you should be looking more specifically. Then you can repeat with a narrower/denser range of sample sizes and more iterations. # make another custom function to return power pwr_func &lt;- function(n, reps = 100, alpha = 0.05) { ps &lt;- replicate(reps, sim_t_ind(n, m$mean, m$sd, f$mean, f$sd, &quot;greater&quot;)) mean(ps &lt; alpha) } # make a table of the n values you want to check power_table &lt;- tibble( n = seq(20, 100, by = 5) ) %&gt;% # run the power function for each n mutate(power = map_dbl(n, pwr_func)) # plot the results ggplot(power_table, aes(n, power)) + geom_smooth() + geom_point() + geom_hline(yintercept = 0.8) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Now we can narrow down our search to values around 55 (plus or minus 5) and increase the number of replications from 1e3 to 1e4. power_table &lt;- tibble( n = seq(50, 60) ) %&gt;% mutate(power = map_dbl(n, pwr_func, reps = 1e4)) ggplot(power_table, aes(n, power)) + geom_smooth() + geom_point() + geom_hline(yintercept = 0.8) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; 8.9 Glossary term definition alpha (stats) The cutoff value for making a decision to reject the null hypothesis; (graphics) A value between 0 and 1 used to control the levels of transparency in a plot beta The false negative rate we accept for a statistical test. binomial distribution The distribution of data where each observation can have one of two outcomes, like success/failure, yes/no or head/tails. bivariate normal Two normally distributed vectors that have a specified correlation with each other. confidence interval A type of interval estimate used to summarise a given statistic or measurement where a proportion of intervals calculated from the sample(s) will contain the true value of the statistic. correlation The relationship two vectors have to each other. covariance matrix Parameters showing how a set of vectors vary and are correlated. discrete Data that can only take certain values, such as integers. effect size The difference between the effect in your data and the null effect (usually a chance value) effect Some measure of your data, such as the mean value, or the number of standard deviations the mean differs from a chance value. false negative When a test concludes there is no effect when there really is an effect false positive When a test concludes there is an effect when there really is no effect function A named section of code that can be reused. nhst Null Hypothesis Signficance Testing normal distribution A symmetric distribution of data where values near the centre are most probable. null effect An outcome that does not show an otherwise expected effect. p value The probability of seeing an effect at least as extreme as what you have, if the real effect was the value you are testing against (e.g., a null effect) parameter A value that describes a distribution, such as the mean or SD poisson distribution A distribution that models independent events happening over a unit of time power The probability of rejecting the null hypothesis when it is false. probability A number between 0 and 1 where 0 indicates impossibility of the event and 1 indicates certainty sesoi Smallest Effect Size of Interest: the smallest effect that is theoretically or practically meaningful significant The conclusion when the p-value is less than the critical alpha. simulation Generating data from summary parameters true positive When a test concludes there is an effect when there is really is an effect type i error A false positive; When a test concludes there is an effect when there is really is no effect type ii error A false negative; When a test concludes there is no effect when there is really is an effect uniform distribution A distribution where all numbers in the range have an equal probability of being sampled univariate Relating to a single variable. 8.10 Exercises Download the exercises. See the answers only after you’ve attempted all the questions. # run this to access the exercise dataskills::exercise(8) # run this to access the answers dataskills::exercise(8, answers = TRUE) E References "],["glm.html", "Chapter 9 Introduction to GLM 9.1 Learning Objectives 9.2 Resources 9.3 Setup 9.4 GLM 9.5 Relationships among tests 9.6 Understanding ANOVA 9.7 Glossary 9.8 Exercises", " Chapter 9 Introduction to GLM 9.1 Learning Objectives 9.1.1 Basic Define the components of the GLM Simulate data using GLM equations (video) Identify the model parameters that correspond to the data-generation parameters Understand and plot residuals (video) Predict new values using the model (video) Explain the differences among coding schemes (video) 9.1.2 Intermediate Demonstrate the relationships among two-sample t-test, one-way ANOVA, and linear regression Given data and a GLM, generate a decomposition matrix and calculate sums of squares, mean squares, and F ratios for a one-way ANOVA 9.2 Resources Stub for this lesson Jeff Miller and Patricia Haden, Statistical Analysis with the Linear Model (free online textbook) lecture slides introducing the General Linear Model GLM shiny app F distribution 9.3 Setup # libraries needed for these examples library(tidyverse) library(broom) set.seed(30250) # makes sure random numbers are reproducible 9.4 GLM 9.4.1 What is the GLM? The General Linear Model (GLM) is a general mathematical framework for expressing relationships among variables that can express or test linear relationships between a numerical dependent variable and any combination of categorical or continuous independent variables. 9.4.2 Components There are some mathematical conventions that you need to learn to understand the equations representing linear models. Once you understand those, learning about the GLM will get much easier. Component of GLM Notation Dependent Variable (DV) \\(Y\\) Grand Average \\(\\mu\\) (the Greek letter “mu”) Main Effects \\(A, B, C, \\ldots\\) Interactions \\(AB, AC, BC, ABC, \\ldots\\) Random Error \\(S(Group)\\) The linear equation predicts the dependent variable (\\(Y\\)) as the sum of the grand average value of \\(Y\\) (\\(\\mu\\), also called the intercept), the main effects of all the predictor variables (\\(A+B+C+ \\ldots\\)), the interactions among all the predictor variables (\\(AB, AC, BC, ABC, \\ldots\\)), and some random error (\\(S(Group)\\)). The equation for a model with two predictor variables (\\(A\\) and \\(B\\)) and their interaction (\\(AB\\)) is written like this: \\(Y\\) ~ \\(\\mu+A+B+AB+S(Group)\\) Don’t worry if this doesn’t make sense until we walk through a concrete example. 9.4.3 Simulating data from GLM A good way to learn about linear models is to simulate data where you know exactly how the variables are related, and then analyse this simulated data to see where the parameters show up in the analysis. We’ll start with a very simple linear model that just has a single categorical factor with two levels. Let’s say we’re predicting reaction times for congruent and incongruent trials in a Stroop task for a single participant. Average reaction time (mu) is 800ms, and is 50ms faster for congruent than incongruent trials (effect). A factor is a categorical variable that is used to divide subjects into groups, usually to draw some comparison. Factors are composed of different levels. Do not confuse factors with levels! In the example above, trial type is a factor level, incongrunt is a factor level, and congruent is a factor level. You need to represent categorical factors with numbers. The numbers, or coding scheme you choose will affect the numbers you get out of the analysis and how you need to interpret them. Here, we will effect code the trial types so that congruent trials are coded as +0.5, and incongruent trials are coded as -0.5. A person won’t always respond exactly the same way. They might be a little faster on some trials than others, due to random fluctuations in attention, learning about the task, or fatigue. So we can add an error term to each trial. We can’t know how much any specific trial will differ, but we can characterise the distribution of how much trials differ from average and then sample from this distribution. Here, we’ll assume the error term is sampled from a normal distribution with a standard deviation of 100 ms (the mean of the error term distribution is always 0). We’ll also sample 100 trials of each type, so we can see a range of variation. So first create variables for all of the parameters that describe your data. n_per_grp &lt;- 100 mu &lt;- 800 # average RT effect &lt;- 50 # average difference between congruent and incongruent trials error_sd &lt;- 100 # standard deviation of the error term trial_types &lt;- c(&quot;congruent&quot; = 0.5, &quot;incongruent&quot; = -0.5) # effect code Then simulate the data by creating a data table with a row for each trial and columns for the trial type and the error term (random numbers samples from a normal distribution with the SD specified by error_sd). For categorical variables, include both a column with the text labels (trial_type) and another column with the coded version (trial_type.e) to make it easier to check what the codings mean and to use for graphing. Calculate the dependent variable (RT) as the sum of the grand mean (mu), the coefficient (effect) multiplied by the effect-coded predictor variable (trial_type.e), and the error term. dat &lt;- data.frame( trial_type = rep(names(trial_types), each = n_per_grp) ) %&gt;% mutate( trial_type.e = recode(trial_type, !!!trial_types), error = rnorm(nrow(.), 0, error_sd), RT = mu + effect*trial_type.e + error ) The !!! (triple bang) in the code recode(trial_type, !!!trial_types) is a way to expand the vector trial_types &lt;- c(“congruent” = 0.5, “incongruent” = -0.5). It’s equivalent to recode(trial_type, “congruent” = 0.5, “incongruent” = -0.5). This pattern avoids making mistakes with recoding because there is only one place where you set up the category to code mapping (in the trial_types vector). Last but not least, always plot simulated data to make sure it looks like you expect. ggplot(dat, aes(trial_type, RT)) + geom_violin() + geom_boxplot(aes(fill = trial_type), width = 0.25, show.legend = FALSE) Figure 9.1: Simulated Data 9.4.4 Linear Regression Now we can analyse the data we simulated using the function lm(). It takes the formula as the first argument. This is the same as the data-generating equation, but you can omit the error term (this is implied), and takes the data table as the second argument. Use the summary() function to see the statistical summary. my_lm &lt;- lm(RT ~ trial_type.e, data = dat) summary(my_lm) ## ## Call: ## lm(formula = RT ~ trial_type.e, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -302.110 -70.052 0.948 68.262 246.220 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 788.192 7.206 109.376 &lt; 2e-16 *** ## trial_type.e 61.938 14.413 4.297 2.71e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 101.9 on 198 degrees of freedom ## Multiple R-squared: 0.08532, Adjusted R-squared: 0.0807 ## F-statistic: 18.47 on 1 and 198 DF, p-value: 2.707e-05 Notice how the estimate for the (Intercept) is close to the value we set for mu and the estimate for trial_type.e is close to the value we set for effect. Change the values of mu and effect, resimulate the data, and re-run the linear model. What happens to the estimates? 9.4.5 Residuals You can use the residuals() function to extract the error term for each each data point. This is the DV values, minus the estimates for the intercept and trial type. We’ll make a density plot of the residuals below and compare it to the normal distribution we used for the error term. res &lt;- residuals(my_lm) ggplot(dat) + stat_function(aes(0), color = &quot;grey60&quot;, fun = dnorm, n = 101, args = list(mean = 0, sd = error_sd)) + geom_density(aes(res, color = trial_type)) Figure 9.2: Model residuals should be approximately normally distributed for each group You can also compare the model residuals to the simulated error values. If the model is accurate, they should be almost identical. If the intercept estimate is slightly off, the points will be slightly above or below the black line. If the estimate for the effect of trial type is slightly off, there will be a small, systematic difference between residuals for congruent and incongruent trials. ggplot(dat) + geom_abline(slope = 1) + geom_point(aes(error, res,color = trial_type)) + ylab(&quot;Model Residuals&quot;) + xlab(&quot;Simulated Error&quot;) Figure 9.3: Model residuals should be very similar to the simulated error What happens to the residuals if you fit a model that ignores trial type (e.g., lm(Y ~ 1, data = dat))? 9.4.6 Predict New Values You can use the estimates from your model to predict new data points, given values for the model parameters. For this simple example, we just need to know the trial type to make a prediction. For congruent trials, you would predict that a new data point would be equal to the intercept estimate plus the trial type estimate multiplied by 0.5 (the effect code for congruent trials). int_est &lt;- my_lm$coefficients[[&quot;(Intercept)&quot;]] tt_est &lt;- my_lm$coefficients[[&quot;trial_type.e&quot;]] tt_code &lt;- trial_types[[&quot;congruent&quot;]] new_congruent_RT &lt;- int_est + tt_est * tt_code new_congruent_RT ## [1] 819.1605 You can also use the predict() function to do this more easily. The second argument is a data table with columns for the factors in the model and rows with the values that you want to use for the prediction. predict(my_lm, newdata = tibble(trial_type.e = 0.5)) ## 1 ## 819.1605 If you look up this function using ?predict, you will see that “The function invokes particular methods which depend on the class of the first argument.” What this means is that predict() works differently depending on whether you’re predicting from the output of lm() or other analysis functions. You can search for help on the lm version with ?predict.lm. 9.4.7 Coding Categorical Variables In the example above, we used effect coding for trial type. You can also use sum coding, which assigns +1 and -1 to the levels instead of +0.5 and -0.5. More commonly, you might want to use treatment coding, which assigns 0 to one level (usually a baseline or control condition) and 1 to the other level (usually a treatment or experimental condition). Here we will add sum-coded and treatment-coded versions of trial_type to the dataset using the recode() function. dat &lt;- dat %&gt;% mutate( trial_type.sum = recode(trial_type, &quot;congruent&quot; = +1, &quot;incongruent&quot; = -1), trial_type.tr = recode(trial_type, &quot;congruent&quot; = 1, &quot;incongruent&quot; = 0) ) If you define named vectors with your levels and coding, you can use them with the recode() function if you expand them using !!!. tt_sum &lt;- c(&quot;congruent&quot; = +1, &quot;incongruent&quot; = -1) tt_tr &lt;- c(&quot;congruent&quot; = 1, &quot;incongruent&quot; = 0) dat &lt;- dat %&gt;% mutate( trial_type.sum = recode(trial_type, !!!tt_sum), trial_type.tr = recode(trial_type, !!!tt_tr) ) Here are the coefficients for the effect-coded version. They should be the same as those from the last analysis. lm(RT ~ trial_type.e, data = dat)$coefficients ## (Intercept) trial_type.e ## 788.19166 61.93773 Here are the coefficients for the sum-coded version. This give the same results as effect coding, except the estimate for the categorical factor will be exactly half as large, as it represents the difference between each trial type and the hypothetical condition of 0 (the overall mean RT), rather than the difference between the two trial types. lm(RT ~ trial_type.sum, data = dat)$coefficients ## (Intercept) trial_type.sum ## 788.19166 30.96887 Here are the coefficients for the treatment-coded version. The estimate for the categorical factor will be the same as in the effect-coded version, but the intercept will decrease. It will be equal to the intercept minus the estimate for trial type from the sum-coded version. lm(RT ~ trial_type.tr, data = dat)$coefficients ## (Intercept) trial_type.tr ## 757.22279 61.93773 9.5 Relationships among tests 9.5.1 T-test The t-test is just a special, limited example of a general linear model. t.test(RT ~ trial_type.e, data = dat, var.equal = TRUE) ## ## Two Sample t-test ## ## data: RT by trial_type.e ## t = -4.2975, df = 198, p-value = 2.707e-05 ## alternative hypothesis: true difference in means between group -0.5 and group 0.5 is not equal to 0 ## 95 percent confidence interval: ## -90.35945 -33.51601 ## sample estimates: ## mean in group -0.5 mean in group 0.5 ## 757.2228 819.1605 What happens when you use other codings for trial type in the t-test above? Which coding maps onto the results of the t-test best? 9.5.2 ANOVA ANOVA is also a special, limited version of the linear model. my_aov &lt;- aov(RT ~ trial_type.e, data = dat) summary(my_aov, intercept = TRUE) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## (Intercept) 1 124249219 124249219 11963.12 &lt; 2e-16 *** ## trial_type.e 1 191814 191814 18.47 2.71e-05 *** ## Residuals 198 2056432 10386 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The easiest way to get parameters out of an analysis is to use the broom::tidy() function. This returns a tidy table that you can extract numbers of interest from. Here, we just want to get the F-value for the effect of trial_type. Compare the square root of this value to the t-value from the t-tests above. f &lt;- broom::tidy(my_aov)$statistic[1] sqrt(f) ## [1] 4.297498 9.6 Understanding ANOVA We’ll walk through an example of a one-way ANOVA with the following equation: \\(Y_{ij} = \\mu + A_i + S(A)_{ij}\\) This means that each data point (\\(Y_{ij}\\)) is predicted to be the sum of the grand mean (\\(\\mu\\)), plus the effect of factor A (\\(A_i\\)), plus some residual error (\\(S(A)_{ij}\\)). 9.6.1 Means, Variability, and Deviation Scores Let’s create a simple simulation function so you can quickly create a two-sample dataset with specified Ns, means, and SDs. two_sample &lt;- function(n = 10, m1 = 0, m2 = 0, sd1 = 1, sd2 = 1) { s1 &lt;- rnorm(n, m1, sd1) s2 &lt;- rnorm(n, m2, sd2) data.frame( Y = c(s1, s2), grp = rep(c(&quot;A&quot;, &quot;B&quot;), each = n) ) } Now we will use two_sample() to create a dataset dat with N=5 per group, means of -2 and +2, and SDs of 1 and 1 (yes, this is an effect size of d = 4). dat &lt;- two_sample(5, -2, +2, 1, 1) You can calculate how each data point (Y) deviates from the overall sample mean (\\(\\hat{\\mu}\\)), which is represented by the horizontal grey line below and the deviations are the vertical grey lines. You can also calculate how different each point is from its group-specific mean (\\(\\hat{A_i}\\)), which are represented by the horizontal coloured lines below and the deviations are the coloured vertical lines. Figure 9.4: Deviations of each data point (Y) from the overall and group means You can use these deviations to calculate variability between groups and within groups. ANOVA tests whether the variability between groups is larger than that within groups, accounting for the number of groups and observations. 9.6.2 Decomposition matrices We can use the estimation equations for a one-factor ANOVA to calculate the model components. mu is the overall mean a is how different each group mean is from the overall mean err is residual error, calculated by subtracting mu and a from Y This produces a decomposition matrix, a table with columns for Y, mu, a, and err. decomp &lt;- dat %&gt;% select(Y, grp) %&gt;% mutate(mu = mean(Y)) %&gt;% # calculate mu_hat group_by(grp) %&gt;% mutate(a = mean(Y) - mu) %&gt;% # calculate a_hat for each grp ungroup() %&gt;% mutate(err = Y - mu - a) # calculate residual error Y grp mu a err -1.4770938 A 0.1207513 -1.533501 -0.0643443 -2.9508741 A 0.1207513 -1.533501 -1.5381246 -0.6376736 A 0.1207513 -1.533501 0.7750759 -1.7579084 A 0.1207513 -1.533501 -0.3451589 -0.2401977 A 0.1207513 -1.533501 1.1725518 0.1968155 B 0.1207513 1.533501 -1.4574367 2.6308008 B 0.1207513 1.533501 0.9765486 2.0293297 B 0.1207513 1.533501 0.3750775 2.1629037 B 0.1207513 1.533501 0.5086516 1.2514112 B 0.1207513 1.533501 -0.4028410 Calculate sums of squares for mu, a, and err. SS &lt;- decomp %&gt;% summarise(mu = sum(mu*mu), a = sum(a*a), err = sum(err*err)) mu a err 0.1458088 23.51625 8.104182 If you’ve done everything right, SS$mu + SS$a + SS$err should equal the sum of squares for Y. SS_Y &lt;- sum(decomp$Y^2) all.equal(SS_Y, SS$mu + SS$a + SS$err) ## [1] TRUE Divide each sum of squares by its corresponding degrees of freedom (df) to calculate mean squares. The df for mu is 1, the df for factor a is K-1 (K is the number of groups), and the df for err is N - K (N is the number of observations). K &lt;- n_distinct(dat$grp) N &lt;- nrow(dat) df &lt;- c(mu = 1, a = K - 1, err = N - K) MS &lt;- SS / df mu a err 0.1458088 23.51625 1.013023 Then calculate an F-ratio for mu and a by dividing their mean squares by the error term mean square. Get the p-values that correspond to these F-values using the pf() function. F_mu &lt;- MS$mu / MS$err F_a &lt;- MS$a / MS$err p_mu &lt;- pf(F_mu, df1 = df[&#39;mu&#39;], df2 = df[&#39;err&#39;], lower.tail = FALSE) p_a &lt;- pf(F_a, df1 = df[&#39;a&#39;], df2 = df[&#39;err&#39;], lower.tail = FALSE) Put everything into a data frame to display it in the same way as the ANOVA summary function. my_calcs &lt;- data.frame( term = c(&quot;Intercept&quot;, &quot;grp&quot;, &quot;Residuals&quot;), Df = df, SS = c(SS$mu, SS$a, SS$err), MS = c(MS$mu, MS$a, MS$err), F = c(F_mu, F_a, NA), p = c(p_mu, p_a, NA) ) term Df SS MS F p mu Intercept 1 0.146 0.146 0.144 0.714 a grp 1 23.516 23.516 23.214 0.001 err Residuals 8 8.104 1.013 NA NA Now run a one-way ANOVA on your results and compare it to what you obtained in your calculations. aov(Y ~ grp, data = dat) %&gt;% summary(intercept = TRUE) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## (Intercept) 1 0.146 0.146 0.144 0.71427 ## grp 1 23.516 23.516 23.214 0.00132 ** ## Residuals 8 8.104 1.013 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Using the code above, write your own function that takes a table of data and returns the ANOVA results table like above. 9.7 Glossary term definition categorical Data that can only take certain values, such as types of pet. coding scheme How to represent categorical variables with numbers for use in models continuous Data that can take on any values between other existing values. dependent variable The target variable that is being analyzed, whose value is assumed to depend on other variables. effect code A coding scheme for categorical variables that contrasts each group mean with the mean of all the group means. error term The term in a model that represents the difference between the actual and predicted values general linear model A mathematical model comparing how one or more independent variables affect a continuous dependent variable independent variable A variable whose value is assumed to influence the value of a dependent variable. normal distribution A symmetric distribution of data where values near the centre are most probable. residual error That part of an observation that cannot be captured by the statistical model, and thus is assumed to reflect unknown factors. simulation Generating data from summary parameters standard deviation A descriptive statistic that measures how spread out data are relative to the mean. 9.8 Exercises Download the exercises. See the answers only after you’ve attempted all the questions. # run this to access the exercise dataskills::exercise(9) # run this to access the answers dataskills::exercise(9, answers = TRUE) "],["repro.html", "Chapter 10 Reproducible Workflows 10.1 Learning Objectives 10.2 Resources 10.3 Setup 10.4 R Markdown 10.5 Glossary 10.6 References", " Chapter 10 Reproducible Workflows 10.1 Learning Objectives 10.1.1 Basic Create a reproducible script in R Markdown Edit the YAML header to add table of contents and other options Include a table Include a figure Use source() to include code from an external file Report the output of an analysis using inline R 10.1.2 Intermediate Output doc and PDF formats Add a bibliography and in-line citations Format tables using kableExtra 10.1.3 Advanced Create a computationally reproducible project in Code Ocean 10.2 Resources Chapter 27: R Markdown in R for Data Science R Markdown Cheat Sheet R Markdown reference Guide R Markdown Tutorial R Markdown: The Definitive Guide by Yihui Xie, J. J. Allaire, &amp; Garrett Grolemund Papaja Reproducible APA Manuscripts Code Ocean for Computational Reproducibility 10.3 Setup library(tidyverse) library(knitr) library(broom) set.seed(8675309) 10.4 R Markdown By now you should be pretty comfortable working with R Markdown files from the weekly formative exercises and set exercises. Here, we’ll explore some of the more advanced options and create an R Markdown document that produces a reproducible manuscript. First, make a new R Markdown document. 10.4.1 knitr options When you create a new R Markdown file in RStudio, a setup chunk is automatically created. ```{r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE) ``` You can set more default options for code chunks here. See the knitr options documentation for explanations of the possible options. ```{r setup, include=FALSE} knitr::opts_chunk$set( fig.width = 8, fig.height = 5, fig.path = &#39;images/&#39;, echo = FALSE, warning = TRUE, message = FALSE, cache = FALSE ) ``` The code above sets the following options: fig.width = 8 : figure width is 8 inches fig.height = 5 : figure height is 5 inches fig.path = 'images/' : figures are saved in the directory “images” echo = FALSE : do not show code chunks in the rendered document warning = FALSE : do not show any function warnings message = FALSE : do not show any function messages cache = FALSE : run all the code to create all of the images and objects each time you knit (set to TRUE if you have time-consuming code) 10.4.2 YAML Header The YAML header is where you can set several options. --- title: &quot;My Demo Document&quot; author: &quot;Me&quot; output: html_document: theme: spacelab highlight: tango toc: true toc_float: collapsed: false smooth_scroll: false toc_depth: 3 number_sections: false --- The built-in themes are: “cerulean,” “cosmo,” “flatly,” “journal,” “lumen,” “paper,” “readable,” “sandstone,” “simplex,” “spacelab,” “united,” and “yeti.” You can view and download more themes. Try changing the values from false to true to see what the options do. 10.4.3 TOC and Document Headers If you include a table of contents (toc), it is created from your document headers. Headers in markdown are created by prefacing the header title with one or more hashes (#). Add a typical paper structure to your document like the one below. ## Abstract My abstract here... ## Introduction What&#39;s the question; why is it interesting? ## Methods ### Participants How many participants and why? Do your power calculation here. ### Procedure What will they do? ### Analysis Describe the analysis plan... ## Results Demo results for simulated data... ## Discussion What does it all mean? ## References 10.4.4 Code Chunks You can include code chunks that create and display images, tables, or computations to include in your text. Let’s start by simulating some data. First, create a code chunk in your document. You can put this before the abstract, since we won’t be showing the code in this document. We’ll use a modified version of the two_sample function from the GLM lecture to create two groups with a difference of 0.75 and 100 observations per group. This function was modified to add sex and effect-code both sex and group. Using the recode function to set effect or difference coding makes it clearer which value corresponds to which level. There is no effect of sex or interaction with group in these simulated data. two_sample &lt;- function(diff = 0.5, n_per_group = 20) { tibble(Y = c(rnorm(n_per_group, -.5 * diff, sd = 1), rnorm(n_per_group, .5 * diff, sd = 1)), grp = factor(rep(c(&quot;a&quot;, &quot;b&quot;), each = n_per_group)), sex = factor(rep(c(&quot;female&quot;, &quot;male&quot;), times = n_per_group)) ) %&gt;% mutate( grp_e = recode(grp, &quot;a&quot; = -0.5, &quot;b&quot; = 0.5), sex_e = recode(sex, &quot;female&quot; = -0.5, &quot;male&quot; = 0.5) ) } This function requires the tibble and dplyr packages, so remember to load the whole tidyverse package at the top of this script (e.g., in the setup chunk). Now we can make a separate code chunk to create our simulated dataset dat. dat &lt;- two_sample(diff = 0.75, n_per_group = 100) 10.4.4.1 Tables Next, create a code chunk where you want to display a table of the descriptives (e.g., Participants section of the Methods). We’ll use tidyverse functions you learned in the data wrangling lectures to create summary statistics for each group. &#96;&#96;&#96;{r, results='asis'} dat %>% group_by(grp, sex) %>% summarise(n = n(), Mean = mean(Y), SD = sd(Y)) %>% rename(group = grp) %>% mutate_if(is.numeric, round, 3) %>% knitr::kable() &#96;&#96;&#96; ## `summarise()` has grouped output by &#39;grp&#39;. You can override using the `.groups` argument. ## `mutate_if()` ignored the following grouping variables: ## Column `group` group sex n Mean SD a female 50 -0.361 0.796 a male 50 -0.284 1.052 b female 50 0.335 1.080 b male 50 0.313 0.904 Notice that the r chunk specifies the option results='asis'. This lets you format the table using the kable() function from knitr. You can also use more specialised functions from papaja or kableExtra to format your tables. 10.4.4.2 Images Next, create a code chunk where you want to display the image in your document. Let’s put it in the Results section. Use what you learned in the data visualisation lecture to show violin-boxplots for the two groups. &#96;&#96;&#96;{r, fig1, fig.cap=\"Figure 1. Scores by group and sex.\"} ggplot(dat, aes(grp, Y, fill = sex)) + geom_violin(alpha = 0.5) + geom_boxplot(width = 0.25, position = position_dodge(width = 0.9), show.legend = FALSE) + scale_fill_manual(values = c(\"orange\", \"purple\")) + xlab(\"Group\") + ylab(\"Score\") + theme(text = element_text(size = 30, family = \"Times\")) &#96;&#96;&#96; The last line changes the default text size and font, which can be useful for generating figures that meet a journal’s requirements. Figure 10.1: Figure 1. Scores by group and sex. You can also include images that you did not create in R using the typical markdown syntax for images: ![All the Things by [Hyperbole and a Half](http://hyperboleandahalf.blogspot.com/)](images/memes/x-all-the-things.png) All the Things by Hyperbole and a Half 10.4.4.3 In-line R Now let’s use what you learned in the GLM lecture to analyse our simulated data. The document is getting a little cluttered, so let’s move this code to external scripts. Create a new R script called “functions.R” Move the library(tidyverse) line and the two_sample() function definition to this file. Create a new R script called “analysis.R” Move the code for creating dat to this file. Add the following code to the end of the setup chunk: source(&quot;functions.R&quot;) source(&quot;analysis.R&quot;) The source function lets you include code from an external file. This is really useful for making your documents readable. Just make sure you call your source files in the right order (e.g., include function definitions before you use the functions). In the “analysis.R” file, we’re going to run the analysis code and save any numbers we might want to use in our manuscript to variables. grp_lm &lt;- lm(Y ~ grp_e * sex_e, data = dat) stats &lt;- grp_lm %&gt;% broom::tidy() %&gt;% mutate_if(is.numeric, round, 3) The code above runs our analysis predicting Y from the effect-coded group variable grp_e, the effect-coded sex variable sex_e and their intereaction. The tidy function from the broom package turns the output into a tidy table. The mutate_if function uses the function is.numeric to check if each column should be mutated, adn if it is numeric, applies the round function with the digits argument set to 3. If you want to report the results of the analysis in a paragraph istead of a table, you need to know how to refer to each number in the table. Like with everything in R, there are many wways to do this. One is by specifying the column and row number like this: stats$p.value[2] ## [1] 0 Another way is to create variables for each row like this: grp_stats &lt;- filter(stats, term == &quot;grp_e&quot;) sex_stats &lt;- filter(stats, term == &quot;sex_e&quot;) ixn_stats &lt;- filter(stats, term == &quot;grp_e:sex_e&quot;) Add the above code to the end of your analysis.R file. Then you can refer to columns by name like this: grp_stats$p.value sex_stats$statistic ixn_stats$estimate ## [1] 0 ## [1] 0.197 ## [1] -0.099 You can insert these numbers into a paragraph with inline R code that looks like this: Scores were higher in group B than group A (B = &#96;r grp_stats$estimate&#96;, t = &#96;r grp_stats$statistic&#96;, p = &#96;r grp_stats$p.value&#96;). There was no significant difference between men and women (B = &#96;r sex_statsestimate&#96;, t = &#96;r sex_stats$statistic&#96;, p = &#96;r sex_stats$p.value&#96;) and the effect of group was not qualified by an interaction with sex (B = &#96;r ixn_stats$estimate&#96;, t = &#96;r ixn_stats$statistic&#96;, p = &#96;r ixn_stats$p.value&#96;). Rendered text: Scores were higher in group B than group A (B = 0.647, t = 4.74, p = 0). There was no significant difference between men and women (B = 0.027, t = 0.197, p = 0.844) and the effect of group was not qualified by an interaction with sex (B = -0.099, t = -0.363, p = 0.717). Remember, line breaks are ignored when you render the file (unless you add two spaces at the end of lines), so you can use line breaks to make it easier to read your text with inline R code. The p-values aren’t formatted in APA style. We wrote a function to deal with this in the function lecture. Add this function to the “functions.R” file and change the inline text to use the report_p function. report_p &lt;- function(p, digits = 3) { if (!is.numeric(p)) stop(&quot;p must be a number&quot;) if (p &lt;= 0) warning(&quot;p-values are normally greater than 0&quot;) if (p &gt;= 1) warning(&quot;p-values are normally less than 1&quot;) if (p &lt; .001) { reported = &quot;p &lt; .001&quot; } else { roundp &lt;- round(p, digits) fmt &lt;- paste0(&quot;p = %.&quot;, digits, &quot;f&quot;) reported = sprintf(fmt, roundp) } reported } Scores were higher in group B than group A (B = &#96;r grp_stats$estimate&#96;, t = &#96;r grp_stats$statistic&#96;, &#96;r report_p(grp_stats$p.value, 3)&#96;). There was no significant difference between men and women (B = &#96;r sex_stats$estimate&#96;, t = &#96;r sex_stats$statistic&#96;, &#96;r report_p(sex_stats$p.value, 3)&#96;) and the effect of group was not qualified by an interaction with sex (B = &#96;r ixn_stats$estimate&#96;, t = &#96;r ixn_stats$statistic&#96;, &#96;r report_p(ixn_stats$p.value, 3)&#96;). Rendered text: Scores were higher in group B than group A (B = 0.647, t = 4.74, p &lt; .001). There was no significant difference between men and women (B = 0.027, t = 0.197, p = 0.844) and the effect of group was not qualified by an interaction with sex (B = -0.099, t = -0.363, p = 0.717). You might also want to report the statistics for the regression. There are a lot of numbers to format and insert, so it is easier to do this in the analysis script using sprintf for formatting. s &lt;- summary(grp_lm) # calculate p value from fstatistic fstat.p &lt;- pf(s$fstatistic[1], s$fstatistic[2], s$fstatistic[3], lower=FALSE) adj_r &lt;- sprintf( &quot;The regression equation had an adjusted $R^{2}$ of %.3f ($F_{(%i, %i)}$ = %.3f, %s).&quot;, round(s$adj.r.squared, 3), s$fstatistic[2], s$fstatistic[3], round(s$fstatistic[1], 3), report_p(fstat.p, 3) ) Then you can just insert the text in your manuscript like this: ` adj_r`: The regression equation had an adjusted \\(R^{2}\\) of 0.090 (\\(F_{(3, 196)}\\) = 7.546, p &lt; .001). 10.4.5 Bibliography There are several ways to do in-text citations and automatically generate a bibliography in RMarkdown. 10.4.5.1 Create a BibTeX File Manually You can just make a BibTeX file and add citations manually. Make a new Text File in RStudio called “bibliography.bib.” Next, add the line bibliography: bibliography.bib to your YAML header. You can add citations in the following format: @article{shortname, author = {Author One and Author Two and Author Three}, title = {Paper Title}, journal = {Journal Title}, volume = {vol}, number = {issue}, pages = {startpage--endpage}, year = {year}, doi = {doi} } 10.4.5.2 Citing R packages You can get the citation for an R package using the function citation. You can paste the bibtex entry into your bibliography.bib file. Make sure to add a short name (e.g., “rmarkdown”) before the first comma to refer to the reference. citation(package=&quot;rmarkdown&quot;) ## ## To cite the &#39;rmarkdown&#39; package in publications, please use: ## ## JJ Allaire and Yihui Xie and Jonathan McPherson and Javier Luraschi ## and Kevin Ushey and Aron Atkins and Hadley Wickham and Joe Cheng and ## Winston Chang and Richard Iannone (2021). rmarkdown: Dynamic ## Documents for R. R package version 2.9.4. URL ## https://rmarkdown.rstudio.com. ## ## Yihui Xie and J.J. Allaire and Garrett Grolemund (2018). R Markdown: ## The Definitive Guide. Chapman and Hall/CRC. ISBN 9781138359338. URL ## https://bookdown.org/yihui/rmarkdown. ## ## Yihui Xie and Christophe Dervieux and Emily Riederer (2020). R ## Markdown Cookbook. Chapman and Hall/CRC. ISBN 9780367563837. URL ## https://bookdown.org/yihui/rmarkdown-cookbook. ## ## To see these entries in BibTeX format, use &#39;print(&lt;citation&gt;, ## bibtex=TRUE)&#39;, &#39;toBibtex(.)&#39;, or set ## &#39;options(citation.bibtex.max=999)&#39;. 10.4.5.3 Download Citation Info You can get the BibTeX formatted citation from most publisher websites. For example, go to the publisher’s page for Equivalence Testing for Psychological Research: A Tutorial, click on the Cite button (in the sidebar or under the bottom Explore More menu), choose BibTeX format, and download the citation. You can open up the file in a text editor and copy the text. It should look like this: @article{doi:10.1177/2515245918770963, author = {Daniël Lakens and Anne M. Scheel and Peder M. Isager}, title ={Equivalence Testing for Psychological Research: A Tutorial}, journal = {Advances in Methods and Practices in Psychological Science}, volume = {1}, number = {2}, pages = {259-269}, year = {2018}, doi = {10.1177/2515245918770963}, URL = { https://doi.org/10.1177/2515245918770963 }, eprint = { https://doi.org/10.1177/2515245918770963 } , abstract = { Psychologists must be able to test both for the presence of an effect and for the absence of an effect. In addition to testing against zero, researchers can use the two one-sided tests (TOST) procedure to test for equivalence and reject the presence of a smallest effect size of interest (SESOI). The TOST procedure can be used to determine if an observed effect is surprisingly small, given that a true effect at least as extreme as the SESOI exists. We explain a range of approaches to determine the SESOI in psychological science and provide detailed examples of how equivalence tests should be performed and reported. Equivalence tests are an important extension of the statistical tools psychologists currently use and enable researchers to falsify predictions about the presence, and declare the absence, of meaningful effects. } } Paste the reference into your bibliography.bib file. Change doi:10.1177/2515245918770963 in the first line of the reference to a short string you will use to cite the reference in your manuscript. We’ll use TOSTtutorial. 10.4.5.4 Converting from reference software Most reference software like EndNote, Zotero or mendeley have exporting options that can export to BibTeX format. You just need to check the shortnames in the resulting file. 10.4.5.5 In-text citations You can cite reference in text like this: This tutorial uses several R packages [@tidyverse;@rmarkdown]. This tutorial uses several R packages (Wickham 2017; Allaire et al. 2018). Put a minus in front of the @ if you just want the year: Lakens, Scheel and Isengar [-@TOSTtutorial] wrote a tutorial explaining how to test for the absence of an effect. Lakens, Scheel and Isengar (2018) wrote a tutorial explaining how to test for the absence of an effect. 10.4.5.6 Citation Styles You can search a list of style files for various journals and download a file that will format your bibliography for a specific journal’s style. You’ll need to add the line csl: filename.csl to your YAML header. Add some citations to your bibliography.bib file, reference them in your text, and render your manuscript to see the automatically generated reference section. Try a few different citation style files. 10.4.6 Output Formats You can knit your file to PDF or Word if you have the right packages installed on your computer. 10.4.7 Computational Reproducibility Computational reproducibility refers to making all aspects of your analysis reproducible, including specifics of the software you used to run the code you wrote. R packages get updated periodically and some of these updates may break your code. Using a computational reproducibility platform guards against this by always running your code in the same environment. Code Ocean is a new platform that lets you run your code in the cloud via a web browser. 10.5 Glossary term definition chunk A section of code in an R Markdown file markdown A way to specify formatting, such as headers, paragraphs, lists, bolding, and links. reproducibility The extent to which the findings of a study can be repeated in some other context yaml A structured format for information 10.6 References E References "],["acknowledgements.html", "Acknowledgements 10.7 Contributors", " Acknowledgements The whole psyTeachR team at the University of Glasgow School of Psychology deserves enormous thanks for making it possible and rewarding to teach methods with a focus on reproducibility and open science. Particularly Heather Cleland Woods, Phil McAleer, Helena Paterson, Emily Nordmann, Benedict Jones, and Niamh Stack. We greatly appreciate Iris Holzleitner’s volunteer in-class assistance with the first year of this course. We were ever so lucky to get Rebecca Lai as a teaching assistant in the second year; her kind and patient approach to teaching technical skills is an inspiration. Thanks to Daniël Lakens for many inspirational discussions and resources. 10.7 Contributors Several people contributed to testing these materials. Rebecca Lai Richard Morey Mossa Merhi Reimert "],["installingr.html", "A Installing R A.1 Installing Base R A.2 Installing RStudio A.3 Installing LaTeX A.4 RStudio Online", " A Installing R Installing R and RStudio is usually straightforward. The sections below explain how and there is a helpful YouTube video here. A.1 Installing Base R Install base R from https://cran.rstudio.com/. Choose the download link for your operating system (Linux, Mac OS X, or Windows). If you have a Mac, install the latest release from the newest R-x.x.x.pkg link (or a legacy version if you have an older operating system). After you install R, you should also install XQuartz to be able to use some visualisation packages. If you are installing the Windows version, choose the “base” subdirectory and click on the download link at the top of the page. After you install R, you should also install RTools; use the “recommended” version highlighted near the top of the list. If you are using Linux, choose your specific operating system and follow the installation instructions. A.2 Installing RStudio Go to rstudio.com and download the RStudio Desktop (Open Source License) version for your operating system under the list titled Installers for Supported Platforms. A.3 Installing LaTeX You can install the LaTeX typesetting system to produce PDF reports from RStudio. Without this additional installation, you will be able to produce reports in HTML but not PDF. To generate PDF reports, you will additionally need: pandoc, and LaTeX, a typesetting language, available for WINDOWS: MikTeX Mac OS: MacTex (3.2GB download) or BasicTeX (78MB download, but should work fine) Linux: TeX Live A.4 RStudio Online You may need to access R and RStudio online if you use a tablet or chromebook that can’t install R. Students in the Institute of Neuroscience and Psychology at the University of Glasgow can access Glasgow Psychology RStudio with their GUID and password. RStudio Cloud is a free online service that allows access to R and RStudio. "],["symbols.html", "B Symbols", " B Symbols Symbol psyTeachR Term Also Known As () (round) brackets parentheses [] square brackets brackets {} curly brackets squiggly brackets &lt;&gt; chevrons angled brackets / guillemets &lt; less than &gt; greater than &amp; ampersand “and” symbol # hash pound / octothorpe / slash forward slash \\ backslash - dash hyphen / minus _ underscore * asterisk star ^ caret power symbol ~ tilde twiddle / squiggle = equal sign == double equal sign . full stop period / point ! exclamation mark bang / not ? question mark ’ single quote quote / apostrophe \" double quote quote %&gt;% pipe magrittr pipe | vertical bar pipe , comma ; semi-colon : colon @ “at” symbol various hilarious regional terms "],["exercise-answers.html", "C Exercise Answers", " C Exercise Answers Download all exercises and data files below as a ZIP archive. The answers are not included in the zip file. 01 intro (answers): Intro to R, functions, R markdown 02 data (answers): Vectors, tabular data, data import, pipes Essential Skills (answers): You must be able to complete these exercises to advance in the class beyond the first two lectures 03 ggplot (answers): Data visualisation 04 tidyr (answers): Tidy Data 05 dplyr (answers): Data wrangling 06 joins (answers): Data relations 07 functions (answers): Functions and iteration 08 simulation (answers): Simulation 09 glm (answers): GLM "],["datasets.html", "D Datasets", " D Datasets You can download a zip file of the datasets or access them from the class package with getdata(). The help files for each data table describe the data and each column. Each data table comes with a codebook in Psych-DS format. Country Codes Multiple country, subregion, and region codes for 249 countries. Three Domain Disgust Questionnaire (correlations) Correlations among questions on the Three Domain Disgust Questionnaire (Tybur et al.) Three Domain Disgust Questionnaire (scores) A dataset containing subscale scores for to the Three Domain Disgust Questionnaire (Tybur et al.), calculated from [disgust]. Three Domain Disgust Questionnaire (items) A dataset containing responses to the 21 items in the Three Domain Disgust Questionnaire (Tybur et al.) Parental Attachment (Mothers) Items starting with r, p and e are for the rejection (r), overprotection (p), and emotional warmth (e) subscales. Empathizing Quotient Reverse coded (Q#R) questions coded and strongly disagree = 2, slightly disagree = 1, else = 0. The other questions are coded as strongly agree = 2, slightly agree = 1, else = 0. Experimentum Project Experiments Data from a demo experiment on Experimentum https://debruine.github.io/experimentum/. Subjects are shown pairs of upright and inverted Mooney faces and asked to click on the upright face. Experimentum Project Questionnaires Data from a demo questionnaire on Experimentum https://debruine.github.io/experimentum/. Subjects are asked questions about dogs to test the different questionnaire response types. Descriptions of Eyes Participant’s written descriptions of the eyes of 50 people Family Composition Responses to a brief questionnaire about family composition. Infant Mortality Infant mortality by country and year from the World Health Organisation. Maternal Mortality Maternal mortality by country and year from the World Health Organisation. Messy Data A dataset with missing values, blank rows, incorrect data types, multiple values in one column, and multiple date types for practicing data import. 5-Factor Personality Scores Archival data from the Face Research Lab of a 5-factor personality questionnaire, with factor score calculated from [personality]. 5-Factor Personality Items Archival data from the Face Research Lab of a 5-factor personality questionnaire. Each question is labelled with the domain (Op = openness, Co = conscientiousness, Ex = extroversion, Ag = agreeableness, and Ne = neuroticism) and the question number. Participants rate each statement on a Likert scale from 0 (Never) to 6 (Always). Questions with REV have already been reverse-coded (0 = Always, 6 = Never). Pets A simulated dataset with one random factor (id), two categorical factors (pet, country) and three continuous variables (score, age, weight). This dataset is useful for practicing plotting. First Impressions of Faces (Aggregated) Aggregated data from Psychological Science Accelerator project: To Which World Regions Does the Valence-Dominance Model of Social Perception Apply? https://psyarxiv.com/n26dy. Mean ratings on 13 traits for each of 120 faces shown in 10 world regions. Face characteristics at [psa001_cfd_faces]. Face Characteristics Face stimulus characteristics from Psychological Science Accelerator project: To Which World Regions Does the Valence-Dominance Model of Social Perception Apply? https://psyarxiv.com/n26dy. To be used with [psa001_agg]. Sensation Seeking Scale Zuckerman M. (1984). Sensation seeking: a comparative approach to a human trait. Behavioral and Brain Sciences. 7: 413-471. Small Factorial Design: 2w*2b Small simulated dataset (n = 5) with one within-subject factor (time) having 2 levels (pre and post) and one beteen-subject factor (group) having two levels(control and experimental). The dataset is in wide format and created with faux. Systemizing Quotient Reverse coded (Q#R) questions coded as strongly disagree = 2, slightly disagree = 1, else = 0. The other questions are coded as strongly agree = 2, slightly agree = 1, else = 0. c(“* Stroop Task 50 simulated subject in a stroop task viewing all combinations of word and ink colours blue, purple, green, red, and brown, 5 times each. Subjects respond with the ink colour. Subjects who do not respond in time have NA for response and rt.” “* Stroop Task 50 simulated subject in a stroop task viewing all combinations of word and ink colours blue, purple, green, red, and brown, 5 times each. Subjects respond with the ink colour. Subjects who do not respond in time have NA for response and rt.” ) User Demographics A dataset with unique participant ID, sex and birth year. To be used in conjunction with data from [disgust], [disgust_scores], [personality], [personality_scores], and [users2]. User Demographics 2 A dataset with unique participant ID, birth year, and sex. To be used in conjunction with data from [disgust], [disgust_scores], [personality], [personality_scores], and [users]. "],["bookrefs.html", "E References", " E References "]]
